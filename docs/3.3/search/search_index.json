{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Iguana documentation! This documentation will help you benchmark your HTTP endpoints (such as your Triple store) using Iguana and help you extend Iguana to your needs. It is split into three parts General Quick Start Guide Usage Development In General you will find a bit of information of what Iguana is and what it's capable of. In the Quick Start Guide you will find how to download and start Iguana as well how to quickly configure your first simple benchmark using Iguana. In Usage you will find everything on how to execute a benchmark with Iguana and how to configure the benchmark to your needs. It further provides details on what tests Iguana is capable of. A Tutorial will finally guide you through all steps broadly which you can use as a quick start. In Development you will find everything you need to know in case that Iguana isn't sufficient for your needs. It shows how to extend Iguana to use your metrics or your specific benchmark test Have exciting Evaluations!","title":"Home"},{"location":"#welcome-to-the-iguana-documentation","text":"This documentation will help you benchmark your HTTP endpoints (such as your Triple store) using Iguana and help you extend Iguana to your needs. It is split into three parts General Quick Start Guide Usage Development In General you will find a bit of information of what Iguana is and what it's capable of. In the Quick Start Guide you will find how to download and start Iguana as well how to quickly configure your first simple benchmark using Iguana. In Usage you will find everything on how to execute a benchmark with Iguana and how to configure the benchmark to your needs. It further provides details on what tests Iguana is capable of. A Tutorial will finally guide you through all steps broadly which you can use as a quick start. In Development you will find everything you need to know in case that Iguana isn't sufficient for your needs. It shows how to extend Iguana to use your metrics or your specific benchmark test Have exciting Evaluations!","title":"Welcome to the Iguana documentation!"},{"location":"about/","text":"Iguana Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications. Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data. Hence it is very important that the triple store must scale on the data and can handle several users. Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily. Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well. Further on it was impossible to compare results for different benchmarks. Iguana tries to solve all these issues. It provides an enviroment which ... is highly configurable provides a realistic scneario benchmark works on every dataset works on SPARQL HTTP endpoints works on HTTP Get & Post endpoints works on CLI applications and is easily extendable What is Iguana Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. What can be benchmarked Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query What Benchmarks are possible Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"About"},{"location":"about/#iguana","text":"Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications. Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data. Hence it is very important that the triple store must scale on the data and can handle several users. Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily. Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well. Further on it was impossible to compare results for different benchmarks. Iguana tries to solve all these issues. It provides an enviroment which ... is highly configurable provides a realistic scneario benchmark works on every dataset works on SPARQL HTTP endpoints works on HTTP Get & Post endpoints works on CLI applications and is easily extendable","title":"Iguana"},{"location":"about/#what-is-iguana","text":"Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line.","title":"What is Iguana"},{"location":"about/#what-can-be-benchmarked","text":"Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query","title":"What can be benchmarked"},{"location":"about/#what-benchmarks-are-possible","text":"Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"What Benchmarks are possible"},{"location":"architecture/","text":"Architecture Iguanas architecture is build as generic as possible to ensure that your benchmark can be executed while you only have to create a configuration file which fits your needs. So ideally you do not need to code anything and can use Iguana out of the box. Iguana will parse your Configuration (YAML or JSON format) and will read which Endpoints/Applications you want to benchmark. What datasets if you have any and what your benchmark should accomplish. Do you just want to check how good your database/triple store performs against the state of the art? Does your new version out performs the old version? Do you want to check read and write performance? ... Whatever you want to do you just need to provide Iguana your tested applications, what to benchmark and which queries to use. Iguana relys mainly on HTTP libraries, the JENA framework and java 11. Overview Iguana will read the configuration, parse it and executes for each specified datasets, each specified connection with the benchmark tasks you specified. After the executions the results will be written as RDF. Either to a NTriple file or directly to a triple store. The results can be queried itself using SPARQL. Iguana currently consists of on implemented Task, the Stresstest. However, this task is very configurable and most definetly will met your needs if you want performance measurement. It starts a user defined amount of Workers, which will try to simulate real users/applications querying your Endpoint/Application. Components Iguana consists of two components, the core controller and the result processor. core controller The core which implements the Tasks and Workers to use. How HTTP responses should be handled. How to analyze the benchmark queries to give a little bit more extra information in the results. result processor The result processor consist of the metrics to apply to the query execution results and how to save the results. Most of the SOtA metrics are implemented in Iguana. If one's missing it is pretty easy to add a metric though. By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store. On the processing side, it calculates various metrics. Per run metrics: * Query Mixes Per Hour (QMPH) * Number of Queries Per Hour (NoQPH) * Number of Queries (NoQ) * Average Queries Per Second (AvgQPS) Per query metrics: * Queries Per Second (QPS) * Number of successful and failed queries * result size * queries per second * sum of execution times You can change these in the Iguana Benchmark suite config. If you use the basic configuration , it will save all mentioned metrics to a file called results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt More Information SPARQL RDF Iguana @ Github Our Paper from 2017 (outdated)","title":"Architecture"},{"location":"architecture/#architecture","text":"Iguanas architecture is build as generic as possible to ensure that your benchmark can be executed while you only have to create a configuration file which fits your needs. So ideally you do not need to code anything and can use Iguana out of the box. Iguana will parse your Configuration (YAML or JSON format) and will read which Endpoints/Applications you want to benchmark. What datasets if you have any and what your benchmark should accomplish. Do you just want to check how good your database/triple store performs against the state of the art? Does your new version out performs the old version? Do you want to check read and write performance? ... Whatever you want to do you just need to provide Iguana your tested applications, what to benchmark and which queries to use. Iguana relys mainly on HTTP libraries, the JENA framework and java 11.","title":"Architecture"},{"location":"architecture/#overview","text":"Iguana will read the configuration, parse it and executes for each specified datasets, each specified connection with the benchmark tasks you specified. After the executions the results will be written as RDF. Either to a NTriple file or directly to a triple store. The results can be queried itself using SPARQL. Iguana currently consists of on implemented Task, the Stresstest. However, this task is very configurable and most definetly will met your needs if you want performance measurement. It starts a user defined amount of Workers, which will try to simulate real users/applications querying your Endpoint/Application.","title":"Overview"},{"location":"architecture/#components","text":"Iguana consists of two components, the core controller and the result processor.","title":"Components"},{"location":"architecture/#core-controller","text":"The core which implements the Tasks and Workers to use. How HTTP responses should be handled. How to analyze the benchmark queries to give a little bit more extra information in the results.","title":"core controller"},{"location":"architecture/#result-processor","text":"The result processor consist of the metrics to apply to the query execution results and how to save the results. Most of the SOtA metrics are implemented in Iguana. If one's missing it is pretty easy to add a metric though. By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store. On the processing side, it calculates various metrics. Per run metrics: * Query Mixes Per Hour (QMPH) * Number of Queries Per Hour (NoQPH) * Number of Queries (NoQ) * Average Queries Per Second (AvgQPS) Per query metrics: * Queries Per Second (QPS) * Number of successful and failed queries * result size * queries per second * sum of execution times You can change these in the Iguana Benchmark suite config. If you use the basic configuration , it will save all mentioned metrics to a file called results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt","title":"result processor"},{"location":"architecture/#more-information","text":"SPARQL RDF Iguana @ Github Our Paper from 2017 (outdated)","title":"More Information"},{"location":"download/","text":"Download Prerequisites You need to have Java 11 or higher installed. In Ubuntu you can do this by sudo apt-get install java Download Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest The zip file contains 3 files. iguana-3.3.1.jar example-suite.yml start-iguana.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"download/#download","text":"","title":"Download"},{"location":"download/#prerequisites","text":"You need to have Java 11 or higher installed. In Ubuntu you can do this by sudo apt-get install java","title":"Prerequisites"},{"location":"download/#download_1","text":"Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest The zip file contains 3 files. iguana-3.3.1.jar example-suite.yml start-iguana.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"quick-config/","text":"Quickly Configure Iguana Here we will setup a quick configuration which will benchmark one triple store (e.g. apache jena fuseki) using one simulated user. We assume that your triple store (or whatever HTTP GET endpoint you want to use) is running and loaded with data. For now we assume that the endpoint is at http://localhost:3030/ds/sparql and uses GET with the parameter query Further on the benchmark should take 10 minutes (or 60.000 ms) and uses plain text queries located in queries.txt . If you do not have created some queries yet, use these for example SELECT * {?s ?p ?o} SELECT * {?s ?p ?o} LIMIT 10 SELECT * {?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o} and save them to queries.txt . Your results will be written as an N-Triple file to first-benchmark-results.nt The following configuration works with these demands. # you can ignore this for now datasets: - name: \"Dataset\" #Your connection connections: - name: \"Fuseki\" # Change this to your actual endpoint you want to use endpoint: \"http://localhost:3030/ds/sparql\" # The benchmark task tasks: - className: \"Stresstest\" configuration: # 10 minutes (time Limit is in ms) timeLimit: 600000 # we are using plain text queries queryHandler: className: \"InstancesQueryHandler\" # create one SPARQL Worker (it's basically a HTTP get worker using the 'query' parameter # it uses the queries.txt file as benchmark queries workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" # tell Iguana where to save your results to storages: - className: \"NTFileStorage\" configuration: fileName: \"first-benchmark-results.nt\" For more information on the confguration have a look at Configuration","title":"Quick Configuration"},{"location":"quick-config/#quickly-configure-iguana","text":"Here we will setup a quick configuration which will benchmark one triple store (e.g. apache jena fuseki) using one simulated user. We assume that your triple store (or whatever HTTP GET endpoint you want to use) is running and loaded with data. For now we assume that the endpoint is at http://localhost:3030/ds/sparql and uses GET with the parameter query Further on the benchmark should take 10 minutes (or 60.000 ms) and uses plain text queries located in queries.txt . If you do not have created some queries yet, use these for example SELECT * {?s ?p ?o} SELECT * {?s ?p ?o} LIMIT 10 SELECT * {?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o} and save them to queries.txt . Your results will be written as an N-Triple file to first-benchmark-results.nt The following configuration works with these demands. # you can ignore this for now datasets: - name: \"Dataset\" #Your connection connections: - name: \"Fuseki\" # Change this to your actual endpoint you want to use endpoint: \"http://localhost:3030/ds/sparql\" # The benchmark task tasks: - className: \"Stresstest\" configuration: # 10 minutes (time Limit is in ms) timeLimit: 600000 # we are using plain text queries queryHandler: className: \"InstancesQueryHandler\" # create one SPARQL Worker (it's basically a HTTP get worker using the 'query' parameter # it uses the queries.txt file as benchmark queries workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" # tell Iguana where to save your results to storages: - className: \"NTFileStorage\" configuration: fileName: \"first-benchmark-results.nt\" For more information on the confguration have a look at Configuration","title":"Quickly Configure Iguana"},{"location":"run-iguana/","text":"Start a Benchmark Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml To set JVM options, you can use $IGUANA_JVM For example to let Iguana use 4GB of RAM you can set the IGUANA_JVM as follows export IGUANA_JVM=-Xmx4g and start as above. or using the jar with java 11 as follows java -jar iguana-corecontroller-3.3.1.jar example-suite.yml","title":"Run Iguana"},{"location":"run-iguana/#start-a-benchmark","text":"Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml To set JVM options, you can use $IGUANA_JVM For example to let Iguana use 4GB of RAM you can set the IGUANA_JVM as follows export IGUANA_JVM=-Xmx4g and start as above. or using the jar with java 11 as follows java -jar iguana-corecontroller-3.3.1.jar example-suite.yml","title":"Start a Benchmark"},{"location":"shorthand-mapping/","text":"Shorthand Class Name Stresstest org.aksw.iguana.cc.tasks.impl.Stresstest ---------- ------- InstancesQueryHandler org.aksw.iguana.cc.query.impl.InstancesQueryHandler DelimInstancesQueryHandler org.aksw.iguana.cc.query.impl.DelimInstancesQueryHandler PatternQueryHandler org.aksw.iguana.cc.query.impl.PatternQueryHandler ---------- ------- lang.RDF org.aksw.iguana.cc.lang.impl.RDFLanguageProcessor lang.SPARQL org.aksw.iguana.cc.lang.impl.SPARQLLanguageProcessor lang.SIMPLE org.aksw.iguana.cc.lang.impl.ThrowawayLanguageProcessor ---------- ------- SPARQLWorker org.aksw.iguana.cc.worker.impl.SPARQLWorker UPDATEWorker org.aksw.iguana.cc.worker.impl.UPDATEWorker HttpPostWorker org.aksw.iguana.cc.worker.impl.HttpPostWorker HttpGetWorker org.aksw.iguana.cc.worker.impl.HttpGetWorker CLIWorker org.aksw.iguana.cc.worker.impl.CLIWorker CLIInputWorker org.aksw.iguana.cc.worker.impl.CLIInputWorker CLIInputFileWorker org.aksw.iguana.cc.worker.impl.CLIInputFileWorker CLIInputPrefixWorker org.aksw.iguana.cc.worker.impl.CLIInputPrefixWorker MultipleCLIInputWorker org.aksw.iguana.cc.worker.impl.MultipleCLIInputWorker ---------- ------- NTFileStorage org.aksw.iguana.rp.storages.impl.NTFileStorage RDFFileStorage org.aksw.iguana.rp.storages.impl.RDFFileStorage TriplestoreStorage org.aksw.iguana.rp.storages.impl.TriplestoreStorage ---------- ------- QPS org.aksw.iguana.rp.metrics.impl.QPSMetric AvgQPS org.aksw.iguana.rp.metrics.impl.AvgQPSMetric NoQ org.aksw.iguana.rp.metrics.impl.NoQMetric NoQPH org.aksw.iguana.rp.metrics.impl.NoQPHMetric QMPH org.aksw.iguana.rp.metrics.impl.QMPHMetric EachQuery org.aksw.iguana.rp.metrics.impl.EQEMetric","title":"Shorthand mapping"},{"location":"develop/architecture/","text":"Test1 Test2","title":"Architecture"},{"location":"develop/architecture/#test1","text":"","title":"Test1"},{"location":"develop/architecture/#test2","text":"","title":"Test2"},{"location":"develop/extend-lang/","text":"Extend Languages If you want to add query specific statistics and/or using the correct result size for an HTTP Worker (Post or Get) you can do so. (This may be interesting if you're not using SPARQL) Let's start by implementing the LanguageProcessor @Shorthand(\"lang.MyLanguage\") public class MyLanguageProcessor implements LanguageProcessor { @Override public String getQueryPrefix() { } @Override public Model generateTripleStats(List<QueryWrapper> queries, String resourcePrefix, String taskID) { } @Override public Long getResultSize(CloseableHttpResponse response) throws ParserConfigurationException, SAXException, ParseException, IOException { } @Override Long getResultSize(Header contentTypeHeader, BigByteArrayOutputStream content) throws ParserConfigurationException, SAXException, ParseException, IOException{ } @Override long readResponse(InputStream inputStream, BigByteArrayOutputStream responseBody) throws IOException{ } } Query prefix Set a query prefix which will be used in the result set, f.e. \"sql\" @Override public String getQueryPrefix() { return \"sql\"; } Generate Query Statistics Generating query specific statistics (which will be added in the result file) You will get the queries (containg of an ID and the query itself) a resourcePrefix you may use to create the URIs and the current taskID. A basic pretty standard exmaple is @Override public Model generateTripleStats(List<QueryWrapper> queries, String resourcePrefix, String taskID) { Model model = ModelFactory.createDefaultModel(); for(QueryWrapper wrappedQuery : queries) { Resource subject = ResourceFactory.createResource(COMMON.RES_BASE_URI + resourcePrefix + \"/\" + wrappedQuery.getId()); model.add(subject, RDF.type, Vocab.queryClass); model.add(subject, Vocab.rdfsID, wrappedQuery.getId().replace(queryPrefix, \"\").replace(\"sql\", \"\")); model.add(subject, RDFS.label, wrappedQuery.getQuery().toString()); //ADD YOUR TRIPLES HERE which contains query specific statistics } return model; } Get the result size To generate the correct result size in the result file do the following @Override public Long getResultSize(CloseableHttpResponse response) throws ParserConfigurationException, SAXException, ParseException, IOException { InputStream inStream = response.getEntity().getContent(); Long size = -1L; //READ INSTREAM ACCORDINGLY return size; } @Override public Long getResultSize(Header contentTypeHeader, BigByteArrayOutputStream content) throws ParserConfigurationException, SAXException, ParseException, IOException { //Read content from Byte Array instead of InputStream InputStream is = new BigByteArrayInputStream(content); Long size=-1L; ... return size; } @Override public long readResponse(InputStream inputStream, BigByteArrayOutputStream responseBody) throws IOException { //simply moves content from inputStream to the byte array responseBody and returns the size; //will be used for parsing the anwser in another thread. return Streams.inputStream2ByteArrayOutputStream(inputStream, responseBody); }","title":"Languages"},{"location":"develop/extend-lang/#extend-languages","text":"If you want to add query specific statistics and/or using the correct result size for an HTTP Worker (Post or Get) you can do so. (This may be interesting if you're not using SPARQL) Let's start by implementing the LanguageProcessor @Shorthand(\"lang.MyLanguage\") public class MyLanguageProcessor implements LanguageProcessor { @Override public String getQueryPrefix() { } @Override public Model generateTripleStats(List<QueryWrapper> queries, String resourcePrefix, String taskID) { } @Override public Long getResultSize(CloseableHttpResponse response) throws ParserConfigurationException, SAXException, ParseException, IOException { } @Override Long getResultSize(Header contentTypeHeader, BigByteArrayOutputStream content) throws ParserConfigurationException, SAXException, ParseException, IOException{ } @Override long readResponse(InputStream inputStream, BigByteArrayOutputStream responseBody) throws IOException{ } }","title":"Extend Languages"},{"location":"develop/extend-lang/#query-prefix","text":"Set a query prefix which will be used in the result set, f.e. \"sql\" @Override public String getQueryPrefix() { return \"sql\"; }","title":"Query prefix"},{"location":"develop/extend-lang/#generate-query-statistics","text":"Generating query specific statistics (which will be added in the result file) You will get the queries (containg of an ID and the query itself) a resourcePrefix you may use to create the URIs and the current taskID. A basic pretty standard exmaple is @Override public Model generateTripleStats(List<QueryWrapper> queries, String resourcePrefix, String taskID) { Model model = ModelFactory.createDefaultModel(); for(QueryWrapper wrappedQuery : queries) { Resource subject = ResourceFactory.createResource(COMMON.RES_BASE_URI + resourcePrefix + \"/\" + wrappedQuery.getId()); model.add(subject, RDF.type, Vocab.queryClass); model.add(subject, Vocab.rdfsID, wrappedQuery.getId().replace(queryPrefix, \"\").replace(\"sql\", \"\")); model.add(subject, RDFS.label, wrappedQuery.getQuery().toString()); //ADD YOUR TRIPLES HERE which contains query specific statistics } return model; }","title":"Generate Query Statistics"},{"location":"develop/extend-lang/#get-the-result-size","text":"To generate the correct result size in the result file do the following @Override public Long getResultSize(CloseableHttpResponse response) throws ParserConfigurationException, SAXException, ParseException, IOException { InputStream inStream = response.getEntity().getContent(); Long size = -1L; //READ INSTREAM ACCORDINGLY return size; } @Override public Long getResultSize(Header contentTypeHeader, BigByteArrayOutputStream content) throws ParserConfigurationException, SAXException, ParseException, IOException { //Read content from Byte Array instead of InputStream InputStream is = new BigByteArrayInputStream(content); Long size=-1L; ... return size; } @Override public long readResponse(InputStream inputStream, BigByteArrayOutputStream responseBody) throws IOException { //simply moves content from inputStream to the byte array responseBody and returns the size; //will be used for parsing the anwser in another thread. return Streams.inputStream2ByteArrayOutputStream(inputStream, responseBody); }","title":"Get the result size"},{"location":"develop/extend-metrics/","text":"Extend Metrics Developed a new metric or simply want to use one that isn't implemented? Start by extending the AbstractMetric package org.benchmark.metric @Shorthand(\"MyMetric\") public class MyMetric extends AbstractMetric{ @Override public void receiveData(Properties p) { } @Override public void close() { callbackClose(); super.close(); } protected void callbackClose() { //ADD YOUR CLOSING HERE } } Receive Data This method will receive all the results during the benchmark. You'll receive a few values regarding that one query execution, the time it took, if it succeeded, if not if it was a timeout, a wrong HTTP Code or unkown. Further on the result size of the query. If your metric is a single value metric you can use the processData method, which will automatically add each value together. However if your metric is query specific you can use the addDataToContainter method. (Look at the QPSMetric . Be aware that both mehtods will save the results for each worker used. This allows to calcualte the overall metric as well the metric for each worker itself. We will go with the single-value metric for now. An example on how to retrieve every possible value and saving the time and success. @Override public void receiveData(Properties p) { double time = Double.parseDouble(p.get(COMMON.RECEIVE_DATA_TIME).toString()); long tmpSuccess = Long.parseLong(p.get(COMMON.RECEIVE_DATA_SUCCESS).toString()); long success = tmpSuccess>0?1:0; long failure = success==1?0:1; long timeout = tmpSuccess==COMMON.QUERY_SOCKET_TIMEOUT?1:0; long unknown = tmpSuccess==COMMON.QUERY_UNKNOWN_EXCEPTION?1:0; long wrongCode = tmpSuccess==COMMON.QUERY_HTTP_FAILURE?1:0; if(p.containsKey(COMMON.RECEIVE_DATA_SIZE)) { size = Long.parseLong(p.get(COMMON.RECEIVE_DATA_SIZE).toString()); } Properties results = new Properties(); results.put(TOTAL_TIME, time); results.put(TOTAL_SUCCESS, success); Properties extra = getExtraMeta(p); processData(extra, results); } Close In this method you should finally calculate your metric and send the results. protected void callbackClose() { //create model to contain results Model m = ModelFactory.createDefaultModel(); Property property = getMetricProperty(); Double sum = 0.0; // Go over each worker and add metric results to model. for(Properties key : dataContainer.keySet()){ Double totalTime = (Double) dataContainer.get(key).get(TOTAL_TIME); Integer success = (Integer) dataContainer.get(key).get(TOTAL_SUCCESS); Double noOfQueriesPerHour = hourInMS*success*1.0/totalTime; sum+=noOfQueriesPerHour; Resource subject = getSubject(key); m.add(getConnectingStatement(subject)); m.add(subject, property, ResourceFactory.createTypedLiteral(noOfQueriesPerHour)); } // Add overall metric to model m.add(getTaskResource(), property, ResourceFactory.createTypedLiteral(sum)); //Send data to storage sendData(m); } Constructor The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task .","title":"Metrics"},{"location":"develop/extend-metrics/#extend-metrics","text":"Developed a new metric or simply want to use one that isn't implemented? Start by extending the AbstractMetric package org.benchmark.metric @Shorthand(\"MyMetric\") public class MyMetric extends AbstractMetric{ @Override public void receiveData(Properties p) { } @Override public void close() { callbackClose(); super.close(); } protected void callbackClose() { //ADD YOUR CLOSING HERE } }","title":"Extend Metrics"},{"location":"develop/extend-metrics/#receive-data","text":"This method will receive all the results during the benchmark. You'll receive a few values regarding that one query execution, the time it took, if it succeeded, if not if it was a timeout, a wrong HTTP Code or unkown. Further on the result size of the query. If your metric is a single value metric you can use the processData method, which will automatically add each value together. However if your metric is query specific you can use the addDataToContainter method. (Look at the QPSMetric . Be aware that both mehtods will save the results for each worker used. This allows to calcualte the overall metric as well the metric for each worker itself. We will go with the single-value metric for now. An example on how to retrieve every possible value and saving the time and success. @Override public void receiveData(Properties p) { double time = Double.parseDouble(p.get(COMMON.RECEIVE_DATA_TIME).toString()); long tmpSuccess = Long.parseLong(p.get(COMMON.RECEIVE_DATA_SUCCESS).toString()); long success = tmpSuccess>0?1:0; long failure = success==1?0:1; long timeout = tmpSuccess==COMMON.QUERY_SOCKET_TIMEOUT?1:0; long unknown = tmpSuccess==COMMON.QUERY_UNKNOWN_EXCEPTION?1:0; long wrongCode = tmpSuccess==COMMON.QUERY_HTTP_FAILURE?1:0; if(p.containsKey(COMMON.RECEIVE_DATA_SIZE)) { size = Long.parseLong(p.get(COMMON.RECEIVE_DATA_SIZE).toString()); } Properties results = new Properties(); results.put(TOTAL_TIME, time); results.put(TOTAL_SUCCESS, success); Properties extra = getExtraMeta(p); processData(extra, results); }","title":"Receive Data"},{"location":"develop/extend-metrics/#close","text":"In this method you should finally calculate your metric and send the results. protected void callbackClose() { //create model to contain results Model m = ModelFactory.createDefaultModel(); Property property = getMetricProperty(); Double sum = 0.0; // Go over each worker and add metric results to model. for(Properties key : dataContainer.keySet()){ Double totalTime = (Double) dataContainer.get(key).get(TOTAL_TIME); Integer success = (Integer) dataContainer.get(key).get(TOTAL_SUCCESS); Double noOfQueriesPerHour = hourInMS*success*1.0/totalTime; sum+=noOfQueriesPerHour; Resource subject = getSubject(key); m.add(getConnectingStatement(subject)); m.add(subject, property, ResourceFactory.createTypedLiteral(noOfQueriesPerHour)); } // Add overall metric to model m.add(getTaskResource(), property, ResourceFactory.createTypedLiteral(sum)); //Send data to storage sendData(m); }","title":"Close"},{"location":"develop/extend-metrics/#constructor","text":"The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task .","title":"Constructor"},{"location":"develop/extend-queryhandling/","text":"Extend Query Handling If you want to use another query generating method as the implemented ones you can do so. Start by extend the AbstractWorkerQueryHandler . It will split up the generation for UPDATE queries and Request queries. package org.benchmark.query public class MyQueryHandler extends AbstractWorkerQueryHandler{ protected abstract QuerySet[] generateQueries(String queryFileName) { } protected abstract QuerySet[] generateUPDATE(String updatePath) { } } for simplicity we will only show the generateQueries as it is pretty much the same. However be aware that the generateUPDATE will use a directory or file instead of just a query file. Generate Queries The class will get a query file containing all the queries. How you read them and what to do with them is up to you. You just need to return an array of QuerySet s A query set is simply a container which contains the name/id of the query as well as the query or several queries (f.e. if they are of the same structure but different values). For simplicity we assume that we deal with only one query per query set. Parse your file and for each query create a QuerySet protected QuerySet[] generateQueries(String queryFileName) { File queryFile = new File(queryFileName); List<QuerySet> ret = new LinkedList<QuerySet>(); int id=0; //TODO parse your queries ... ret.add(new InMemQuerySet(idPrefix+id++, queryString)); ... return ret.toArray(new QuerySet[]{}); } This function will parse your query accodringly and add an In Memory QuerySet (another option is a File Based Query Set, where each QuerySet will be stored in a file and IO happens during the benchmark itself.","title":"Query Handling"},{"location":"develop/extend-queryhandling/#extend-query-handling","text":"If you want to use another query generating method as the implemented ones you can do so. Start by extend the AbstractWorkerQueryHandler . It will split up the generation for UPDATE queries and Request queries. package org.benchmark.query public class MyQueryHandler extends AbstractWorkerQueryHandler{ protected abstract QuerySet[] generateQueries(String queryFileName) { } protected abstract QuerySet[] generateUPDATE(String updatePath) { } } for simplicity we will only show the generateQueries as it is pretty much the same. However be aware that the generateUPDATE will use a directory or file instead of just a query file.","title":"Extend Query Handling"},{"location":"develop/extend-queryhandling/#generate-queries","text":"The class will get a query file containing all the queries. How you read them and what to do with them is up to you. You just need to return an array of QuerySet s A query set is simply a container which contains the name/id of the query as well as the query or several queries (f.e. if they are of the same structure but different values). For simplicity we assume that we deal with only one query per query set. Parse your file and for each query create a QuerySet protected QuerySet[] generateQueries(String queryFileName) { File queryFile = new File(queryFileName); List<QuerySet> ret = new LinkedList<QuerySet>(); int id=0; //TODO parse your queries ... ret.add(new InMemQuerySet(idPrefix+id++, queryString)); ... return ret.toArray(new QuerySet[]{}); } This function will parse your query accodringly and add an In Memory QuerySet (another option is a File Based Query Set, where each QuerySet will be stored in a file and IO happens during the benchmark itself.","title":"Generate Queries"},{"location":"develop/extend-result-storages/","text":"Extend Result Storages If you want to use a different storage than RDF you can extend the storages However it is highly optimized for RDF so we suggest to work on top of the TripleBasedStorage package org.benchmark.storage @Shorthand(\"MyStorage\") public class MyStorage extends TripleBasedStorage { @Override public void commit() { } @Override public String toString(){ return this.getClass().getSimpleName(); } } Commit This should take all the current results, store them and remove them from memory. You can access the results at the Jena Model this.metricResults . For example: @Override public void commit() { try (OutputStream os = new FileOutputStream(file.toString(), true)) { RDFDataMgr.write(os, metricResults, RDFFormat.NTRIPLES); metricResults.removeAll(); } catch (IOException e) { LOGGER.error(\"Could not commit to NTFileStorage.\", e); } } Constructor The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task .","title":"Result storage"},{"location":"develop/extend-result-storages/#extend-result-storages","text":"If you want to use a different storage than RDF you can extend the storages However it is highly optimized for RDF so we suggest to work on top of the TripleBasedStorage package org.benchmark.storage @Shorthand(\"MyStorage\") public class MyStorage extends TripleBasedStorage { @Override public void commit() { } @Override public String toString(){ return this.getClass().getSimpleName(); } }","title":"Extend Result Storages"},{"location":"develop/extend-result-storages/#commit","text":"This should take all the current results, store them and remove them from memory. You can access the results at the Jena Model this.metricResults . For example: @Override public void commit() { try (OutputStream os = new FileOutputStream(file.toString(), true)) { RDFDataMgr.write(os, metricResults, RDFFormat.NTRIPLES); metricResults.removeAll(); } catch (IOException e) { LOGGER.error(\"Could not commit to NTFileStorage.\", e); } }","title":"Commit"},{"location":"develop/extend-result-storages/#constructor","text":"The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task .","title":"Constructor"},{"location":"develop/extend-task/","text":"Extend Tasks You can extend Iguana with your benchmark task, if the Stresstest doesn't fit your needs. F.e. you may want to check systems if they answer correctly rather than stresstest them. You will need to create your own task either in the Iguana code itself or by using Iguana as a library. Either way start by extending the AbstractTask. package org.benchmark @Shorthand(\"MyBenchmarkTask\") public class MyBenchmarkTask extend AbstractTask { } You will need to override some functions. For now include them and go through them step by step package org.benchmark @Shorthand(\"MyBenchmarkTask\") public class MyBenchmarkTask extend AbstractTask { //Your constructor(s) public MyBenchmarkTask(Integer timeLimit, ArrayList workers, LinkedHashMap queryHandler) throws FileNotFoundException { } //Meta Data (which will be added in the resultsfile) @Override public void addMetaData() { super.addMetaData(); } //Initializing @Override public void init(String[] ids, String dataset, Connection connection) { super.init(ids, dataset, connection); } //Your actual Task @Override public void execute() { } //Closing the benchmark, freeing some stuff etc. @Override public void close() { super.close(); } } Constructor and Configuration Let's start with the Constructor. The YAML benchmark configuration will provide you the constructor parameters. Imagine you want to have three different parameters. The first one should provide an integer (e.g. the time limit of the task) The second one should provide a list of objects (e.g. a list of integers to use) The third parameter should provide a map of specific key-value pairs. You can set this up by using the following parameters: public MyBenchmarkTask(Integer param1, ArrayList param2, LinkedHashMap param3) throws FileNotFoundException { //TODO whatever you need to do with the parameters } Then Your configuration may look like the following ... className: \"MyBenchmarkTask\" configuration: param1: 123 param2: - \"1\" - \"2\" param3: val1: \"abc\" val2: 123 The parameters will then be matched by their names to the names of the parameters of your constructor, allowing multiple constructors These are the three types you can represent in a Yaml configuration. * Single Values * Lists of Objects * Key-Value Pairs Add Meta Data If you want to add Meta Data to be written in the results file do the following, Let noOfWorkers a value you already set. /** * Add extra Meta Data */ @Override public void addMetaData() { super.addMetaData(); Properties extraMeta = new Properties(); extraMeta.put(\"noOfWorkers\", noOfWorkers); //Adding them to the actual meta data this.metaData.put(COMMON.EXTRA_META_KEY, extraMeta); } Then the resultsfile will contain all the mappings you put in extraMeta. Initialize the Task You may want to initialize your task, set some more values, start something in the background etc. etc. You will be provided the suiteID , experimentID and the taskID in the ids array, as well as the name of the dataset and the connection currently beeing benchmarked. @Override public void init(String[] ids, String dataset, Connection connection) { super.init(ids, dataset, connection); //ADD YOUR CODE HERE } The ids, the dataset and the connection will be set in the AbstractTask which you can simply access by using this.connection for example. Execute Now you can create the actual benchmark task you want to use. @Override public void execute() { //ADD YOUR CODE HERE } Be aware that if you are using the workers implemented in Iguana, you need to stop them after your benchmark using the worker.stopSending() method. Close If you need to close some streams at the end of your benchmark task, you can do that in the close function. Simply override the existing one and call the super method and implement what you need. @Override public void close() { super.close(); } Full overview package org.benchmark @Shorthand(\"MyBenchmarkTask\") public class MyBenchmarkTask extend AbstractTask { private Integer param1; private ArrayList param2; private LinkedHashMap param3; //Your constructor(s) public MyBenchmarkTask(Integer param1, ArrayList param2, LinkedHashMap param3) throws FileNotFoundException { this.param1=param1; this.param2=param2; this.param3=param3; } //Meta Data (which will be added in the resultsfile) @Override public void addMetaData() { super.addMetaData(); Properties extraMeta = new Properties(); extraMeta.put(\"noOfWorkers\", noOfWorkers); //Adding them to the actual meta data this.metaData.put(COMMON.EXTRA_META_KEY, extraMeta); } @Override public void init(String[] ids, String dataset, Connection connection) { super.init(ids, dataset, connection); //ADD YOUR CODE HERE } @Override public void execute() { //ADD YOUR CODE HERE } //Closing the benchmark, freeing some stuff etc. @Override public void close() { super.close(); } }","title":"Tasks"},{"location":"develop/extend-task/#extend-tasks","text":"You can extend Iguana with your benchmark task, if the Stresstest doesn't fit your needs. F.e. you may want to check systems if they answer correctly rather than stresstest them. You will need to create your own task either in the Iguana code itself or by using Iguana as a library. Either way start by extending the AbstractTask. package org.benchmark @Shorthand(\"MyBenchmarkTask\") public class MyBenchmarkTask extend AbstractTask { } You will need to override some functions. For now include them and go through them step by step package org.benchmark @Shorthand(\"MyBenchmarkTask\") public class MyBenchmarkTask extend AbstractTask { //Your constructor(s) public MyBenchmarkTask(Integer timeLimit, ArrayList workers, LinkedHashMap queryHandler) throws FileNotFoundException { } //Meta Data (which will be added in the resultsfile) @Override public void addMetaData() { super.addMetaData(); } //Initializing @Override public void init(String[] ids, String dataset, Connection connection) { super.init(ids, dataset, connection); } //Your actual Task @Override public void execute() { } //Closing the benchmark, freeing some stuff etc. @Override public void close() { super.close(); } }","title":"Extend Tasks"},{"location":"develop/extend-task/#constructor-and-configuration","text":"Let's start with the Constructor. The YAML benchmark configuration will provide you the constructor parameters. Imagine you want to have three different parameters. The first one should provide an integer (e.g. the time limit of the task) The second one should provide a list of objects (e.g. a list of integers to use) The third parameter should provide a map of specific key-value pairs. You can set this up by using the following parameters: public MyBenchmarkTask(Integer param1, ArrayList param2, LinkedHashMap param3) throws FileNotFoundException { //TODO whatever you need to do with the parameters } Then Your configuration may look like the following ... className: \"MyBenchmarkTask\" configuration: param1: 123 param2: - \"1\" - \"2\" param3: val1: \"abc\" val2: 123 The parameters will then be matched by their names to the names of the parameters of your constructor, allowing multiple constructors These are the three types you can represent in a Yaml configuration. * Single Values * Lists of Objects * Key-Value Pairs","title":"Constructor and Configuration"},{"location":"develop/extend-task/#add-meta-data","text":"If you want to add Meta Data to be written in the results file do the following, Let noOfWorkers a value you already set. /** * Add extra Meta Data */ @Override public void addMetaData() { super.addMetaData(); Properties extraMeta = new Properties(); extraMeta.put(\"noOfWorkers\", noOfWorkers); //Adding them to the actual meta data this.metaData.put(COMMON.EXTRA_META_KEY, extraMeta); } Then the resultsfile will contain all the mappings you put in extraMeta.","title":"Add Meta Data"},{"location":"develop/extend-task/#initialize-the-task","text":"You may want to initialize your task, set some more values, start something in the background etc. etc. You will be provided the suiteID , experimentID and the taskID in the ids array, as well as the name of the dataset and the connection currently beeing benchmarked. @Override public void init(String[] ids, String dataset, Connection connection) { super.init(ids, dataset, connection); //ADD YOUR CODE HERE } The ids, the dataset and the connection will be set in the AbstractTask which you can simply access by using this.connection for example.","title":"Initialize the Task"},{"location":"develop/extend-task/#execute","text":"Now you can create the actual benchmark task you want to use. @Override public void execute() { //ADD YOUR CODE HERE } Be aware that if you are using the workers implemented in Iguana, you need to stop them after your benchmark using the worker.stopSending() method.","title":"Execute"},{"location":"develop/extend-task/#close","text":"If you need to close some streams at the end of your benchmark task, you can do that in the close function. Simply override the existing one and call the super method and implement what you need. @Override public void close() { super.close(); }","title":"Close"},{"location":"develop/extend-task/#full-overview","text":"package org.benchmark @Shorthand(\"MyBenchmarkTask\") public class MyBenchmarkTask extend AbstractTask { private Integer param1; private ArrayList param2; private LinkedHashMap param3; //Your constructor(s) public MyBenchmarkTask(Integer param1, ArrayList param2, LinkedHashMap param3) throws FileNotFoundException { this.param1=param1; this.param2=param2; this.param3=param3; } //Meta Data (which will be added in the resultsfile) @Override public void addMetaData() { super.addMetaData(); Properties extraMeta = new Properties(); extraMeta.put(\"noOfWorkers\", noOfWorkers); //Adding them to the actual meta data this.metaData.put(COMMON.EXTRA_META_KEY, extraMeta); } @Override public void init(String[] ids, String dataset, Connection connection) { super.init(ids, dataset, connection); //ADD YOUR CODE HERE } @Override public void execute() { //ADD YOUR CODE HERE } //Closing the benchmark, freeing some stuff etc. @Override public void close() { super.close(); } }","title":"Full overview"},{"location":"develop/extend-workers/","text":"Extend Workers If the implemented workers aren't sufficient you can create your own one. Start by extending the AbstractWorker package org.benchmark.workers @Shorthand(\"MyWorker\") public class MyWorker extends AbstractWorker{ //Setting the next query to be benchmarked in queryStr and queryID public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException{ } //Executing the current benchmark query public void executeQuery(String query, String queryID){ } } These are the only two functions you need to implement, the rest is done by the AbstractWorker . You can override more functions, please consider looking into the javadoc for that. Constructor The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task . Get the next query The benchmark task should create and initialize the benchmark queries and will set them accordingly to the worker. You can access these queries using the queryFileList array. Each element consists of one query set, containing the queryID/name and a list of one to several queries. In the following we will choose the next query set, counted by currentQueryID and use a random query of this. @Override public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException { // get next Query File and next random Query out of it. QuerySet currentQuery = this.queryFileList[this.currentQueryID++]; queryID.append(currentQuery.getName()); int queriesInSet = currentQuery.size(); int queryLine = queryChooser.nextInt(queriesInSet); queryStr.append(currentQuery.getQueryAtPos(queryLine)); // If there is no more query(Pattern) start from beginning. if (this.currentQueryID >= this.queryFileList.length) { this.currentQueryID = 0; } } Thats it. This exact method is implemented in the AbstractRandomQueryChooserWorker class and instead of extend the AbstractWorker class, you can also extend this and spare your time. However if you need another way like only executing one query and if there are no mery queries to test end the worker you can do so: @Override public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException { // If there is no more query(Pattern) start from beginning. if (this.currentQueryID >= this.queryFileList.length) { this.stopSending(); } // get next Query File and the first Query out of it. QuerySet currentQuery = this.queryFileList[this.currentQueryID++]; queryID.append(currentQuery.getName()); int queriesInSet = currentQuery.size(); queryStr.append(currentQuery.getQueryAtPos(0)); } Execute the current query Now you can execute the query against the current connection ( this.con ). As this is up to you how to do that, here is an example implementation for using HTTP Get. @Override public void executeQuery(String query, String queryID) { Instant start = Instant.now(); try { String qEncoded = URLEncoder.encode(query, \"UTF-8\"); String addChar = \"?\"; if (con.getEndpoint().contains(\"?\")) { addChar = \"&\"; } String url = con.getEndpoint() + addChar + parameter+\"=\" + qEncoded; HttpGet request = new HttpGet(url); RequestConfig requestConfig = RequestConfig.custom().setSocketTimeout(timeOut.intValue()) .setConnectTimeout(timeOut.intValue()).build(); if(this.responseType != null) request.setHeader(HttpHeaders.ACCEPT, this.responseType); request.setConfig(requestConfig); CloseableHttpClient client = HttpClients.createDefault(); CloseableHttpResponse response = client.execute(request, getAuthContext(con.getEndpoint())); // method to process the result in background super.processHttpResponse(queryID, start, client, response); } catch (Exception e) { LOGGER.warn(\"Worker[{{}} : {{}}]: Could not execute the following query\\n{{}}\\n due to\", this.workerType, this.workerID, query, e); super.addResults(new QueryExecutionStats(queryID, COMMON.QUERY_UNKNOWN_EXCEPTION, durationInMilliseconds(start, Instant.now()))); } }","title":"Workers"},{"location":"develop/extend-workers/#extend-workers","text":"If the implemented workers aren't sufficient you can create your own one. Start by extending the AbstractWorker package org.benchmark.workers @Shorthand(\"MyWorker\") public class MyWorker extends AbstractWorker{ //Setting the next query to be benchmarked in queryStr and queryID public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException{ } //Executing the current benchmark query public void executeQuery(String query, String queryID){ } } These are the only two functions you need to implement, the rest is done by the AbstractWorker . You can override more functions, please consider looking into the javadoc for that.","title":"Extend Workers"},{"location":"develop/extend-workers/#constructor","text":"The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task .","title":"Constructor"},{"location":"develop/extend-workers/#get-the-next-query","text":"The benchmark task should create and initialize the benchmark queries and will set them accordingly to the worker. You can access these queries using the queryFileList array. Each element consists of one query set, containing the queryID/name and a list of one to several queries. In the following we will choose the next query set, counted by currentQueryID and use a random query of this. @Override public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException { // get next Query File and next random Query out of it. QuerySet currentQuery = this.queryFileList[this.currentQueryID++]; queryID.append(currentQuery.getName()); int queriesInSet = currentQuery.size(); int queryLine = queryChooser.nextInt(queriesInSet); queryStr.append(currentQuery.getQueryAtPos(queryLine)); // If there is no more query(Pattern) start from beginning. if (this.currentQueryID >= this.queryFileList.length) { this.currentQueryID = 0; } } Thats it. This exact method is implemented in the AbstractRandomQueryChooserWorker class and instead of extend the AbstractWorker class, you can also extend this and spare your time. However if you need another way like only executing one query and if there are no mery queries to test end the worker you can do so: @Override public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException { // If there is no more query(Pattern) start from beginning. if (this.currentQueryID >= this.queryFileList.length) { this.stopSending(); } // get next Query File and the first Query out of it. QuerySet currentQuery = this.queryFileList[this.currentQueryID++]; queryID.append(currentQuery.getName()); int queriesInSet = currentQuery.size(); queryStr.append(currentQuery.getQueryAtPos(0)); }","title":"Get the next query"},{"location":"develop/extend-workers/#execute-the-current-query","text":"Now you can execute the query against the current connection ( this.con ). As this is up to you how to do that, here is an example implementation for using HTTP Get. @Override public void executeQuery(String query, String queryID) { Instant start = Instant.now(); try { String qEncoded = URLEncoder.encode(query, \"UTF-8\"); String addChar = \"?\"; if (con.getEndpoint().contains(\"?\")) { addChar = \"&\"; } String url = con.getEndpoint() + addChar + parameter+\"=\" + qEncoded; HttpGet request = new HttpGet(url); RequestConfig requestConfig = RequestConfig.custom().setSocketTimeout(timeOut.intValue()) .setConnectTimeout(timeOut.intValue()).build(); if(this.responseType != null) request.setHeader(HttpHeaders.ACCEPT, this.responseType); request.setConfig(requestConfig); CloseableHttpClient client = HttpClients.createDefault(); CloseableHttpResponse response = client.execute(request, getAuthContext(con.getEndpoint())); // method to process the result in background super.processHttpResponse(queryID, start, client, response); } catch (Exception e) { LOGGER.warn(\"Worker[{{}} : {{}}]: Could not execute the following query\\n{{}}\\n due to\", this.workerType, this.workerID, query, e); super.addResults(new QueryExecutionStats(queryID, COMMON.QUERY_UNKNOWN_EXCEPTION, durationInMilliseconds(start, Instant.now()))); } }","title":"Execute the current query"},{"location":"develop/how-to-start/","text":"","title":"How to start"},{"location":"develop/maven/","text":"Use Iguana as a Maven dependency Iguana provides 3 packages iguana.commons which consists of some helper classes. iguana.resultprocessor which consists of metrics and the result storage workflow and iguana.corecontroller which contains the tasks, the workers, the query handlers, and the overall Iguana workflow to use one of these packages in your maven project add the following repository to your pom: <repository> <id>iguana-github</id> <name>Iguana Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Iguana</url> </repository> Afterwards add the package you want to add using the following, for the core controller, which will also include the result processor as well as the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.corecontroller</artifactId> <version>${iguana-version}</version> </dependency> for the result processor which will also include the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.resultprocessor</artifactId> <version>${iguana-version}</version> </dependency> or for the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.commons</artifactId> <version>${iguana-version}</version> </dependency>","title":"Maven"},{"location":"develop/maven/#use-iguana-as-a-maven-dependency","text":"Iguana provides 3 packages iguana.commons which consists of some helper classes. iguana.resultprocessor which consists of metrics and the result storage workflow and iguana.corecontroller which contains the tasks, the workers, the query handlers, and the overall Iguana workflow to use one of these packages in your maven project add the following repository to your pom: <repository> <id>iguana-github</id> <name>Iguana Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Iguana</url> </repository> Afterwards add the package you want to add using the following, for the core controller, which will also include the result processor as well as the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.corecontroller</artifactId> <version>${iguana-version}</version> </dependency> for the result processor which will also include the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.resultprocessor</artifactId> <version>${iguana-version}</version> </dependency> or for the commons. <dependency> <groupId>org.aksw</groupId> <artifactId>iguana.commons</artifactId> <version>${iguana-version}</version> </dependency>","title":"Use Iguana as a Maven dependency"},{"location":"develop/overview/","text":"Development Overview Iguana is open source and available at Github here . There are two main options to work on Iguana. Fork the git repository and work directly on Iguana or use the Iguana Maven Packages as a library Iguana is a benchmark framework which can be extended to fit your needs. Extend There are several things you can extend in Iguana. Tasks - Add your benchmark task Workers - Your system won't work with HTTP GET or POST, or work completely different? Add your specific worker. Query Handling - You do not use Plain Text queries or SPARQL? Add your query handler. Language - Want more statistics about your specific queries? The result size isn't accurate? add your language support Result Storage - Don't want to use RDF? Add your own solution to store the benchmark results. Metrics - The metrics won't fit your needs? Add your own. Bugs For bugs please open an issue at our Github Issue Tracker","title":"Overview"},{"location":"develop/overview/#development-overview","text":"Iguana is open source and available at Github here . There are two main options to work on Iguana. Fork the git repository and work directly on Iguana or use the Iguana Maven Packages as a library Iguana is a benchmark framework which can be extended to fit your needs.","title":"Development Overview"},{"location":"develop/overview/#extend","text":"There are several things you can extend in Iguana. Tasks - Add your benchmark task Workers - Your system won't work with HTTP GET or POST, or work completely different? Add your specific worker. Query Handling - You do not use Plain Text queries or SPARQL? Add your query handler. Language - Want more statistics about your specific queries? The result size isn't accurate? add your language support Result Storage - Don't want to use RDF? Add your own solution to store the benchmark results. Metrics - The metrics won't fit your needs? Add your own.","title":"Extend"},{"location":"develop/overview/#bugs","text":"For bugs please open an issue at our Github Issue Tracker","title":"Bugs"},{"location":"usage/configuration/","text":"Configuration The Configuration explains Iguana how to execute your benchmark. It is divided into 5 categories Connections Datasets Tasks Storages Metrics Additionally a pre and post task script hook can be set. The configuration has to be either in YAML or JSON. Each section will be detailed out and shows configuration examples. At the end the full configuration will be shown. For this we will stick to the YAML format, however the equivalent JSON is also valid and can be parsed by Iguana. Connections Every benchmark suite can execute several connections (e.g. an HTTP endpoint, or a CLI application). A connection has the following items name - the name you want to give the connection, which will be saved in the results. endpoint - the HTTP endpoint or CLI call. updateEndpoint - If your HTTP endpoint is an HTTP Post endpoint set this to the post endpoint. (optional) user - for authentication purposes (optional) password - for authentication purposes (optional) version - setting the version of the tested triplestore, if set resource URI will be ires:name-version (optional) To setup an endpoint as well as an updateEndpoint might be confusing at first, but if you to test read and write performance simultanously and how updates might have an impact on read performance, you can set up both. For more detail on how to setup the CLI call look at Implemented Workers . There are all CLI Workers explained and how to set the endpoint such that the application will be run correctly. Let's look at an example: connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" version: 1.0-SNAP - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" Here we have two connections: System1 and System2. System1 is only setup to use an HTTP Get endpoint at http://localhost:8800/query. System2 however uses authentication and has an update endpoint as well, and thus will be correctly test with updates (POSTs) too. Datasets Pretty straight forward. You might want to test your system with different datasets (e.g. databases, triplestores etc.) If you system does not work on different datasets, just add one datasetname like datasets: - name: \"DoesNotMatter\" otherwise you might want to benchmark different datasets. Hence you can setup a Dataset Name, as well as file. The dataset name will be added to the results, whereas both can be used in the task script hooks, to automatize dataset load into your system. Let's look at an example: datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" Tasks A Task is one benchmark Task which will be executed against all connections for all datasets. A Task might be a stresstest which we will be using in this example. Have a look at the full configuration of the Stresstest The configuration of one Task consists of the following: className - The className or Shorthand configuration - The parameters of the task tasks: - className: \"YourTask\" configuration: parameter1: value1 parameter2: \"value2\" Let's look at an example: tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 We configured two Tasks, both Stresstests. The first one will be executed for one hour and uses simple text queries which can be executed right away. Further on it uses 2 simulated SPARQLWorkers with the same configuration. At this point it's recommend to check out the Stresstest Configuration in detail for further configuration. Storages Tells Iguana how to save your results. Currently Iguana supports two solutions NTFileStorage - will save your results into one NTriple File. RDFFileStorage - will save your results into an RDF File (default TURTLE). TriplestoreStorage - Will upload the results into a specified Triplestore This is optional. The default storage is NTFileStorage . NTFileStorage can be setup by just stating to use it like storages: - className: \"NTFileStorage\" However it can be configured to use a different result file name. The default is results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt . See example below. storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.nt\" The RDFFileStorage is similar to the NTFileStorage but will determine the RDF format from the file extension To use RDF/XML f.e. you would end the file on .rdf, for TURTLE end it on .ttl storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.rdf\" The TriplestoreStorage can be configured as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" if you triple store uses authentication you can set that up as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" user: \"UserName\" password: \"secret\" For further detail on how to read the results have a look here Metrics Let's Iguana know what Metrics you want to include in the results. Iguana supports the following metrics: Queries Per Second (QPS) Average Queries Per Second (AvgQPS) Query Mixes Per Hour (QMPH) Number of Queries successfully executed (NoQ) Number of Queries per Hour (NoQPH) Each query execution (EachQuery) - experimental For more detail on each of the metrics have a look at Metrics Let's look at an example: metrics: - className: \"QPS\" - className: \"AvgQPS\" - className: \"QMPH\" - className: \"NoQ\" - className: \"NoQPH\" In this case we use all the default metrics which would be included if you do not specify metrics in the configuration at all. However you can also just use a subset of these like the following: metrics: - className: \"NoQ\" - className: \"AvgQPS\" For more detail on how the results will include these metrics have a look at Results . Task script hooks To automatize the whole benchmark workflow, you can setup a script which will be executed before each task, as well as a script which will be executed after each task. To make it easier, the script can get the following values dataset.name - The current dataset name dataset.file - The current dataset file name if there is anyone connection - The current connection name connection.version - The current connection version, if no version is set -> {{connection.version}} taskID - The current taskID You can set each one of them as an argument using brackets like {{connection}} . Thus you can setup scripts which will start your system and load it with the correct dataset file beforehand and stop the system after every task. However these script hooks are completely optional. Let's look at an example: preScriptHook: \"/full/path/{{connection}}-{{connection.version}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" Full Example connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" metrics: - className: \"QMPH\" - className: \"QPS\" - className: \"NoQPH\" - className: \"NoQ\" - className: \"AvgQPS\" storages: - className: \"NTFileStorage\" #optional - configuration: fileName: \"results-of-my-benchmark.nt\" Shorthand A shorthand is a short name for a class in Iguana which can be used in the configuration instead of the complete class name: e.g. instead of storages: - className: \"org.aksw.iguana.rp.storage.impl.NTFileStorage\" you can use the shortname NTFileStorage: storages: - className: \"NTFileStorage\" For a full map of the Shorthands have a look at Shorthand-Mapping","title":"Configuration"},{"location":"usage/configuration/#configuration","text":"The Configuration explains Iguana how to execute your benchmark. It is divided into 5 categories Connections Datasets Tasks Storages Metrics Additionally a pre and post task script hook can be set. The configuration has to be either in YAML or JSON. Each section will be detailed out and shows configuration examples. At the end the full configuration will be shown. For this we will stick to the YAML format, however the equivalent JSON is also valid and can be parsed by Iguana.","title":"Configuration"},{"location":"usage/configuration/#connections","text":"Every benchmark suite can execute several connections (e.g. an HTTP endpoint, or a CLI application). A connection has the following items name - the name you want to give the connection, which will be saved in the results. endpoint - the HTTP endpoint or CLI call. updateEndpoint - If your HTTP endpoint is an HTTP Post endpoint set this to the post endpoint. (optional) user - for authentication purposes (optional) password - for authentication purposes (optional) version - setting the version of the tested triplestore, if set resource URI will be ires:name-version (optional) To setup an endpoint as well as an updateEndpoint might be confusing at first, but if you to test read and write performance simultanously and how updates might have an impact on read performance, you can set up both. For more detail on how to setup the CLI call look at Implemented Workers . There are all CLI Workers explained and how to set the endpoint such that the application will be run correctly. Let's look at an example: connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" version: 1.0-SNAP - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" Here we have two connections: System1 and System2. System1 is only setup to use an HTTP Get endpoint at http://localhost:8800/query. System2 however uses authentication and has an update endpoint as well, and thus will be correctly test with updates (POSTs) too.","title":"Connections"},{"location":"usage/configuration/#datasets","text":"Pretty straight forward. You might want to test your system with different datasets (e.g. databases, triplestores etc.) If you system does not work on different datasets, just add one datasetname like datasets: - name: \"DoesNotMatter\" otherwise you might want to benchmark different datasets. Hence you can setup a Dataset Name, as well as file. The dataset name will be added to the results, whereas both can be used in the task script hooks, to automatize dataset load into your system. Let's look at an example: datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\"","title":"Datasets"},{"location":"usage/configuration/#tasks","text":"A Task is one benchmark Task which will be executed against all connections for all datasets. A Task might be a stresstest which we will be using in this example. Have a look at the full configuration of the Stresstest The configuration of one Task consists of the following: className - The className or Shorthand configuration - The parameters of the task tasks: - className: \"YourTask\" configuration: parameter1: value1 parameter2: \"value2\" Let's look at an example: tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 We configured two Tasks, both Stresstests. The first one will be executed for one hour and uses simple text queries which can be executed right away. Further on it uses 2 simulated SPARQLWorkers with the same configuration. At this point it's recommend to check out the Stresstest Configuration in detail for further configuration.","title":"Tasks"},{"location":"usage/configuration/#storages","text":"Tells Iguana how to save your results. Currently Iguana supports two solutions NTFileStorage - will save your results into one NTriple File. RDFFileStorage - will save your results into an RDF File (default TURTLE). TriplestoreStorage - Will upload the results into a specified Triplestore This is optional. The default storage is NTFileStorage . NTFileStorage can be setup by just stating to use it like storages: - className: \"NTFileStorage\" However it can be configured to use a different result file name. The default is results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt . See example below. storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.nt\" The RDFFileStorage is similar to the NTFileStorage but will determine the RDF format from the file extension To use RDF/XML f.e. you would end the file on .rdf, for TURTLE end it on .ttl storages: - className: \"NTFileStorage\" #optional configuration: fileName: \"results-of-my-benchmark.rdf\" The TriplestoreStorage can be configured as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" if you triple store uses authentication you can set that up as follows: storages: - className: TriplestoreStorage configuration: endpoint: \"http://localhost:9999/sparql\" updateEndpoint: \"http://localhost:9999/update\" user: \"UserName\" password: \"secret\" For further detail on how to read the results have a look here","title":"Storages"},{"location":"usage/configuration/#metrics","text":"Let's Iguana know what Metrics you want to include in the results. Iguana supports the following metrics: Queries Per Second (QPS) Average Queries Per Second (AvgQPS) Query Mixes Per Hour (QMPH) Number of Queries successfully executed (NoQ) Number of Queries per Hour (NoQPH) Each query execution (EachQuery) - experimental For more detail on each of the metrics have a look at Metrics Let's look at an example: metrics: - className: \"QPS\" - className: \"AvgQPS\" - className: \"QMPH\" - className: \"NoQ\" - className: \"NoQPH\" In this case we use all the default metrics which would be included if you do not specify metrics in the configuration at all. However you can also just use a subset of these like the following: metrics: - className: \"NoQ\" - className: \"AvgQPS\" For more detail on how the results will include these metrics have a look at Results .","title":"Metrics"},{"location":"usage/configuration/#task-script-hooks","text":"To automatize the whole benchmark workflow, you can setup a script which will be executed before each task, as well as a script which will be executed after each task. To make it easier, the script can get the following values dataset.name - The current dataset name dataset.file - The current dataset file name if there is anyone connection - The current connection name connection.version - The current connection version, if no version is set -> {{connection.version}} taskID - The current taskID You can set each one of them as an argument using brackets like {{connection}} . Thus you can setup scripts which will start your system and load it with the correct dataset file beforehand and stop the system after every task. However these script hooks are completely optional. Let's look at an example: preScriptHook: \"/full/path/{{connection}}-{{connection.version}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\"","title":"Task script hooks"},{"location":"usage/configuration/#full-example","text":"connections: - name: \"System1\" endpoint: \"http://localhost:8800/query\" - name: \"System2\" endpoint: \"http://localhost:8802/query\" updateEndpoint: \"http://localhost:8802/update\" user: \"testuser\" password: \"secret\" datasets: - name: \"DatasetName\" file: \"your-data-base.nt\" - name: \"Dataset2\" tasks: - className: \"Stresstest\" configuration: #timeLimit is in ms timeLimit: 3600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 - className: \"Stresstest\" configuration: noOfQueryMixes: 1 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"queries.txt\" timeOut: 180000 preScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\" postScriptHook: \"/full/path/{{connection}}/stop.sh\" metrics: - className: \"QMPH\" - className: \"QPS\" - className: \"NoQPH\" - className: \"NoQ\" - className: \"AvgQPS\" storages: - className: \"NTFileStorage\" #optional - configuration: fileName: \"results-of-my-benchmark.nt\"","title":"Full Example"},{"location":"usage/configuration/#shorthand","text":"A shorthand is a short name for a class in Iguana which can be used in the configuration instead of the complete class name: e.g. instead of storages: - className: \"org.aksw.iguana.rp.storage.impl.NTFileStorage\" you can use the shortname NTFileStorage: storages: - className: \"NTFileStorage\" For a full map of the Shorthands have a look at Shorthand-Mapping","title":"Shorthand"},{"location":"usage/getting-started/","text":"What is Iguana Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. What can be benchmarked Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query What Benchmarks are possible Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint. Download Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki. Start a Benchmark Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.3.0.jar example-suite.yml","title":"Getting started"},{"location":"usage/getting-started/#what-is-iguana","text":"Iguana is a HTTP and CLI read/write performance benchmark framework suite. It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries. Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line.","title":"What is Iguana"},{"location":"usage/getting-started/#what-can-be-benchmarked","text":"Iguana is capable of benchmarking and stresstesting the following applications HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints) CLI Applications which either exit after every query or awaiting input after each query","title":"What can be benchmarked"},{"location":"usage/getting-started/#what-benchmarks-are-possible","text":"Every simulated User (named Worker in the following) gets a set of queries. These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark. Iguana will then let every Worker execute these queries against the endpoint.","title":"What Benchmarks are possible"},{"location":"usage/getting-started/#download","text":"Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest The zip file contains 3 files. iguana-corecontroller-x.y.z.jar example-suite.yml start.sh The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.","title":"Download"},{"location":"usage/getting-started/#start-a-benchmark","text":"Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script ./start-iguana.sh example-suite.yml or using java 11 if you want to give Iguana more RAM or in general set JVM options. java -jar iguana-corecontroller-3.3.0.jar example-suite.yml","title":"Start a Benchmark"},{"location":"usage/languages/","text":"Supported Languages The Language tag is set to assure that the result size returned by the benchmarked system is correctly read and that result can give a little extra query statistics. Currently two languages are implemented, however you can use lang.SPARQL or simply ignore it all the way. If they are not in SPARQL the query statistics will be just containing the query text and the result size will be read as if each returned line were one result. Additionaly a SIMPLE language tag is added which parses nothing and sets the result size as the content length of the results. If you work with results which have a content length >=2GB please use lang.SIMPLE , as lang.SPARQL and lang.RDF cannot work with results >=2GB at the moment. The 3 languages are: lang.SPARQL lang.RDF lang.SIMPLE","title":"Supported Languages"},{"location":"usage/languages/#supported-languages","text":"The Language tag is set to assure that the result size returned by the benchmarked system is correctly read and that result can give a little extra query statistics. Currently two languages are implemented, however you can use lang.SPARQL or simply ignore it all the way. If they are not in SPARQL the query statistics will be just containing the query text and the result size will be read as if each returned line were one result. Additionaly a SIMPLE language tag is added which parses nothing and sets the result size as the content length of the results. If you work with results which have a content length >=2GB please use lang.SIMPLE , as lang.SPARQL and lang.RDF cannot work with results >=2GB at the moment. The 3 languages are: lang.SPARQL lang.RDF lang.SIMPLE","title":"Supported Languages"},{"location":"usage/metrics/","text":"Implemented Metrics Every metric will be calculated globally (for one Experiment Task) and locally (for each Worker) Hence you can just analyze the overall metrics or if you want to look closer, you can look at each worker. NoQ The number of successfully executed Queries QMPH The number of executed Query Mixes Per Hour NoQPH The number of successfully executed Number of Queries Per Hour QPS For each query the queries per second , the total time in ms (summed up time of each execution), the no of succeeded and failed executions and the result size will be saved. Additionaly will try to tell how many times a query failed with what reason. ( timeout , wrong return code e.g. 400, or unknown ) Further on the QPS metrics provides a penalized QPS which penalizes queries which will fail. As some systems who cannot resolve a query just returns an error code and thus can have a very high score, even though they could only handle a few queries it would be rather unfair to the compared systems. Thus we introduced the penalty QPS. It is calculated the same as the QPS score, but for each failed query it uses the penalty instead of the actual time the failed query took. The default is set to the timeOut of the task. However you can override it as follows: metrics: - className: \"QPS\" configuration: #in MS penality: 10000 AvgQPS The average of all queries per second. Also adding a penalizedAvgQPS. Default penalty is timeOut, can be overwritten as follows: metrics: - className: \"AvgQPS\" confiugration: # in ms penalty: 10000 EachQuery Will save every query execution. (Experimental)","title":"Metrics"},{"location":"usage/metrics/#implemented-metrics","text":"Every metric will be calculated globally (for one Experiment Task) and locally (for each Worker) Hence you can just analyze the overall metrics or if you want to look closer, you can look at each worker.","title":"Implemented Metrics"},{"location":"usage/metrics/#noq","text":"The number of successfully executed Queries","title":"NoQ"},{"location":"usage/metrics/#qmph","text":"The number of executed Query Mixes Per Hour","title":"QMPH"},{"location":"usage/metrics/#noqph","text":"The number of successfully executed Number of Queries Per Hour","title":"NoQPH"},{"location":"usage/metrics/#qps","text":"For each query the queries per second , the total time in ms (summed up time of each execution), the no of succeeded and failed executions and the result size will be saved. Additionaly will try to tell how many times a query failed with what reason. ( timeout , wrong return code e.g. 400, or unknown ) Further on the QPS metrics provides a penalized QPS which penalizes queries which will fail. As some systems who cannot resolve a query just returns an error code and thus can have a very high score, even though they could only handle a few queries it would be rather unfair to the compared systems. Thus we introduced the penalty QPS. It is calculated the same as the QPS score, but for each failed query it uses the penalty instead of the actual time the failed query took. The default is set to the timeOut of the task. However you can override it as follows: metrics: - className: \"QPS\" configuration: #in MS penality: 10000","title":"QPS"},{"location":"usage/metrics/#avgqps","text":"The average of all queries per second. Also adding a penalizedAvgQPS. Default penalty is timeOut, can be overwritten as follows: metrics: - className: \"AvgQPS\" confiugration: # in ms penalty: 10000","title":"AvgQPS"},{"location":"usage/metrics/#eachquery","text":"Will save every query execution. (Experimental)","title":"EachQuery"},{"location":"usage/queries/","text":"Supported Queries There are currently two query types supported: plain text queries SPARQL pattern queries Plain Text Queries This can be anything: SPARQL, SQL, a whole book if you need to. The only limitation is that it has to fit in one line per query. If that isn't possible use the Multiple Line Plain Text Queries . Every query can be executed as is. This can be set using the following: ... queryHandler: className: \"InstancesQueryHandler\" SPARQL Pattern Queries This only works for SPARQL Queries at the moment. The idea came from the DBpedia SPARQL Benchmark paper from 2011 and 2012. Instead of SPARQL queries as they are, you can set variables, which will be exchanged with real data. Hence Iguana can create thousands of queries using a SPARQL pattern query. A pattern query might look like the following: SELECT * {?s rdf:type %%var0%% ; %%var1%% %%var2%%. %%var2%% ?p ?o} This query in itself cannot be send to a triple store, however we can exchange the variables using real data. Thus we need a reference endpoint (ideally) containing the same data as the dataset which will be tested. This query will then be exchanged to SELECT ?var0 ?var1 ?var2 {?s rdf:type ?var0 ; ?var1 ?var2. ?var2 ?p ?o} LIMIT 2000 and be queried against the reference endpoint. For each result (limited to 2000) a query instance will be created. This will be done for every query in the benchmark queries. All instances of these query patterns will be subsummed as if they were one query in the results. This can be set using the following: ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" or ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" limit: 4000 Multiple Line Plain Text Queries Basically like Plain Text Queries. However allows queries which need more than one line. You basically seperate queries using a delimiter line. Let's look at an example, where the delimiter line is simply an empty line (this is the default) QUERY 1 { still query 1 } QUERY 2 { still Query2 } however if you set the delim= ### for example the file has to look like: QUERY 1 { still query 1 } ### QUERY 2 { still Query2 } The delimiter query handler can be set as follows ... queryHandler: className: \"DelimInstancesQueryHandler\" or if you want to set the delimiter line ... queryHandler: className: \"DelimInstancesQueryHandler\" delim: \"###\"","title":"Supported Queries"},{"location":"usage/queries/#supported-queries","text":"There are currently two query types supported: plain text queries SPARQL pattern queries","title":"Supported Queries"},{"location":"usage/queries/#plain-text-queries","text":"This can be anything: SPARQL, SQL, a whole book if you need to. The only limitation is that it has to fit in one line per query. If that isn't possible use the Multiple Line Plain Text Queries . Every query can be executed as is. This can be set using the following: ... queryHandler: className: \"InstancesQueryHandler\"","title":"Plain Text Queries"},{"location":"usage/queries/#sparql-pattern-queries","text":"This only works for SPARQL Queries at the moment. The idea came from the DBpedia SPARQL Benchmark paper from 2011 and 2012. Instead of SPARQL queries as they are, you can set variables, which will be exchanged with real data. Hence Iguana can create thousands of queries using a SPARQL pattern query. A pattern query might look like the following: SELECT * {?s rdf:type %%var0%% ; %%var1%% %%var2%%. %%var2%% ?p ?o} This query in itself cannot be send to a triple store, however we can exchange the variables using real data. Thus we need a reference endpoint (ideally) containing the same data as the dataset which will be tested. This query will then be exchanged to SELECT ?var0 ?var1 ?var2 {?s rdf:type ?var0 ; ?var1 ?var2. ?var2 ?p ?o} LIMIT 2000 and be queried against the reference endpoint. For each result (limited to 2000) a query instance will be created. This will be done for every query in the benchmark queries. All instances of these query patterns will be subsummed as if they were one query in the results. This can be set using the following: ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" or ... queryHandler: className: \"PatternQueryHandler\" endpoint: \"http://your-reference-endpoint/sparql\" limit: 4000","title":"SPARQL Pattern Queries"},{"location":"usage/queries/#multiple-line-plain-text-queries","text":"Basically like Plain Text Queries. However allows queries which need more than one line. You basically seperate queries using a delimiter line. Let's look at an example, where the delimiter line is simply an empty line (this is the default) QUERY 1 { still query 1 } QUERY 2 { still Query2 } however if you set the delim= ### for example the file has to look like: QUERY 1 { still query 1 } ### QUERY 2 { still Query2 } The delimiter query handler can be set as follows ... queryHandler: className: \"DelimInstancesQueryHandler\" or if you want to set the delimiter line ... queryHandler: className: \"DelimInstancesQueryHandler\" delim: \"###\"","title":"Multiple Line Plain Text Queries"},{"location":"usage/results/","text":"Experiment Results Fundamentals The results are saved into RDF. For those who don't know what RDF is, it is best described as a way to represent a directed graph. The according query language is called SPARQL. The graph schema of an iguana result is shown above, where as each node represents a class object containg several annotations. To retrieve all TaskIDs you can do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . } Let's look at an example to clarify how to request the global NoQ metric for a taskID you already know. Let's assume the taskID is 123/1/1 PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?noq { ires:123/1/1 iprop:NoQ ?noq } If you want to get all the local worker NoQ metrics do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?workerID ?noq { ires:123/1/1 iprop:workerResult ?workerID ?workerID iprop:NoQ ?noq } However if you just want to see the global NoQ metric for all taskIDs in your results do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . ?taskID iprop:NoQ ?noq. } Instead of the NoQ metric you can do this for all other metrics, except QPS . To retrieve QPS look above in the results schema and let's look at an example. Let's assume the taskID is 123/1/1 again. You can retrieve the global qps values (seen above in ExecutedQueries, e.g QPS , succeeded etc.) as follows, PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?failed ?resultSize { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:failed ?failed . ?executedQuery iprop:resultSize ?resultSize . } This will get you the QPS value, the no. of failed queries and the result size of the query. Further on you can show the dataset and connection names. PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?taskID ?datasetLabel ?connectionLabel ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:dataset ?dataset . ?dataset rdfs:label ?datasetLabel ?expID iprop:task ?taskID . ?taskID iprop:connection ?connection. ?connection rdfs:label ?connectionLabel . ?taskID iprop:NoQ ?noq. } This query will show a table containing for each task, the taskID, the dataset name, the connection name and the no. of queries succesfully executed. SPARQL Query statistics If you were using SPARQL queries as your benchmark queries you can add addtional further statistics of a query, such as: does the query has a FILTER. PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?hasFilter ?queryText { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:queryID ?query . ?query iprop:filter ?hasFilter . ?query rdfs:label ?queryText . } This provides the qps value, if the SPARQL query has a filter and the actual query string. Ontology The results ontology (description of what each property and class means) can be found here Adding LSQ analyzation If you're using SPARQL and want some more indepth analysation of the query statistics, you can use LSQ to do so. Iguana will add an owl:sameAs link between the SPARQL queries used in your benchmark and the equivalent LSQ query links. Hence you can run the performance measurement using Iguana and the query analyzation using LSQ independently and combine both results afterwards","title":"Benchmark Results"},{"location":"usage/results/#experiment-results","text":"","title":"Experiment Results"},{"location":"usage/results/#fundamentals","text":"The results are saved into RDF. For those who don't know what RDF is, it is best described as a way to represent a directed graph. The according query language is called SPARQL. The graph schema of an iguana result is shown above, where as each node represents a class object containg several annotations. To retrieve all TaskIDs you can do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . } Let's look at an example to clarify how to request the global NoQ metric for a taskID you already know. Let's assume the taskID is 123/1/1 PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?noq { ires:123/1/1 iprop:NoQ ?noq } If you want to get all the local worker NoQ metrics do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?workerID ?noq { ires:123/1/1 iprop:workerResult ?workerID ?workerID iprop:NoQ ?noq } However if you just want to see the global NoQ metric for all taskIDs in your results do the following: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?taskID ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:task ?taskID . ?taskID iprop:NoQ ?noq. } Instead of the NoQ metric you can do this for all other metrics, except QPS . To retrieve QPS look above in the results schema and let's look at an example. Let's assume the taskID is 123/1/1 again. You can retrieve the global qps values (seen above in ExecutedQueries, e.g QPS , succeeded etc.) as follows, PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?failed ?resultSize { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:failed ?failed . ?executedQuery iprop:resultSize ?resultSize . } This will get you the QPS value, the no. of failed queries and the result size of the query. Further on you can show the dataset and connection names. PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?taskID ?datasetLabel ?connectionLabel ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:dataset ?dataset . ?dataset rdfs:label ?datasetLabel ?expID iprop:task ?taskID . ?taskID iprop:connection ?connection. ?connection rdfs:label ?connectionLabel . ?taskID iprop:NoQ ?noq. } This query will show a table containing for each task, the taskID, the dataset name, the connection name and the no. of queries succesfully executed.","title":"Fundamentals"},{"location":"usage/results/#sparql-query-statistics","text":"If you were using SPARQL queries as your benchmark queries you can add addtional further statistics of a query, such as: does the query has a FILTER. PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> SELECT ?executedQuery ?qps ?hasFilter ?queryText { ires:123/1/1 iprop:query ?executedQuery . ?executedQuery iprop:QPS ?qps. ?executedQuery iprop:queryID ?query . ?query iprop:filter ?hasFilter . ?query rdfs:label ?queryText . } This provides the qps value, if the SPARQL query has a filter and the actual query string.","title":"SPARQL Query statistics"},{"location":"usage/results/#ontology","text":"The results ontology (description of what each property and class means) can be found here","title":"Ontology"},{"location":"usage/results/#adding-lsq-analyzation","text":"If you're using SPARQL and want some more indepth analysation of the query statistics, you can use LSQ to do so. Iguana will add an owl:sameAs link between the SPARQL queries used in your benchmark and the equivalent LSQ query links. Hence you can run the performance measurement using Iguana and the query analyzation using LSQ independently and combine both results afterwards","title":"Adding LSQ analyzation"},{"location":"usage/stresstest/","text":"Stresstest Iguanas implemented Stresstest benchmark task tries to emulate a real case scenario under which an endpoint or application is under high stress. As in real life endpoints might get multiple simultaneous request within seconds, it is very important to verify that you application can handle this. The stresstest emulates users or applications which will bombard the endpoint using a set of queries for a specific amount of time or a specific amount of queries executed. Each simulated user is called Worker in the following. As you might want to test read and write performance or just want to emulate different user behaviour, the stresstest allows to configure several workers. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query. Configuration To configure this task you have to first tell Iguana to use the implemented task like the following: tasks: - className: \"Stresstest\" Further on you need to configure the Stresstest using the configuration parameter like: tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 ... As an end restriction you can either use timeLimit which will stop the stresstest after the specified amount in ms or you can set noOfQueryMixes which stops every worker after they executed the amount of queries in the provided query set. Additionaly to either timeLimit or noOfQueryMixes you can set the following parameters queryHandler workers warmup (optional) Query Handling The queryHandler parameter let's the stresstest know what queries will be used. Normally you will need the InstancesQueryHandler which will use plain text queries (could be SQL, SPARQL, a whole RDF document). The only restriction is that each query has to be in one line. You can set the query handler like the following: tasks: - className: \"Stresstest\" queryHandler: className: \"InstancesQueryHandler\" ... To see which query handlers are supported see Supported Queries Workers (simulated Users) Further on you have to add which workers to use. As described above you can set different worker configurations. Let's look at an example: - className: \"Stresstest\" timeLimit: 600000 workers: - threads: 4 className: \"SPARQLWorker\" queriesFile: \"/path/to/your/queries.txt\" - threads: 16 className: \"SPARQLWorker\" queriesFile: \"/other/queries.txt\" fixedLatency: 5000 In this example we have two different worker configurations we want to use. The first want will create 4 SPARQLWorker s using queries at /path/to/your/queries.txt with any latencym thus every query will be executed immediatly after another. The second worker configuration will execute 16 SPARQLWorker s using queries at /other/queries.txt using a fixed waiting time of 5000ms between each query. Hence every worker will execute their queries independently from each other but will wait 5s after each of their query execution before executing the next one. This configuration may simulate that we have a few Users requesting your endpoint locally (e.g. some of your application relying on your database) and several users querying your endpoint from outside the network where we would have network latency and other interferences which we will try to simulate with 5s. A full list of supported workers and their parameters can be found at Supported Workers In this example our Stresstest would create 20 workers, which will simultaenously request the endpoint for 60000ms (10 minutes). Warmup Additionaly to these you can optionally set a warmup, which will aim to let the system be benchmarked under a normal situation (Some times a database is faster when it was already running for a bit) The configuration is similar to the stresstest itself you can set a timeLimit (however not a certain no of query executions), you can set different workers , and a queryHandler to use. If you don't set the queryHandler parameter the warmup will simply use the queryHandler specified in the Stresstest itself. You can set the Warmup as following: tasks: - className: \"Stresstest\" warmup: timeLimit: 600000 workers: ... queryHandler: ... That's it. A full example might look like this tasks: - className: \"Stresstest\" configuration: # 1 hour (time Limit is in ms) timeLimit: 3600000 # warmup is optional warmup: # 10 minutes (is in ms) timeLimit: 600000 # queryHandler could be set too, same as in the stresstest configuration, otherwise the same queryHandler will be use. # workers are set the same way as in the configuration part workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries_warmup.txt\" timeOut: 180000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 16 className: \"SPARQLWorker\" queriesFile: \"queries_easy.txt\" timeOut: 180000 - threads: 4 className: \"SPARQLWorker\" queriesFile: \"queries_complex.txt\" fixedLatency: 100 References Supported Queries Supported Workers","title":"Stresstest"},{"location":"usage/stresstest/#stresstest","text":"Iguanas implemented Stresstest benchmark task tries to emulate a real case scenario under which an endpoint or application is under high stress. As in real life endpoints might get multiple simultaneous request within seconds, it is very important to verify that you application can handle this. The stresstest emulates users or applications which will bombard the endpoint using a set of queries for a specific amount of time or a specific amount of queries executed. Each simulated user is called Worker in the following. As you might want to test read and write performance or just want to emulate different user behaviour, the stresstest allows to configure several workers. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query.","title":"Stresstest"},{"location":"usage/stresstest/#configuration","text":"To configure this task you have to first tell Iguana to use the implemented task like the following: tasks: - className: \"Stresstest\" Further on you need to configure the Stresstest using the configuration parameter like: tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 ... As an end restriction you can either use timeLimit which will stop the stresstest after the specified amount in ms or you can set noOfQueryMixes which stops every worker after they executed the amount of queries in the provided query set. Additionaly to either timeLimit or noOfQueryMixes you can set the following parameters queryHandler workers warmup (optional)","title":"Configuration"},{"location":"usage/stresstest/#query-handling","text":"The queryHandler parameter let's the stresstest know what queries will be used. Normally you will need the InstancesQueryHandler which will use plain text queries (could be SQL, SPARQL, a whole RDF document). The only restriction is that each query has to be in one line. You can set the query handler like the following: tasks: - className: \"Stresstest\" queryHandler: className: \"InstancesQueryHandler\" ... To see which query handlers are supported see Supported Queries","title":"Query Handling"},{"location":"usage/stresstest/#workers-simulated-users","text":"Further on you have to add which workers to use. As described above you can set different worker configurations. Let's look at an example: - className: \"Stresstest\" timeLimit: 600000 workers: - threads: 4 className: \"SPARQLWorker\" queriesFile: \"/path/to/your/queries.txt\" - threads: 16 className: \"SPARQLWorker\" queriesFile: \"/other/queries.txt\" fixedLatency: 5000 In this example we have two different worker configurations we want to use. The first want will create 4 SPARQLWorker s using queries at /path/to/your/queries.txt with any latencym thus every query will be executed immediatly after another. The second worker configuration will execute 16 SPARQLWorker s using queries at /other/queries.txt using a fixed waiting time of 5000ms between each query. Hence every worker will execute their queries independently from each other but will wait 5s after each of their query execution before executing the next one. This configuration may simulate that we have a few Users requesting your endpoint locally (e.g. some of your application relying on your database) and several users querying your endpoint from outside the network where we would have network latency and other interferences which we will try to simulate with 5s. A full list of supported workers and their parameters can be found at Supported Workers In this example our Stresstest would create 20 workers, which will simultaenously request the endpoint for 60000ms (10 minutes).","title":"Workers (simulated Users)"},{"location":"usage/stresstest/#warmup","text":"Additionaly to these you can optionally set a warmup, which will aim to let the system be benchmarked under a normal situation (Some times a database is faster when it was already running for a bit) The configuration is similar to the stresstest itself you can set a timeLimit (however not a certain no of query executions), you can set different workers , and a queryHandler to use. If you don't set the queryHandler parameter the warmup will simply use the queryHandler specified in the Stresstest itself. You can set the Warmup as following: tasks: - className: \"Stresstest\" warmup: timeLimit: 600000 workers: ... queryHandler: ... That's it. A full example might look like this tasks: - className: \"Stresstest\" configuration: # 1 hour (time Limit is in ms) timeLimit: 3600000 # warmup is optional warmup: # 10 minutes (is in ms) timeLimit: 600000 # queryHandler could be set too, same as in the stresstest configuration, otherwise the same queryHandler will be use. # workers are set the same way as in the configuration part workers: - threads: 1 className: \"SPARQLWorker\" queriesFile: \"queries_warmup.txt\" timeOut: 180000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 16 className: \"SPARQLWorker\" queriesFile: \"queries_easy.txt\" timeOut: 180000 - threads: 4 className: \"SPARQLWorker\" queriesFile: \"queries_complex.txt\" fixedLatency: 100","title":"Warmup"},{"location":"usage/stresstest/#references","text":"Supported Queries Supported Workers","title":"References"},{"location":"usage/tutorial/","text":"Tutorial In this tutorial we will go through one benchmark using two systems, two datasets and one Stresstest. We are using the following Iguana v3.0.2 Apache Jena Fuseki 3 Blazegraph Download First lets create a working directory mkdir myBenchmark cd myBenchmark Now let's download all required systems and Iguana. Starting with Iguana wget https://github.com/dice-group/IGUANA/releases/download/v3.0.2/iguana-3.0.2.zip unzip iguana-3.0.2.zip Now we will download Blazegraph mkdir blazegraph && cd blazegraph wget https://downloads.sourceforge.net/project/bigdata/bigdata/2.1.5/blazegraph.jar?r=https%3A%2F%2Fsourceforge.net%2Fprojects%2Fbigdata%2Ffiles%2Fbigdata%2F2.1.5%2Fblazegraph.jar%2Fdownload%3Fuse_mirror%3Dmaster%26r%3Dhttps%253A%252F%252Fwww.blazegraph.com%252Fdownload%252F%26use_mirror%3Dnetix&ts=1602007009 cd ../ At last we just need to download Apache Jena Fuseki and Apache Jena mkdir fuseki && cd fuseki wget https://downloads.apache.org/jena/binaries/apache-jena-3.16.0.zip unzip apache-jena-3.16.0.zip wget https://downloads.apache.org/jena/binaries/apache-jena-fuseki-3.16.0.zip unzip apache-jena-fuseki-3.16.0.zip Finally we have to download our datasets. We use two small datasets from scholarly data. The ISWC 2010 and the ekaw 2012 rich dataset. mkdir datasets/ cd datasets wget http://www.scholarlydata.org/dumps/conferences/alignments/iswc-2010-complete-alignments.rdf wget http://www.scholarlydata.org/dumps/conferences/alignments/ekaw-2012-complete-alignments.rdf cd .. That's it. Let's setup blazegraph and fuseki. Setting Up Systems To simplify the benchmark workflow we will use the pre and post script hook, in which we will load the current system and after the benchmark stop the system. Blazegraph First let's create the script files cd blazegraph touch load-and-start.sh touch stop.sh The load-and-start.sh script will start blazegraph and use curl to POST our dataset. In our case the datasets are pretty small, hence the loading time is minimal. Otherwise it would be wise to load the dataset beforehand, backup the blazegraph.jnl file and simply exchanging the file in the pre script hook. For now put this into the script load-and-start.sh #starting blazegraph with 4 GB ram cd ../blazegraph && java -Xmx4g -server -jar blazegraph.jar & #load the dataset file in, which will be set as the first script argument curl -X POST H 'Content-Type:application/rdf+xml' --data-binary '@$1' http://localhost:9999/blazegraph/sparql Now edit stop.sh and adding the following: pkill -f blazegraph Be aware that this kills all blazegraph instances, so make sure that no other process which includes the word blazegraph is running. finally get into the correct working directory again cd .. Fuseki Now the same for fuseki: cd fuseki touch load-and-start.sh touch stop.sh The load-and-start.sh script will load the dataset into a TDB directory and start fuseki using the directory. Edit the script load-and-start.sh as follows cd ../fuseki # load the dataset as a tdb directory apache-jena-3.16.0/bin/tdbloader2 --loc DB $1 # start fuseki apache-jena-fuseki-3.16.0/fuseki-server --loc DB /ds & To assure fairness and provide Fuseki with 4GB as well edit apache-jena-fuseki-3.16.0/fuseki-server and go to the last bit exchange the following JVM_ARGS=${JVM_ARGS:--Xmx1200M} to JVM_ARGS=${JVM_ARGS:--Xmx4G} Now edit stop.sh and adding the following: pkill -f fuseki Be aware that this kills all Fuseki instances, so make sure that no other process which includes the word fuseki is running. finally get into the correct working directory again cd .. Benchmark queries We need some queries to benchmark. For now we will just use 3 simple queryies SELECT * {?s ?p ?o} SELECT * {?s ?p ?o} LIMIT 10 SELECT * {?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o} save this to queries.txt Creating the Benchmark Configuration Now let's create the Iguana benchmark configuration. Create a file called benchmark-suite.yml touch benchmark-suite.yml Add the following subscections to this file, or simply go to #Full Configuration and add the whole piece to it. Be aware that the configuration will be started on directory level below our working directory and thus paths will use ../ to get the correct path. Datasets We have two datasets, the ekaw 2012 and the iswc 2010 datasets. Let's name them as such and set the file path, s.t. the script hooks can use the file paths. datasets: - name: \"ekaw-2012\" file: \"../datasets/ekaw-2012-complete-alignments.rdf\" - name: \"iswc-2010\" file: \"../datasets/iswc-2010-complete-alignments.rdf\" Connections We have two connections, blazegraph and fuseki with their respective endpoint at them as following: connections: - name: \"blazegraph\" endpoint: \"http://localhost:9999/blazegraph/sparql\" - name: \"fuseki\" endpoint: \"http://localhost:3030/ds/sparql\" Task script hooks To assure that the correct triple store will be loaded with the correct dataset add the following pre script hook ../{{connection}}/load-and-start.sh {{dataset.file}} {{connection}} will be set to the current benchmarked connection name (e.g. fuseki ) and the {{dataset.file}} will be set to the current dataset file path. For example the start script of fuseki is located at fuseki/load-and-start.sh . Further on add the stop.sh script as the post-script hook, assuring that the store will be stopped after each task This will look like this: pre-script-hook: \"../{{connection}}/load-and-start.sh {{dataset.file}}\" post-script-hook: \"../{{connection}}/stop.sh Task configuration We want to stresstest our stores using 10 minutes (60.000 ms)for each dataset connection pair. We are using plain text queries ( InstancesQueryHandler ) and want to have two simulated users querying SPARQL queries. The queries file is located at our working directory at queries.txt . Be aware that we start Iguana one level below, which makes the correct path ../queries.txt To achieve this restrictions add the following to your file tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"../queries.txt\" Result Storage Let's put the results as an NTriple file and for smootheness of this tutorial let's put it into the file my-first-iguana-results.nt Add the following to do this. storages: - className: \"NTFileStorage\" configuration: fileName: \"my-first-iguana-results.nt\" Full configuration datasets: - name: \"ekaw-2012\" file: \"../datasets/ekaw-2012-complete-alignments.rdf\" - name: \"iswc-2010\" file: \"../datasets/iswc-2010-complete-alignments.rdf\" connections: - name: \"blazegraph\" endpoint: \"http://localhost:9999/blazegraph/sparql\" - name: \"fuseki\" endpoint: \"http://localhost:3030/ds/sparql\" pre-script-hook: \"../{{connection}}/load-and-start.sh {{dataset.file}}\" post-script-hook: \"../{{connection}}/stop.sh tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"../queries.txt\" storages: - className: \"NTFileStorage\" configuration: fileName: \"my-first-iguana-results.nt\" Starting Benchmark Simply use the previous created benchmark-suite.yml and start with cd iguana/ ./start-iguana.sh ../benchmark-suite.yml Now we wait for 40 minutes until the benchmark is finished. Results As previously shown, our results will be shown in my-first-iguana-results.nt . Load this into a triple store of your choice and query for the results you want to use. Just use blazegraph for example: cd blazegraph ../load-and-start.sh ../my-first-iguana-results.nt To query the results go to http://localhost:9999/blazegraph/ . An example: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?taskID ?datasetLabel ?connectionLabel ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:dataset ?dataset . ?dataset rdfs:label ?datasetLabel ?expID iprop:task ?taskID . ?taskID iprop:connection ?connection. ?connection rdfs:label ?connectionLabel . ?taskID iprop:NoQ ?noq. } This will provide a list of all task, naming the dataset, the connection and the no. of queries which were succesfully executed We will however not go into detail on how to read the results. This can be read at Benchmark Results","title":"Tutorial"},{"location":"usage/tutorial/#tutorial","text":"In this tutorial we will go through one benchmark using two systems, two datasets and one Stresstest. We are using the following Iguana v3.0.2 Apache Jena Fuseki 3 Blazegraph","title":"Tutorial"},{"location":"usage/tutorial/#download","text":"First lets create a working directory mkdir myBenchmark cd myBenchmark Now let's download all required systems and Iguana. Starting with Iguana wget https://github.com/dice-group/IGUANA/releases/download/v3.0.2/iguana-3.0.2.zip unzip iguana-3.0.2.zip Now we will download Blazegraph mkdir blazegraph && cd blazegraph wget https://downloads.sourceforge.net/project/bigdata/bigdata/2.1.5/blazegraph.jar?r=https%3A%2F%2Fsourceforge.net%2Fprojects%2Fbigdata%2Ffiles%2Fbigdata%2F2.1.5%2Fblazegraph.jar%2Fdownload%3Fuse_mirror%3Dmaster%26r%3Dhttps%253A%252F%252Fwww.blazegraph.com%252Fdownload%252F%26use_mirror%3Dnetix&ts=1602007009 cd ../ At last we just need to download Apache Jena Fuseki and Apache Jena mkdir fuseki && cd fuseki wget https://downloads.apache.org/jena/binaries/apache-jena-3.16.0.zip unzip apache-jena-3.16.0.zip wget https://downloads.apache.org/jena/binaries/apache-jena-fuseki-3.16.0.zip unzip apache-jena-fuseki-3.16.0.zip Finally we have to download our datasets. We use two small datasets from scholarly data. The ISWC 2010 and the ekaw 2012 rich dataset. mkdir datasets/ cd datasets wget http://www.scholarlydata.org/dumps/conferences/alignments/iswc-2010-complete-alignments.rdf wget http://www.scholarlydata.org/dumps/conferences/alignments/ekaw-2012-complete-alignments.rdf cd .. That's it. Let's setup blazegraph and fuseki.","title":"Download"},{"location":"usage/tutorial/#setting-up-systems","text":"To simplify the benchmark workflow we will use the pre and post script hook, in which we will load the current system and after the benchmark stop the system.","title":"Setting Up Systems"},{"location":"usage/tutorial/#blazegraph","text":"First let's create the script files cd blazegraph touch load-and-start.sh touch stop.sh The load-and-start.sh script will start blazegraph and use curl to POST our dataset. In our case the datasets are pretty small, hence the loading time is minimal. Otherwise it would be wise to load the dataset beforehand, backup the blazegraph.jnl file and simply exchanging the file in the pre script hook. For now put this into the script load-and-start.sh #starting blazegraph with 4 GB ram cd ../blazegraph && java -Xmx4g -server -jar blazegraph.jar & #load the dataset file in, which will be set as the first script argument curl -X POST H 'Content-Type:application/rdf+xml' --data-binary '@$1' http://localhost:9999/blazegraph/sparql Now edit stop.sh and adding the following: pkill -f blazegraph Be aware that this kills all blazegraph instances, so make sure that no other process which includes the word blazegraph is running. finally get into the correct working directory again cd ..","title":"Blazegraph"},{"location":"usage/tutorial/#fuseki","text":"Now the same for fuseki: cd fuseki touch load-and-start.sh touch stop.sh The load-and-start.sh script will load the dataset into a TDB directory and start fuseki using the directory. Edit the script load-and-start.sh as follows cd ../fuseki # load the dataset as a tdb directory apache-jena-3.16.0/bin/tdbloader2 --loc DB $1 # start fuseki apache-jena-fuseki-3.16.0/fuseki-server --loc DB /ds & To assure fairness and provide Fuseki with 4GB as well edit apache-jena-fuseki-3.16.0/fuseki-server and go to the last bit exchange the following JVM_ARGS=${JVM_ARGS:--Xmx1200M} to JVM_ARGS=${JVM_ARGS:--Xmx4G} Now edit stop.sh and adding the following: pkill -f fuseki Be aware that this kills all Fuseki instances, so make sure that no other process which includes the word fuseki is running. finally get into the correct working directory again cd ..","title":"Fuseki"},{"location":"usage/tutorial/#benchmark-queries","text":"We need some queries to benchmark. For now we will just use 3 simple queryies SELECT * {?s ?p ?o} SELECT * {?s ?p ?o} LIMIT 10 SELECT * {?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o} save this to queries.txt","title":"Benchmark queries"},{"location":"usage/tutorial/#creating-the-benchmark-configuration","text":"Now let's create the Iguana benchmark configuration. Create a file called benchmark-suite.yml touch benchmark-suite.yml Add the following subscections to this file, or simply go to #Full Configuration and add the whole piece to it. Be aware that the configuration will be started on directory level below our working directory and thus paths will use ../ to get the correct path.","title":"Creating the Benchmark Configuration"},{"location":"usage/tutorial/#datasets","text":"We have two datasets, the ekaw 2012 and the iswc 2010 datasets. Let's name them as such and set the file path, s.t. the script hooks can use the file paths. datasets: - name: \"ekaw-2012\" file: \"../datasets/ekaw-2012-complete-alignments.rdf\" - name: \"iswc-2010\" file: \"../datasets/iswc-2010-complete-alignments.rdf\"","title":"Datasets"},{"location":"usage/tutorial/#connections","text":"We have two connections, blazegraph and fuseki with their respective endpoint at them as following: connections: - name: \"blazegraph\" endpoint: \"http://localhost:9999/blazegraph/sparql\" - name: \"fuseki\" endpoint: \"http://localhost:3030/ds/sparql\"","title":"Connections"},{"location":"usage/tutorial/#task-script-hooks","text":"To assure that the correct triple store will be loaded with the correct dataset add the following pre script hook ../{{connection}}/load-and-start.sh {{dataset.file}} {{connection}} will be set to the current benchmarked connection name (e.g. fuseki ) and the {{dataset.file}} will be set to the current dataset file path. For example the start script of fuseki is located at fuseki/load-and-start.sh . Further on add the stop.sh script as the post-script hook, assuring that the store will be stopped after each task This will look like this: pre-script-hook: \"../{{connection}}/load-and-start.sh {{dataset.file}}\" post-script-hook: \"../{{connection}}/stop.sh","title":"Task script hooks"},{"location":"usage/tutorial/#task-configuration","text":"We want to stresstest our stores using 10 minutes (60.000 ms)for each dataset connection pair. We are using plain text queries ( InstancesQueryHandler ) and want to have two simulated users querying SPARQL queries. The queries file is located at our working directory at queries.txt . Be aware that we start Iguana one level below, which makes the correct path ../queries.txt To achieve this restrictions add the following to your file tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"../queries.txt\"","title":"Task configuration"},{"location":"usage/tutorial/#result-storage","text":"Let's put the results as an NTriple file and for smootheness of this tutorial let's put it into the file my-first-iguana-results.nt Add the following to do this. storages: - className: \"NTFileStorage\" configuration: fileName: \"my-first-iguana-results.nt\"","title":"Result Storage"},{"location":"usage/tutorial/#full-configuration","text":"datasets: - name: \"ekaw-2012\" file: \"../datasets/ekaw-2012-complete-alignments.rdf\" - name: \"iswc-2010\" file: \"../datasets/iswc-2010-complete-alignments.rdf\" connections: - name: \"blazegraph\" endpoint: \"http://localhost:9999/blazegraph/sparql\" - name: \"fuseki\" endpoint: \"http://localhost:3030/ds/sparql\" pre-script-hook: \"../{{connection}}/load-and-start.sh {{dataset.file}}\" post-script-hook: \"../{{connection}}/stop.sh tasks: - className: \"Stresstest\" configuration: timeLimit: 600000 queryHandler: className: \"InstancesQueryHandler\" workers: - threads: 2 className: \"SPARQLWorker\" queriesFile: \"../queries.txt\" storages: - className: \"NTFileStorage\" configuration: fileName: \"my-first-iguana-results.nt\"","title":"Full configuration"},{"location":"usage/tutorial/#starting-benchmark","text":"Simply use the previous created benchmark-suite.yml and start with cd iguana/ ./start-iguana.sh ../benchmark-suite.yml Now we wait for 40 minutes until the benchmark is finished.","title":"Starting Benchmark"},{"location":"usage/tutorial/#results","text":"As previously shown, our results will be shown in my-first-iguana-results.nt . Load this into a triple store of your choice and query for the results you want to use. Just use blazegraph for example: cd blazegraph ../load-and-start.sh ../my-first-iguana-results.nt To query the results go to http://localhost:9999/blazegraph/ . An example: PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> PREFIX iprop: <http://iguana-benchmark.eu/properties/> PREFIX iont: <http://iguana-benchmark.eu/class/> PREFIX ires: <http://iguana-benchmark.eu/resource/> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?taskID ?datasetLabel ?connectionLabel ?noq { ?suiteID rdf:type iont:Suite . ?suiteID iprop:experiment ?expID . ?expID iprop:dataset ?dataset . ?dataset rdfs:label ?datasetLabel ?expID iprop:task ?taskID . ?taskID iprop:connection ?connection. ?connection rdfs:label ?connectionLabel . ?taskID iprop:NoQ ?noq. } This will provide a list of all task, naming the dataset, the connection and the no. of queries which were succesfully executed We will however not go into detail on how to read the results. This can be read at Benchmark Results","title":"Results"},{"location":"usage/workers/","text":"Supported Workers A Worker is basically just a thread querying the endpoint/application. It tries to emulate a single user/application requesting your system until it should stop. In a task (e.g. the stresstest ) you can configure several worker configurations which will then be used inside the task. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query. There a few workers implemented, which can be seperated into two main categories Http Workers CLI Workers Http Workers These Workers can be used to benchmark Http Applications (such as a SPARQL endpoint). Http Get Worker A Http worker using GET requests. This worker will use the endpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpGetWorker\" timeOut: 180000 parameterName: \"text\" This will use one HttpGetWOrker using a timeout of 3 minutes and the get parameter text to request the query through. Http Post Worker A Http worker using POST requests. This worker will use the updateEndpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) contentType yes text/plain The content type of the update queries. Setting the Content-Type: header responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpPostWorker\" timeOut: 180000 This will use one HttpGetWOrker using a timeout of 3 minutes. SPARQL Worker Simply a GET worker but the language parameter is set to lang.SPARQL . Otherwise see the Http Get Worker for configuration An Example: ... workers: - threads: 1 className: \"SPARQLWorker\" timeOut: 180000 SPARQL UPDATE Worker Simply a POST worker but specified for SPARQL Updates. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timerStrategy yes NONE NONE , FIXED or DISTRIBUTED . see below for explanation. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. The timerStrategy parameter let's the worker know how to distribute the updates. The fixedLatency and gaussianLatency parameters are not affected, the worker will wait those additionally. NONE: the worker just updates each update query after another FIXED: calculating the distribution by timeLimit / #updates at the start and waiting the amount between each update. Time Limit will be used of the task the worker is executed in. DISTRIBUTED: calculating the time to wait between two updates after each update by timeRemaining / #updatesRemaining . An Example: ... workers: - threads: 1 className: \"UPDATEWorker\" timeOut: 180000 timerStrategy: \"FIXED\" CLI Workers These workers can be used to benchmark a CLI application. CLI Worker This Worker should be used if the CLI application runs a query once and exits afterwards. Something like $ cli-script.sh query HEADER QUERY RESULT 1 QUERY RESULT 2 ... $ Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIWorker\" CLI Input Worker This Worker should be used if the CLI application runs and the query will be send using the Input. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" Multiple CLI Input Worker This Worker should be used if the CLI application runs and the query will be send using the Input and will quit on errors. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: ERROR ERROR happend, exiting $ To assure a smooth benchmark, the CLI application will be run multiple times instead of once, and if the application quits, the next running process will be used, while in the background the old process will be restarted. Thus as soon as an error happend, the benchmark can continue without a problem. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see above. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"MultipleCLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" CLI Input File Worker Same as the Multiple CLI Input Worker . However the query won't be send to the input but written to a file and the file will be send to the input Something like $ cli-script.sh start Your Input: file-containg-the-query.txt HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend directory no Directory in which the file including the query should be saved. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputFileWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" directory: \"/tmp/\" CLI Input Prefix Worker Same as the Multiple CLI Input Worker . However the CLI application might need a pre and suffix. Something like $ cli-script.sh start Your Input: PREFIX QUERY SUFFIX HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend queryPrefix no String to use as a PREFIX before the query. querySuffix no String to use as a SUFFIX after the query. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputPrefixWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" queryPrefix: \"SPARQL\" querySuffix: \";\" Will send the following as Input SPARQL QUERY ;","title":"Supported Workers"},{"location":"usage/workers/#supported-workers","text":"A Worker is basically just a thread querying the endpoint/application. It tries to emulate a single user/application requesting your system until it should stop. In a task (e.g. the stresstest ) you can configure several worker configurations which will then be used inside the task. Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time. However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query. There a few workers implemented, which can be seperated into two main categories Http Workers CLI Workers","title":"Supported Workers"},{"location":"usage/workers/#http-workers","text":"These Workers can be used to benchmark Http Applications (such as a SPARQL endpoint).","title":"Http Workers"},{"location":"usage/workers/#http-get-worker","text":"A Http worker using GET requests. This worker will use the endpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpGetWorker\" timeOut: 180000 parameterName: \"text\" This will use one HttpGetWOrker using a timeout of 3 minutes and the get parameter text to request the query through.","title":"Http Get Worker"},{"location":"usage/workers/#http-post-worker","text":"A Http worker using POST requests. This worker will use the updateEndpoint of the connection. This worker has several configurations listed in the following table: parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) contentType yes text/plain The content type of the update queries. Setting the Content-Type: header responseType yes The content type the endpoint should return. Setting the Accept: header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. Let's look at an example: ... workers: - threads: 1 className: \"HttpPostWorker\" timeOut: 180000 This will use one HttpGetWOrker using a timeout of 3 minutes.","title":"Http Post Worker"},{"location":"usage/workers/#sparql-worker","text":"Simply a GET worker but the language parameter is set to lang.SPARQL . Otherwise see the Http Get Worker for configuration An Example: ... workers: - threads: 1 className: \"SPARQLWorker\" timeOut: 180000","title":"SPARQL Worker"},{"location":"usage/workers/#sparql-update-worker","text":"Simply a POST worker but specified for SPARQL Updates. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timerStrategy yes NONE NONE , FIXED or DISTRIBUTED . see below for explanation. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. The timerStrategy parameter let's the worker know how to distribute the updates. The fixedLatency and gaussianLatency parameters are not affected, the worker will wait those additionally. NONE: the worker just updates each update query after another FIXED: calculating the distribution by timeLimit / #updates at the start and waiting the amount between each update. Time Limit will be used of the task the worker is executed in. DISTRIBUTED: calculating the time to wait between two updates after each update by timeRemaining / #updatesRemaining . An Example: ... workers: - threads: 1 className: \"UPDATEWorker\" timeOut: 180000 timerStrategy: \"FIXED\"","title":"SPARQL UPDATE Worker"},{"location":"usage/workers/#cli-workers","text":"These workers can be used to benchmark a CLI application.","title":"CLI Workers"},{"location":"usage/workers/#cli-worker","text":"This Worker should be used if the CLI application runs a query once and exits afterwards. Something like $ cli-script.sh query HEADER QUERY RESULT 1 QUERY RESULT 2 ... $ Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIWorker\"","title":"CLI Worker"},{"location":"usage/workers/#cli-input-worker","text":"This Worker should be used if the CLI application runs and the query will be send using the Input. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\"","title":"CLI Input Worker"},{"location":"usage/workers/#multiple-cli-input-worker","text":"This Worker should be used if the CLI application runs and the query will be send using the Input and will quit on errors. Something like $ cli-script.sh start Your Input: QUERY HEADER QUERY RESULT 1 QUERY RESULT 2 ... Your Input: ERROR ERROR happend, exiting $ To assure a smooth benchmark, the CLI application will be run multiple times instead of once, and if the application quits, the next running process will be used, while in the background the old process will be restarted. Thus as soon as an error happend, the benchmark can continue without a problem. Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see above. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"MultipleCLIInputWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\"","title":"Multiple CLI Input Worker"},{"location":"usage/workers/#cli-input-file-worker","text":"Same as the Multiple CLI Input Worker . However the query won't be send to the input but written to a file and the file will be send to the input Something like $ cli-script.sh start Your Input: file-containg-the-query.txt HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend directory no Directory in which the file including the query should be saved. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputFileWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" directory: \"/tmp/\"","title":"CLI Input File Worker"},{"location":"usage/workers/#cli-input-prefix-worker","text":"Same as the Multiple CLI Input Worker . However the CLI application might need a pre and suffix. Something like $ cli-script.sh start Your Input: PREFIX QUERY SUFFIX HEADER QUERY RESULT 1 QUERY RESULT 2 ... Parameters are : parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend queryPrefix no String to use as a PREFIX before the query. querySuffix no String to use as a SUFFIX after the query. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker . timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between [0, 2*value] (in MS) will be waited between each query. Simulating network latency or user behaviour. An Example: ... workers: - threads: 1 className: \"CLIInputPrefixWorker\" initFinished: \"loading finished\" queryFinished: \"query execution took:\" queryError: \"Error happend during request\" queryPrefix: \"SPARQL\" querySuffix: \";\" Will send the following as Input SPARQL QUERY ;","title":"CLI Input Prefix Worker"},{"location":"usage/workflow/","text":"Workflow Iguana will first parse configuration and afterwards will execute each task for each connection for each dataset. Imagine it like the following: for each dataset D for each connection C for each task T execute pre script hook execute task T(D, C) collect and calculate results write results execute post script hook","title":"Workflow"},{"location":"usage/workflow/#workflow","text":"Iguana will first parse configuration and afterwards will execute each task for each connection for each dataset. Imagine it like the following: for each dataset D for each connection C for each task T execute pre script hook execute task T(D, C) collect and calculate results write results execute post script hook","title":"Workflow"}]}
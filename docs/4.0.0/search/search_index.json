{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"#iguana","title":"IGUANA","text":"<p>Iguana is a benchmarking framework for testing the read performances of HTTP endpoints. It is mostly designed for benchmarking triplestores by using the SPARQL protocol. Iguana stresstests endpoints by simulating users which send a set of queries independently of each other.</p> <p>Benchmarks are configured using a YAML-file, this allows them to be easily repeated and adjustable. Results are stored in RDF-files and can also be exported as CSV-files.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Benchmarking of (SPARQL) HTTP endpoints</li> <li>Reusable configuration</li> <li>Calculation of various metrics for better comparisons</li> <li>Processing of HTTP responses (e.g., results counting)</li> </ul>"},{"location":"#setup","title":"Setup","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>If you're using the native version of IGUANA, you need to have at least a <code>x86-64-v3</code> (Intel Haswell and AMD Excavator or newer) system that is running Linux.</p> <p>If you're using the Java version of IGUANA, you need to have <code>Java 17</code> or higher installed. On Ubuntu it can be installed by executing the following command:</p> <pre><code>sudo apt install openjdk-17-jre\n</code></pre>"},{"location":"#download","title":"Download","text":"<p>The latest release can be downloaded at https://github.com/dice-group/IGUANA/releases/latest. The zip file contains three files:</p> <ul> <li><code>iguana</code></li> <li><code>iguana.jar</code></li> <li><code>example-suite.yml</code></li> <li><code>start-iguana.sh</code></li> </ul> <p>The <code>iguana</code> file is a native executable for IGUANA that has been compiled with GraalVM. The <code>iguana.jar</code> file is the standard Java executable for IGUANA. The <code>start-iguana.sh</code> script is a helper script to start IGUANA with the <code>iguana.jar</code> file.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>The <code>example-suite.yml</code> file contains an extensive configuration for a benchmark suite. It can be used as a starting point for your own benchmark suite. For a detailed explanation of the configuration, see the configuration documentation.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#native-version","title":"Native Version","text":"<p>Start Iguana with a benchmark suite (e.g., the <code>example-suite.yml</code>) by executing the binary:</p> <pre><code>./iguana example-suite.yml\n</code></pre>"},{"location":"#java-version","title":"Java Version","text":"<p>Start Iguana with a benchmark suite (e.g., the <code>example-suite.yml</code>) either by using the start script:</p> <pre><code>./start-iguana.sh example-suite.yml\n</code></pre> <p>or by directly executing the jar-file:</p> <pre><code>java -jar iguana.jar example-suite.yml\n</code></pre> <p>If you're using the script, you can use JVM arguments by setting the environment variable <code>IGUANA_JVM</code>. For example, to let Iguana use 4GB of RAM you can set <code>IGUANA_JVM</code> as follows:</p> <pre><code>export IGUANA_JVM=-Xmx4g\n</code></pre>"},{"location":"#how-to-cite","title":"How to Cite","text":"<pre><code>@InProceedings{10.1007/978-3-319-68204-4_5,\nauthor=\"Conrads, Lixi\nand Lehmann, Jens\nand Saleem, Muhammad\nand Morsey, Mohamed\nand Ngonga Ngomo, Axel-Cyrille\",\neditor=\"d'Amato, Claudia\nand Fernandez, Miriam\nand Tamma, Valentina\nand Lecue, Freddy\nand Cudr{\\'e}-Mauroux, Philippe\nand Sequeda, Juan\nand Lange, Christoph\nand Heflin, Jeff\",\ntitle=\"Iguana: A Generic Framework for Benchmarking the Read-Write Performance of Triple Stores\",\nbooktitle=\"The Semantic Web -- ISWC 2017\",\nyear=\"2017\",\npublisher=\"Springer International Publishing\",\naddress=\"Cham\",\npages=\"48--65\",\nabstract=\"The performance of triples stores is crucial for applications driven by RDF. Several benchmarks have been proposed that assess the performance of triple stores. However, no integrated benchmark-independent execution framework for these benchmarks has yet been provided. We propose a novel SPARQL benchmark execution framework called Iguana. Our framework complements benchmarks by providing an execution environment which can measure the performance of triple stores during data loading, data updates as well as under different loads and parallel requests. Moreover, it allows a uniform comparison of results on different benchmarks. We execute the FEASIBLE and DBPSB benchmarks using the Iguana framework and measure the performance of popular triple stores under updates and parallel user requests. We compare our results (See https://doi.org/10.6084/m9.figshare.c.3767501.v1) with state-of-the-art benchmarking results and show that our benchmark execution framework can unveil new insights pertaining to the performance of triple stores.\",\nisbn=\"978-3-319-68204-4\"\n}\n</code></pre>"},{"location":"configuration/ahead-of-time-compilation/","title":"Ahead of Time Compilation","text":"<p>Because IGUANA is written in Java, the benchmark results might become inaccurate due to the architecture of the JVM. The benchmark results might appear to be slower at the beginning of the execution and faster at the end, even though the  benchmarked system's performance remains constant.</p> <p>To minimize this effect, IGUANA uses GraalVM's ahead-of-time compilation feature.  This feature compiles the Java code to a native executable, which can be run without the need for a JVM.</p> <p>This section explains how to compile IGUANA with GraalVM and how to use the compiled binary.</p>"},{"location":"configuration/ahead-of-time-compilation/#prerequisites","title":"Prerequisites","text":"<p>To compile IGUANA with GraalVM, you need to have GraalVM installed on your system. The <code>native-image</code> tool also requires some additional libraries to be installed on your system. The further prerequisites can be found here.</p> <p>The default target architecture for the native binary is <code>x86-64-v3</code> (Intel Haswell and AMD Excavator or newer). This and other settings can be adjusted in the <code>pom.xml</code> file.</p>"},{"location":"configuration/ahead-of-time-compilation/#compilation","title":"Compilation","text":"<p>To compile IGUANA with GraalVM, execute the following command:</p> <pre><code>mvn -Pnative -Dagent=true package\n</code></pre> <p>This command creates a native binary named <code>iguana</code> in the <code>target/</code> directory.</p>"},{"location":"configuration/ahead-of-time-compilation/#usage","title":"Usage","text":"<p>The compiled executable can be run like any other executable and behaves the same as the Java version.</p> <pre><code>./iguana &lt;SUITE_FILE&gt;\n</code></pre>"},{"location":"configuration/language_processor/","title":"Language Processor","text":"<p>Language processors are used to process the response bodies of the HTTP requests that are executed by the workers.  The processing is done to extract relevant information from the responses and store them in the results.</p> <p>Language processors are defined by the content type of the response body they process. They cannot be configured directly in the configuration file, but are used by the response body processors.</p> <p>Currently only the <code>SaxSparqlJsonResultCountingParser</code> language processor is supported for the <code>application/sparql-results+json</code> content type.</p>"},{"location":"configuration/language_processor/#saxsparqljsonresultcountingparser","title":"SaxSparqlJsonResultCountingParser","text":"<p>The <code>SaxSparqlJsonResultCountingParser</code> is a language processor used to extract simple information from the responses of SPARQL endpoints that are in the <code>application/sparql-results+json</code> format. It counts the number of results, the number of variables,  and the number of bindings from the response of a <code>SELECT</code> or <code>ASK</code> query.</p>"},{"location":"configuration/metrics/","title":"Metrics","text":"<p>Metrics are used to measure and compare the performance of the system during the stresstest. They are divided into task metrics, worker metrics, and query metrics.</p> <p>Task metrics are calculated for every query execution across the whole task. Worker metrics are calculated for every query execution of one worker. Query metrics are calculated for every execution of one query across one worker and across every worker.</p> <p>For a detailed description of how results for tasks, workers and queries are reported in the RDF result file, please refer to the section RDF results.</p>"},{"location":"configuration/metrics/#configuration","title":"Configuration","text":"<p>The metrics are configured in the <code>metrics</code> section of the configuration file. To enable a metric, add an entry to the <code>metrics</code> list with the <code>type</code> of the metric. Some metrics (<code>PQPS</code>, <code>PAvgQPS</code>) require the configuration of a <code>penalty</code> value, which is the time in milliseconds that a failed query will be penalized with.</p> <pre><code>metrics:\n  - type: \"QPS\"\n  - type: \"AvgQPS\"\n  - type: \"PQPS\"\n    penalty: 180000 # in milliseconds\n</code></pre> <p>If the <code>metrics</code> section is not present in the configuration file, the following default configuration is used:</p> <pre><code>metrics:\n  - type: \"AES\"\n  - type: \"EachQuery\"\n  - type: \"QPS\"\n  - type: \"AvgQPS\"\n  - type: \"NoQ\"\n  - type: \"NoQPH\"\n  - type: \"QMPH\"\n</code></pre>"},{"location":"configuration/metrics/#available-metrics","title":"Available metrics","text":"Name Configuration type Additional parameters Scope Description Queries per second <code>QPS</code> query The number of successfully executed queries per second. It is calculated by dividing the number of successfully executed queries Average queries per second <code>AvgQPS</code> task, worker The average number of queries successfully executed per second. It is calculated by dividing the sum of the QPS values of every query the task or worker has by the number of queries. Number of queries <code>NoQ</code> task, worker The number of successfully executed queries. This metric is calculated for each worker and for the whole task. Number of queries per hour <code>NoQPH</code> task, worker The number of successfully executed queries per hour. It is calculated by dividing the number of successfully executed queries by their sum of time (in hours) it took to execute them. The metric value for the task is the sum of the metric for each worker. Query mixes per hour <code>QMPH</code> task, worker The number of query mixes executed per hour. A query mix is the set of queries executed by a worker, or the whole task. This metric is calculated for each worker and for the whole task. It is calculated by dividing the number of successfully executed queries by the number of queries inside the query mix and by their sum of time (in hours) it took to execute them. Penalized queries per second <code>PQPS</code> <code>penalty</code> (in milliseconds) query The number of queries executed per second, penalized by the number of failed queries. It is calculated by dividing the number of successful and failed query executions by their sum of time (in seconds) it took to execute them. If a query fails, the time it took to execute it is set to the given <code>penalty</code> value. Penalized average queries per second <code>PAvgQPS</code> <code>penalty</code> (in milliseconds) task, worker The average number of queries executed per second, penalized by the number of failed queries. It is calculated by dividing the sum of the PQPS of each query the task or worker has executed by the number of queries. Aggregated execution statistics <code>AES</code> task, worker see below Each execution statistic <code>EachQuery</code> query see below"},{"location":"configuration/metrics/#other-metrics","title":"Other metrics","text":""},{"location":"configuration/metrics/#aggregated-execution-statistics-aes","title":"Aggregated Execution Statistics (AES)","text":"<p>This metric collects for each query that belongs to a worker or a task a number of statistics that are aggregated for each execution.</p> Name Description <code>succeeded</code> The number of successful executions. <code>failed</code> The number of failed executions. <code>resultSize</code> The size of the HTTP response. (only stores the last result) <code>timeOuts</code> The number of executions that resulted with a timeout. <code>wrongCodes</code> The number of HTTP status codes received that were not 200. <code>unknownExceptions</code> The number of unknown exceptions during execution. <code>totalTime</code> The total time it took to execute the queries. <p>The <code>resultSize</code> is the size of the HTTP response in bytes and is an exception to the aggregation.</p>"},{"location":"configuration/metrics/#each-execution-statistic-eachquery","title":"Each Execution Statistic (EachQuery)","text":"<p>This metric collects statistics for each execution of a query. </p> Name Description <code>run</code> The number of the execution. <code>startTime</code> The time stamp where the execution started. <code>time</code> The time it took to execute the query. <code>success</code> If the execution was successful. <code>code</code> Numerical value of the end state of the execution. (success=0, timeout=110, http_error=111, exception=1) <code>resultSize</code> The size of the HTTP response. <code>exception</code> The exception that occurred during execution. (if any occurred) <code>httpCode</code> The HTTP status code received. (if any was received) <code>responseBody</code> The hash of the HTTP response body. (only if <code>parseResults</code> inside the stresstest has been set to <code>true</code>)"},{"location":"configuration/overview/","title":"Configuration","text":"<p>The configuration file for a benchmark suite can either be <code>.yaml</code>-file or a <code>.json</code>-file. YAML is recommended and all examples will be presented as YAML. </p>"},{"location":"configuration/overview/#example","title":"Example","text":"<p>The following example shows a basic configuration for a benchmark suite as an introduction.</p> <pre><code>dataset:\n  - name: \"sp2b\"                                        # for documentation purposes\n\nconnections:\n  - name: \"fuseki\"\n    endpoint: \"http://localhost:3030/sparql\"\n    dataset: \"sp2b\"\n\ntasks:\n  - type: \"stresstest\"                                  # stresstest the endpoint\n    workers:\n    - type: \"SPARQLProtocolWorker\"                      # this worker type sends SPARQL queries over HTTP with the SPARQL protocol\n      number: 2                                         # generate 2 workers with the same configuration\n      connection: \"fuseki\"                              # the endpoint to which the workers are sending the queries to\n      queries:\n        path: \"./example/suite/queries.txt\"             # the file with the queries\n        format: \"one-per-line\"                          # the format of the queries\n      completionTarget:\n        number: 1                                       # each worker stops after executing all queries once\n      timeout: \"3 min\"                                  # a query will time out after 3 minutes\n      acceptHeader: \"application/sparql-results+json\"   # the expected content type of the HTTP response (HTTP Accept header)\n      parseResults: false\n\n# calculate queries per second only for successful queries and the queries per second with a penalty for failed queries\nmetrics:\n  - type: \"PQPS\"\n    penalty: 180000   # in milliseconds (3 minutes)\n  - type: \"QPS\"\n\n# store the results in an n-triples file and in CSV files\nstorages:\n  - type: \"rdf file\"             \n    path: \"./results/result.nt\"\n  - type: \"csv file\"\n    directory: \"./results/\"\n</code></pre> <p>This configuration defines a benchmark suite that stresstests a triplestore with two workers.</p> <p>The triplestore is named <code>fuseki</code> and is located at <code>http://localhost:3030/sparql</code>. The dataset, that is used for the benchmark, is named <code>sp2b</code>. During the stresstest the workers will send SPARQL queries that are located in the file <code>./example/suite/queries.txt</code> to the triplestore. They will stop after they have executed all queries once, which is defined by the <code>completionTarget</code>-property.</p> <p>After the queries have been executed, two metrics are calculated based on the results. The first metric is the <code>PQPS</code>-metric, which calculates the queries per second with a penalty for failed queries. The second metric is the <code>QPS</code>-metric, which calculates the queries per second only for successful queries.</p> <p>The results are stored in an RDF file at <code>./results/result.nt</code> and in CSV files in the directory <code>./results/</code>.</p>"},{"location":"configuration/overview/#structure","title":"Structure","text":"<p>The configuration file consists of the following six sections: - Datasets - Connections - Tasks - Response-Body-Processors - Metrics - Storages</p> <p>Each section holds an array of their respective items. Each item type will be defined further in this documentation. The order of the sections is not important. The general structure of a suite configuration may look like this:</p> <pre><code>tasks:\n  - # item 1\n  - # item 2\n  - # ...\n\nstorages:\n  - # item 1\n  - # item 2\n  - # ...\n\ndatasets:\n  - # item 1\n  - # item 2\n  - # ...\n\nconnections:\n  - # item 1\n  - # item 2\n  - # ...\n\n\nresponseBodyProcessors:\n  - # item 1\n  - # item 2\n  - # ...\n\nmetrics:\n  - # item 1\n  - # item 2\n  - # ...\n</code></pre>"},{"location":"configuration/overview/#durations","title":"Durations","text":"<p>Durations are used to define time spans in the configuration. They can be used for the <code>timeout</code>-property of the workers or the response body processors or for the <code>completionTarget</code>-property of the tasks. Duration values can be defined as a XSD duration string or as a string with a number and a unit. The following units are supported: - <code>s</code> or <code>sec</code>or <code>secs</code> for seconds - <code>m</code> or <code>min</code> or <code>mins</code> for minutes - <code>h</code> or <code>hr</code> or <code>hrs</code> for hours - <code>d</code> or <code>day</code> or <code>days</code> for days</p> <p>Some examples for duration values:</p> <pre><code>timeout: \"2S\" # 2 seconds\ntimeout: \"10s\" # 10 seconds\ntimeout: \"PT10S\" # 10 seconds\n</code></pre>"},{"location":"configuration/overview/#tasks","title":"Tasks","text":"<p>The tasks are the core of the benchmark suite. They define the actual process of the benchmarking suite and are executed from top to bottom in the order they are defined in the configuration. At the moment, the <code>stresstest</code> is the only implemented task. The <code>stresstest</code>-task queries specified endpoints with the given queries and evaluates the performance of the endpoint by measuring the time each query execution took.  After the execution of the queries, the task calculates the required metrics based on the measurements.</p> <p>The tasks are explained in more detail in the Tasks documentation.</p>"},{"location":"configuration/overview/#storages","title":"Storages","text":"<p>The storages define where and how the results of the benchmarking suite are stored. There are three types of storages that are supported at the moment: - <code>rdf file</code> - <code>csv file</code> - <code>triplestore</code></p> <p>Each storage type will be explained in more detail in the Storages documentation.</p>"},{"location":"configuration/overview/#datasets","title":"Datasets","text":"<p>The datasets that have been used for the benchmark can be defined here. Right now, this is only used for documentation purposes. For example, you might want to know which dataset was loaded into a triplestore at the time a stresstest  was executed.</p> <p>The datasets are therefore later on referenced in the <code>connections</code>-property to document which dataset has been loaded into which endpoint.</p>"},{"location":"configuration/overview/#properties","title":"Properties","text":"<p>Each dataset entry has the following properties:</p> property required description example name yes This is a descriptive name for the dataset. <code>\"sp2b\"</code> file no File path of the dataset. (not used for anything at the moment) <code>\"./datasets/sp2b.nt\"</code>"},{"location":"configuration/overview/#example_1","title":"Example","text":"<pre><code>datasets:\n  - name: \"sp2b\"\n    file: \"./datasets/sp2b.nt\"\n\nconnections:\n  - name: \"fuseki\"\n    endpoint: \"https://localhost:3030/query\"\n    dataset: \"sp2b\"\n</code></pre> <p>As already mentioned, the <code>datasets</code>-property is only used for documentation. The information about the datasets will be stored in the results. For the csv storage, the above configuration might result with the following <code>task-configuration.csv</code>-file:</p> taskID connection version dataset http://iguana-benchmark.eu/resource/1699354119-3273189568/0 fuseki v2 sp2b <p>The resulting triples for the rdf file storage might look like this:</p> <pre><code>ires:fuseki a iont:Connection ;\n    rdfs:label      \"fuseki\" ;\n    iprop:dataset   ires:sp2b .\n\nires:sp2b a iont:Dataset ;\n    rdfs:label  \"sp2b\" .\n</code></pre>"},{"location":"configuration/overview/#connections","title":"Connections","text":"<p>The connections are used to define the endpoints for the triplestores. The defined connections can later be used in the <code>tasks</code>-configuration to specify the endpoints for the benchmarking process.</p>"},{"location":"configuration/overview/#properties_1","title":"Properties","text":"property required description example name yes This is a descriptive name for the connection. (needs to be unique) <code>\"fuseki\"</code> version no This serves to document the version of the connection. It has no functional property. <code>\"v1.0.1\"</code> dataset no This serves to document the dataset, that has been loaded into the specified connection. It has no functional property. (needs to reference an already defined dataset in <code>datasets</code>) <code>\"sp2b\"</code> endpoint yes An URI at which the endpoint is located. <code>\"http://localhost:3030/query\"</code> authentication no Basic authentication data for the connection. see below updateEndpoint no An URI at which an additional update-endpoint might be located. This is useful for triplestores that have separate endpoints for update queries. <code>\"http://localhost:3030/update\"</code> updateAuthentication no Basic Authentication data for the updateEndpoint. see below <p>Iguana only supports the HTTP basic authentication for now. The authentication properties are objects that are defined as follows:</p> property required description example user yes The user name. <code>\"admin\"</code> password yes The password of the user. <code>\"password\"</code>"},{"location":"configuration/overview/#example_2","title":"Example","text":"<pre><code>datasets:\n  - name: \"wikidata\"\n\nconnections:\n  - name: \"fuseki\"\n    endpoint: \"https://localhost:3030/query\"\n  - name: \"tentris\"\n    version: \"v0.4.0\"\n    dataset: \"wikidata\" # needs to reference an existing definition in datasets\n    endpoint: \"https://localhost:9080/query\"\n    authentication:\n      user: \"admin\"\n      password: \"password\"\n    updateEndpoint: \"https://localhost:8080/update\"\n    updateAuthentication:\n      user: \"updateUser\"\n      password: \"123\"\n</code></pre>"},{"location":"configuration/overview/#response-body-processor","title":"Response-Body-Processor","text":"<p>The response body processors are used to process the response bodies that are received for each query from the benchmarked endpoints. The processors extract relevant information from the response bodies and store them in the results. Processors are defined by the content type of the response body they process. At the moment, only the <code>application/sparql-results+json</code> content type is supported.</p> <p>The response body processors are explained in more detail in the Response-Body-Processor documentation.</p>"},{"location":"configuration/overview/#metrics","title":"Metrics","text":"<p>Metrics are used to compare the performance of the benchmarked endpoints. The metrics are calculated from the results of the benchmarking tasks. Depending on the type of the metric, they are calculated for each query, for each worker, or for the whole task.</p> <p>Each metric will be explained in more detail in the Metrics documentation.</p>"},{"location":"configuration/overview/#basic-example","title":"Basic Example","text":"<pre><code>datasets:\n  - name: \"sp2b\"\n\nconnections:\n  - name: \"fuseki\"\n    dataset: \"sp2b\"\n    endpoint: \"http://localhost:3030/sp2b\"\n\ntasks:\n  - type: \"stresstest\"\n    workers:\n      - number: 2\n        type: \"SPARQLProtocolWorker\"\n        parseResults: true\n        acceptHeader: \"application/sparql-results+json\"\n        queries:\n          path: \"./example/suite/queries/\"\n          format: \"folder\"\n        completionTarget:\n          number: 1\n        connection: \"fuseki\"\n        timeout: \"2S\"\n\nresponseBodyProcessors:\n  - contentType: \"application/sparql-results+json\"\n    threads: 1\n\nmetrics:\n  - type: \"PQPS\"\n    penalty: 100\n  - type: \"QPS\"\n\nstorages:\n  - type: \"rdf file\"\n    path: \"./results/result.nt\"\n  - type: \"csv file\"\n    directory: \"./results/\"\n</code></pre>"},{"location":"configuration/queries/","title":"Queries","text":"<p>Benchmarks often involve running a series of queries against a database and measuring their performances. The query handler in Iguana is responsible for loading and selecting queries for the benchmarking process.</p> <p>Inside the stresstest task, the query handler is configured with the <code>queries</code> property. Every worker instance of the same worker configuration will use the same query handler. The <code>queries</code> property is an object that contains the following properties:</p> property required default description example path yes The path to the queries. It can be a file or a folder. <code>./example/suite/queries/</code> format no <code>one-per-line</code> The format of the queries. <code>folder</code> or <code>separator</code> or <code>one-per-line</code> separator no <code>\"\"</code> The separator that should be used if the format is set to <code>separator</code>. <code>\\n###\\n</code> caching no <code>true</code> If set to <code>true</code>, the queries will be cached into memory. If set to <code>false</code>, the queries will be read from the file system every time they are needed. <code>false</code> order no <code>linear</code> The order in which the queries are executed. If set to <code>linear</code> the queries will be executed in their order inside the file. If <code>format</code> is set to <code>folder</code>, queries will be sorted by their file name first. <code>random</code> or <code>linear</code> seed no <code>0</code> The seed for the random number generator that selects the queries. If multiple workers use the same query handler, their seed will be the sum of the given seed and their worker id. <code>12345</code> lang no <code>SPARQL</code> Not used for anything at the moment."},{"location":"configuration/queries/#format","title":"Format","text":""},{"location":"configuration/queries/#one-per-line","title":"One-per-line","text":"<p>The <code>one-per-line</code> format is the default format. In this format, every query is written in a single line inside one file.</p> <p>In this example, the queries are written in a single file, each query in a single line:</p> <pre><code>SELECT DISTINCT * WHERE { ?s ?p ?o }\nSELECT DISTINCT ?s ?p ?o WHERE { ?s ?p ?o }\n</code></pre>"},{"location":"configuration/queries/#folder","title":"Folder","text":"<p>It is possible to write every query in a separate file and put them all in a folder. Queries will be sorted by their file name before they are read.</p> <p>In this example, the queries are written in separate files inside the folder <code>./example/suite/queries/</code>:</p> <pre><code>./example/suite/queries/\n\u251c\u2500\u2500 query1.txt\n\u2514\u2500\u2500 query2.txt\n</code></pre> <p>The file <code>query1.txt</code> contains the following query:</p> <pre><code>SELECT DISTINCT * \nWHERE { \n    ?s ?p ?o \n}\n</code></pre> <p>The file <code>query2.txt</code> contains the following query:</p> <pre><code>SELECT DISTINCT ?s ?p ?o \nWHERE { \n    ?s ?p ?o \n}\n</code></pre>"},{"location":"configuration/queries/#separator","title":"Separator","text":"<p>It is possible to write every query in a single file and separate them with a separator. The separator can be set with the <code>separator</code> property. Iguana will then split the file into queries based on the separator. If the <code>separator</code> property is set to an empty string <code>\"\"</code> (default) the queries will be separated by an empty line. The separator string can also contain escape sequences like <code>\\n</code> or <code>\\t</code>.</p> <p>In this example, the queries inside this file are separated by a line consisting of the string <code>###</code>:</p> <pre><code>SELECT DISTINCT * \nWHERE { \n    ?s ?p ?o \n}\n###\nSELECT DISTINCT ?s ?p ?o \nWHERE { \n    ?s ?p ?o \n}\n</code></pre> <p>The <code>separator</code> property should be set to <code>\"\\n###\\n\"</code>. (be aware of different line endings on different operating systems)</p>"},{"location":"configuration/queries/#example","title":"Example","text":"<pre><code>tasks:\n  - type: \"stresstest\"\n    workers:\n    - type: \"SPARQLProtocolWorker\"\n      queries:\n        path: \"./example/suite/queries.txt\"\n        format: \"separator\"\n        separator: \"\\n###\\n\"\n        caching: false\n        order: \"random\"\n        seed: 12345\n        lang: \"SPARQL\"\n      # ... additional worker properties\n</code></pre>"},{"location":"configuration/rdf_results/","title":"RDF Results","text":"<p>The differences between task, worker, and query metrics will be explained in more detail with the following examples. The results shown have been generated with the <code>rdf file</code> storage type.</p>"},{"location":"configuration/rdf_results/#task-and-worker-metrics","title":"Task and Worker Metrics","text":"<p>The first excerpt shows the results for the task <code>ires:1710247002-3043500295/0</code> and its worker <code>ires:1710247002-3043500295/0/0</code>:</p> <pre><code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0&gt;\n        a                     iont:Stresstest , iont:Task ;\n        iprop:AvgQPS          84.121083502 ;\n        iprop:NoQ             16 ;\n        iprop:NoQPH           21894.0313677612 ;\n        iprop:QMPH            1287.8841981036 ;\n        iprop:endDate         \"2024-03-12T12:36:48.323Z\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;\n        iprop:metric          ires:QMPH , ires:NoQPH , ires:AvgQPS , ires:NoQ ;\n        iprop:noOfWorkers     \"1\"^^&lt;http://www.w3.org/2001/XMLSchema#int&gt; ;\n        iprop:query           (iri of every query that has been executed inside the task) ;\n        iprop:startDate       \"2024-03-12T12:36:42.636Z\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;\n        iprop:workerResult    &lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0&gt; .\n\n&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0&gt;\n        a                  iont:Worker ;\n        iprop:AvgQPS       84.121083502 ;\n        iprop:NoQ          16 ;\n        iprop:NoQPH        21894.0313677612 ;\n        iprop:QMPH         1287.8841981036 ;\n        iprop:connection   ires:fuseki ;\n        iprop:endDate      \"2024-03-12T12:36:48.322204Z\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;\n        iprop:metric       ires:NoQ , ires:NoQPH , ires:QMPH , ires:AvgQPS ;\n        iprop:noOfQueries  \"17\"^^&lt;http://www.w3.org/2001/XMLSchema#int&gt; ;\n        iprop:noOfQueryMixes  \"1\"^^&lt;http://www.w3.org/2001/XMLSchema#int&gt; ;\n        iprop:query        (iri of every query the worker has executed) ;\n        iprop:startDate    \"2024-03-12T12:36:42.6457629Z\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;\n        iprop:timeOut      \"PT10S\"^^&lt;http://www.w3.org/2001/XMLSchema#dayTimeDuration&gt; ;\n        iprop:workerID     \"0\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt; ;\n        iprop:workerType   \"SPARQLProtocolWorker\" .\n</code></pre> <ul> <li>The IRI <code>ires:1710247002-3043500295/0</code> represents the task <code>0</code> of the benchmark suite <code>1710247002-3043500295</code>.</li> <li>The IRI <code>ires:1710247002-3043500295/0/0</code> represents the worker <code>0</code> of the task described above.</li> </ul> <p>Both task and worker contain results of the <code>AvgQPS</code>, <code>NoQ</code>, <code>NoQPH</code>, and <code>QMPH</code> metrics. These metrics are calculated for the whole task and for each worker, which can be seen in the example. Because the task of this example only had one worker, the results are the same.</p> <p>Additional information about the task and worker, besides the metric results, are stored as well. The following properties are stored for the task: - <code>noOfWorkers</code>: The number of workers that executed the task. - <code>query</code>: The IRI of every query that was executed by the task. - <code>startDate</code>: The time when the task started. - <code>endDate</code>: The time when the task ended. - <code>workerResult</code>: The IRIs of the workers that executed the task. - <code>metric</code>: The IRIs of the metrics that were calculated for the task.</p> <p>The following properties are stored for the worker: - <code>connection</code>: The IRI of the connection that the worker used. - <code>noOfQueries</code>: The number of queries. - <code>noOfQueryMixes</code>: The number of queries mixes that the worker executed (mutually exclusive to <code>timeLimit</code>). - <code>timeLimit</code>: The time duration for which the worker has executed queries (mutually exclusive to <code>noOfQueryMixes</code>). - <code>query</code>: The IRI of every query that the worker executed. - <code>startDate</code>: The time when the worker started. - <code>endDate</code>: The time when the worker ended. - <code>timeOut</code>: The maximum time a query execution should take. - <code>workerID</code>: The id of the worker. - <code>workerType</code>: The type of the worker.</p>"},{"location":"configuration/rdf_results/#query-metrics","title":"Query Metrics","text":"<p>Every query of each query handler has its own id. It consists of a hash value of the query handler and the query id in this format: <code>ires:&lt;query_handler_hash&gt;:&lt;query_id&gt;</code>. In this example, results for the query <code>ires:1181728761:0</code> are shown:</p> <pre><code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/1181728761:0&gt;\n        a                       iont:ExecutedQuery ;\n        iprop:QPS               18.975908187 ;\n        iprop:failed            0 ;\n        iprop:queryID           ires:1181728761:0 ;\n        iprop:resultSize        212 ;\n        iprop:succeeded         1 ;\n        iprop:timeOuts          0 ;\n        iprop:totalTime         \"PT0.0526984S\"^^&lt;http://www.w3.org/2001/XMLSchema#dayTimeDuration&gt; ;\n        iprop:unknownException  0 ;\n        iprop:wrongCodes        0 .\n\n&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0/1181728761:0&gt;\n        a                       iont:ExecutedQuery ;\n        iprop:QPS               18.975908187 ;\n        iprop:failed            0 ;\n        iprop:queryExecution    &lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0/1181728761:0/1&gt; ;\n        iprop:queryID           ires:1181728761:0 ;\n        iprop:resultSize        212 ;\n        iprop:succeeded         1 ;\n        iprop:timeOuts          0 ;\n        iprop:totalTime         \"PT0.0526984S\"^^&lt;http://www.w3.org/2001/XMLSchema#dayTimeDuration&gt; ;\n        iprop:unknownException  0 ;\n        iprop:wrongCodes        0 .\n</code></pre> <p>The IRI <code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0/1181728761:0&gt;</code> consists of the following segments: - <code>ires:1710247002-3043500295</code> is the IRI of the benchmark suite. - <code>ires:1710247002-3043500295/0</code> is the IRI of the first task. - <code>ires:1710247002-3043500295/0/0</code> is the IRI of the first task's worker. - <code>1181728761:0</code> is the query id.</p> <p>The suite id is made up of the timestamp and the hash value of the suite configuration in this pattern: <code>ires:&lt;timestamp&gt;-&lt;hash&gt;</code>.</p> <p>The subject <code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0/1181728761:0&gt;</code> represents the results of the query <code>ires:1181728761:0</code> from first worker of the task <code>1710247002-3043500295/0</code>.</p> <p>The subject <code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/1181728761:0&gt;</code> represents the results of the query <code>ires:1181728761:0</code> from every worker across the whole task <code>1710247002-3043500295/0</code>.</p> <p>Results of query metrics, like the <code>QPS</code> metric (also the <code>AES</code> metric), are therefore calculated for each query of each worker and for each query of the whole task.</p> <p>The <code>iprop:queryExecution</code> property of <code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0/1181728761:0&gt;</code>  contains the IRIs of the executions of that query from that worker. These will be explained in the next section.</p>"},{"location":"configuration/rdf_results/#each-execution-statistic","title":"Each Execution Statistic","text":"<p>With the <code>EachQuery</code> metric Iguana stores the statistics of each execution of a query. The following excerpt shows the execution statistics of the query <code>ires:1181728761:0</code>:</p> <pre><code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0/1181728761:0/1&gt;\n        iprop:code          \"0\"^^&lt;http://www.w3.org/2001/XMLSchema#int&gt; ;\n        iprop:httpCode      \"200\" ;\n        iprop:queryID       ires:1181728761:0 ;\n        iprop:responseBody  &lt;http://iguana-benchmark.eu/resource/responseBody/-3025899826584824492&gt; ;\n        iprop:resultSize    \"212\"^^&lt;http://www.w3.org/2001/XMLSchema#long&gt; ;\n        iprop:run           1 ;\n        iprop:startTime     \"2024-03-12T12:36:42.647764Z\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ;\n        iprop:success       true ;\n        iprop:time          \"PT0.0526984S\"^^&lt;http://www.w3.org/2001/XMLSchema#dayTimeDuration&gt; .\n</code></pre> <p>The IRI <code>&lt;http://iguana-benchmark.eu/resource/1710247002-3043500295/0/0/1181728761:0/1&gt;</code> consists of the worker query IRI as described above and the run number of the query execution.</p> <p>The properties of the <code>EachQuery</code> metric are described in the metrics section.</p>"},{"location":"configuration/response_body_processor/","title":"Response-Body-Processor","text":"<p>The response body processor is used to process the response bodies of the HTTP requests that are executed by the workers. The processing is done to extract relevant information from the responses and store them in the results.</p> <p>Iguana supports multiple response body processors that are defined by the content type of the response body they process.</p> <p>Currently only the <code>application/sparql-results+json</code> content type is supported,  and it only uses the <code>SaxSparqlJsonResultCountingParser</code> language processor  to extract simple information from the responses.</p> <p>Workers send the response bodies to the response body processors,  after receiving the full response bodies from the HTTP requests. Response bodies are processed in parallel by the number of threads that are defined in the configuration.</p> <p>To use a response body processor, it needs to be defined in the configuration file with the <code>contentType</code> property in the <code>responseBodyProcessors</code> list.</p>"},{"location":"configuration/response_body_processor/#properties","title":"Properties","text":"property required description example contentType yes The content type of the response body. <code>\"application/sparql-results+json\"</code> threads no The number of threads that are used to process the response bodies. (default is 1) <code>2</code> timeout no The maximum duration that the response body processor can take to process a response body. (default is 10 minutes) <code>10m</code>"},{"location":"configuration/storages/","title":"Storages","text":"<p>Storages are used to store the results of the benchmark suite.  It is possible to use multiple storages at the same time. They can be configured with the <code>storages</code> property in the configuration file by providing a list of storage configurations.</p>"},{"location":"configuration/storages/#example","title":"Example","text":"<pre><code>storages:\n  - type: \"csv file\"\n    directory: \"./results\"\n  - type: \"rdf file\"\n    path: \"./results\"\n  - type: \"triplestore\"\n    endpoint: \"http://localhost:3030/ds\"\n    username: \"admin\"\n    password: \"password\"\n</code></pre> <p>The following values for the <code>type</code> property are supported:</p> <ul> <li>csv file</li> <li>rdf file</li> <li>triplestore</li> </ul>"},{"location":"configuration/storages/#csv-file-storage","title":"CSV File Storage","text":"<p>The csv file storage writes the results of the benchmark suite to multiple csv files. It only has a single property, <code>directory</code>,  which defines the path to the directory where the csv files should be written to.</p> <p>Inside the directory, a new directory for the execution of the benchmark suite will be created. The name of the directory is <code>suite-&lt;timestamp&gt;-&lt;config-hash&gt;</code> where the <code>timestamp</code> is the benchmark's time of execution and <code>config-hash</code> the hash value of the benchmark configuration.</p> <p>The following shows an example of the directory structure and created files of the csv storage:</p> <pre><code>suite-1710241608-1701417056/\n\u251c\u2500\u2500 suite-summary.csv\n\u251c\u2500\u2500 task-0\n\u2502   \u251c\u2500\u2500 application-sparql+json\n\u2502   \u2502   \u2514\u2500\u2500 sax-sparql-result-data.csv\n\u2502   \u251c\u2500\u2500 each-execution-worker-0.csv\n\u2502   \u251c\u2500\u2500 query-summary-task.csv\n\u2502   \u251c\u2500\u2500 query-summary-worker-0.csv\n\u2502   \u2514\u2500\u2500 worker-summary.csv\n\u2514\u2500\u2500 task-configuration.csv\n</code></pre> <ul> <li>The <code>suite-summary.csv</code> file contains the summary of each task.</li> <li>The <code>task-configuration.csv</code> file contains information about the configuration of each task.</li> <li>Inside the <code>task-0</code> directory, the results of the task with the id <code>0</code> are stored.</li> <li>The <code>each-execution-worker-0.csv</code> file contains the metric results of each query execution for <code>worker 0</code>.</li> <li>The <code>query-summary-task.csv</code> file contains the summary of the metric results for every query inside the task.</li> <li>The <code>query-summary-worker-0.csv</code> file contains the summary of the metric results for every query of <code>worker 0</code>.</li> <li>The <code>worker-summary.csv</code> file contains the summary of metrics for each worker of the task.</li> </ul> <p>The <code>application-sparql+json</code> directory contains results from Language Processors  that process results with the <code>application/sparql+json</code> content type. Each Language Processor creates their own files in their respective directory.</p>"},{"location":"configuration/storages/#rdf-file-storage","title":"RDF File Storage","text":"<p>The rdf file storage writes the results of the benchmark suite to a single rdf file.</p> <p>It only has a single property, <code>path</code>, which defines the path to the rdf file where the results should be written to. The path can be either a file or a directory. The file extension of the file determines in which format the rdf data is stored (e.g., <code>.nt</code> for n-triples, <code>.ttl</code> for turtle).</p> <p>If the path is a directory or a file that already exists,  the file will be a turtle file with a timestamp as its name.</p>"},{"location":"configuration/storages/#triplestore-storage","title":"Triplestore Storage","text":"<p>The triplestore storage writes the results of the benchmark suite directly to a triplestore as triples, similar to the rdf file storage.</p> <p>It has the following properties:</p> <ul> <li><code>endpoint</code>: The update endpoint of the triplestore.</li> <li><code>username</code>: The username for the authentication of the triplestore.</li> <li><code>password</code>: The password for the authentication of the triplestore.</li> </ul> <p>The <code>username</code> and <code>password</code> properties are optional.</p>"},{"location":"configuration/tasks/","title":"Tasks","text":"<p>The tasks are the core of the benchmark suite. They define the actual process of the benchmarking suite and are executed from top to bottom in the order they are defined in the configuration. At the moment, the <code>stresstest</code> is the only implemented task.</p> <p>Tasks are defined in the <code>tasks</code> section of the configuration and are distinguished by the <code>type</code> property.</p>"},{"location":"configuration/tasks/#example","title":"Example","text":"<pre><code>tasks:\n  - type: \"stresstest\"\n    # properties of the task\n    # ...\n</code></pre>"},{"location":"configuration/tasks/#stresstest","title":"Stresstest","text":"<p>The <code>stresstest</code>-task queries the specified endpoints in rapid succession with the given queries. It measures the time it takes to execute each query and calculates the required metrics based on the measurements.  The task is used to measure the performance of the endpoint for each query. The task is configured with the following properties:</p> property required description workers yes An array that contains worker configurations. warmupworkers no An array that contains worker configurations for the warmup. <p>The stresstest uses workers to execute the queries, which are supposed to simulate users. Each worker has its own set of queries and executes them parallel to the other workers.</p> <p>Warmup workers have the same functionality as normal workers, but their results won't be processed and stored. The stresstest runs the warmup workers before the actual workers. They're used to warm up the system before the actual benchmarking starts.</p> <p>For more information about the worker configuration, see here.</p>"},{"location":"configuration/tasks/#example_1","title":"Example","text":"<pre><code>tasks:\n  - type: \"stresstest\"\n    workers:\n    - type: \"SPARQLProtocolWorker\"\n      # ... worker properties\n    warmupworkers:\n    - type: \"SPARQLProtocolWorker\"\n      # ...\n</code></pre>"},{"location":"configuration/workers/","title":"Workers","text":"<p>The stresstest uses workers to execute the queries, which are supposed to simulate users. Each worker has its own set of queries and executes them parallel to the other workers.</p> <p>Iguana supports multiple worker types, but currently only the <code>SPARQLProtocolWorker</code> is implemented. Workers have the common <code>type</code> property which defines the type of the worker.</p> <pre><code>tasks:\n  - type: \"stresstest\"\n    workers:\n    - type: \"SPARQLProtocolWorker\"\n      # properties of the worker\n      # ...\n    - type: \"SPARQLProtocolWorker\"\n      # properties of the worker\n      # ...\n</code></pre>"},{"location":"configuration/workers/#sparqlprotocolworker","title":"SPARQLProtocolWorker","text":"<p>The <code>SPARQLProtocolWorker</code> is a worker that sends SPARQL queries to an endpoint using the SPARQL protocol. The worker can be configured with the following properties:</p> property required default description number no <code>1</code> The number of workers that should be initiated with that same configuration. queries yes The configuration of the query handler these workers should use. (see here) completionTarget yes Either defines how many queries the worker should send, or how long it should send them. connection yes The name of the connection that the worker should use.  (needs to reference an already defined connection) timeout yes The duration for the query timeout. acceptHeader no The accept header that the worker should use for the HTTP requests. requestType no <code>get query</code> The request type that the worker should use. parseResults no <code>true</code> Whether the worker should parse the results. <p>Each property is explained in more detail below.</p>"},{"location":"configuration/workers/#example","title":"Example","text":"<pre><code>connection:\n  - name: \"fuseki\"\n    dataset: \"sp2b\"\n    endpoint: \"http://localhost:3030/sp2b\"\n\ntasks:\n  - type: \"stresstest\"\n    workers:\n    - type: \"SPARQLProtocolWorker\"\n      number: 2                       # two workers with the same configuration will be initiated\n      queries:                        # the query handler configuration, both workers will use the same query handler\n        path: \"./example/suite/queries/\"\n        format: \"folder\"\n      completionTarget:\n        number: 1                     # each query will be executed once\n      connection: \"fuseki\"            # the worker will use the connection with the name \"fuseki\", which is defined above\n      timeout: \"2S\"\n      acceptHeader: \"application/sparql-results+json\"\n      requestType: \"get query\"\n      parseResults: true\n</code></pre>"},{"location":"configuration/workers/#number","title":"Number","text":"<p>The <code>number</code> property defines the number of workers that should be initiated with the same configuration. Workers with the same configuration will use the same query handler instance.</p>"},{"location":"configuration/workers/#queries","title":"Queries","text":"<p>The <code>queries</code> property is the configuration of the query handler that the worker should use. The query handler is responsible for loading and selecting the queries that the worker should execute. The query handler configuration is explained in more detail here.</p>"},{"location":"configuration/workers/#completion-target","title":"Completion Target","text":"<p>The <code>completionTarget</code> property defines when the worker should stop executing queries. The property takes an object as its value that contains either one of the following properties: - <code>number</code>: The number of times the worker should execute each query. - <code>duration</code>: The duration during which the worker should iterate and execute every query.</p> <p>Example:</p> <pre><code>tasks:\n  - type: \"stresstest\"\n    workers:\n    - type: \"SPARQLProtocolWorker\"\n      number: 1\n      completionTarget:\n        number: 100 # execute each query 100 times\n      # ...\n    - type: \"SPARQLProtocolWorker\"\n      number: 1\n      completionTarget:\n        duration: \"10s\" # execute queries for 10 seconds\n      # ...\n</code></pre>"},{"location":"configuration/workers/#timeout","title":"Timeout","text":"<p>The <code>timeout</code> property defines the maximum time a query execution should take, this includes the time it takes to send the request and to receive the response. If the timeout is reached, the worker will mark it as failed, cancel the HTTP request and continue with the execution of the next query.</p> <p>The system that's being tested should make sure that it's able to abort the further execution of the query if the timeout has been reached (e.g., by using a timeout parameter for the system, if available). Otherwise, problems like high resource usage or other issues might occur.</p>"},{"location":"configuration/workers/#request-type","title":"Request Type","text":"<p>The <code>requestType</code> property defines the type of the HTTP request that the worker should use. It consists of a string that can be one of the following values:</p> request type HTTP method Content-Type header value description <code>\"get query\"</code> <code>GET</code> The worker will send a <code>GET</code> request with a <code>query</code> parameter that contains the query. <code>\"post query\"</code> <code>POST</code> <code>application/sparq-query</code> The body will contain the query. <code>\"post update\"</code> <code>POST</code> <code>application/sparq-update</code> The body will contain the update query. <code>\"post url-enc query\"</code> <code>POST</code> <code>application/x-www-form-urlencoded</code> The body will contain the a url-encoded key-value pair with the key being <code>query</code> and the query as the value. <code>\"post url-enc update\"</code> <code>POST</code> <code>application/x-www-form-urlencoded</code> The body will contain the a url-encoded key-value pair with the key being <code>update</code> and the query as the value."},{"location":"configuration/workers/#accept-header","title":"Accept Header","text":"<p>The <code>acceptHeader</code> property defines the value for the <code>Accept</code> header of the HTTP requests that a worker sends to the defined endpoint. This property also affects the Response-Body-Processors that are used to process the response bodies.</p>"},{"location":"configuration/workers/#parse-results","title":"Parse Results","text":"<p>The <code>parseResults</code> property defines whether the worker should parse the results of the queries. If the property is set to <code>true</code>, the worker will send the response body to the Response-Body-Processors for processing and calculate hash values for the response bodies. If the property is set to <code>false</code>, the worker will not parse the response bodies and will not calculate hash values for the response bodies.</p> <p>Setting the property to <code>false</code> can improve the performance of the worker.  This means that the worker is able to measure the performance more accurately. If the property is set to <code>true</code>, the worker will temporarily store the whole response bodies in memory for processing. If the property is set to <code>false</code>, the worker will discard any received bytes from the response.</p>"}]}
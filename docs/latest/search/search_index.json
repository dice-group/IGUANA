{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-iguana-documentation","title":"Welcome to the Iguana documentation!","text":"<p>This documentation will help you benchmark your HTTP endpoints (such as your Triple store) using Iguana and help you extend Iguana to your needs. It is split into three parts</p> <ul> <li>General</li> <li>Quick Start Guide</li> <li>Usage</li> <li>Development</li> </ul> <p>In General you will find a bit of information of what Iguana is and what it's capable of.</p> <p>In the Quick Start Guide you will find how to download and start Iguana as well how to quickly configure your first simple benchmark using Iguana.</p> <p>In Usage you will find everything on how to execute a benchmark with Iguana and how to configure the benchmark to your needs.  It further provides details on what tests Iguana is capable of. A Tutorial will finally guide you through all steps broadly which you can use as a quick start. </p> <p>In Development you will find everything you need to know in case that Iguana isn't sufficient for your needs. It shows how to extend Iguana to use your metrics or your specific benchmark test</p> <p>Have exciting Evaluations!</p>"},{"location":"about/","title":"Iguana","text":"<p>Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications. Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data. Hence it is very important that the triple store must scale on the data and can handle several users. Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily. Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well. Further on it was impossible to compare results for different benchmarks.</p> <p>Iguana tries to solve all these issues. It provides an enviroment which ...</p> <ul> <li>is highly configurable</li> <li>provides a realistic scneario benchmark</li> <li>works on every dataset</li> <li>works on SPARQL HTTP endpoints</li> <li>works on HTTP Get &amp; Post endpoints</li> <li>works on CLI applications</li> <li>and is easily extendable</li> </ul>"},{"location":"about/#what-is-iguana","title":"What is Iguana","text":"<p>Iguana is a HTTP and CLI read/write performance benchmark framework suite.  It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries.  Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. </p>"},{"location":"about/#what-can-be-benchmarked","title":"What can be benchmarked","text":"<p>Iguana is capable of benchmarking and stresstesting the following applications</p> <ul> <li>HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints)</li> <li>CLI Applications which either</li> <li>exit after every query</li> <li>or awaiting input after each query</li> </ul>"},{"location":"about/#what-benchmarks-are-possible","title":"What Benchmarks are possible","text":"<p>Every simulated User (named Worker in the following) gets a set of queries.  These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark.  Iguana will then let every Worker execute these queries against the endpoint. </p>"},{"location":"architecture/","title":"Architecture","text":"<p>Iguanas architecture is build as generic as possible to ensure that your benchmark can be executed while you only have  to create a configuration file which fits your needs.  So ideally you do not need to code anything and can use Iguana out of the box.</p> <p>Iguana will parse your Configuration (YAML or JSON format) and will read which Endpoints/Applications you want to benchmark. What datasets if you have any and what your benchmark should accomplish.  Do you just want to check how good your database/triple store performs against the state of the art? Does your new version out performs the old version?  Do you want to check read and write performance?  ... </p> <p>Whatever you want to do you just need to provide Iguana your tested applications, what to benchmark and which queries to use.</p> <p>Iguana relys mainly on HTTP libraries, the JENA framework and java 11. </p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>Iguana will read the configuration, parse it and executes for each specified datasets, each specified connection with the benchmark tasks you specified. After the executions the results will be written as RDF. Either to a NTriple file or directly to a triple store. The results can be queried itself using SPARQL.</p> <p>Iguana currently consists of on implemented Task, the Stresstest.  However, this task is very configurable and most definetly will met your needs if you want performance measurement. It starts a user defined amount of Workers, which will try to simulate real users/applications querying your Endpoint/Application.  </p>"},{"location":"architecture/#components","title":"Components","text":"<p>Iguana consists of two components, the core controller and the result processor.</p>"},{"location":"architecture/#core-controller","title":"core controller","text":"<p>The core which implements the Tasks and Workers to use. How HTTP responses should be handled. How to analyze the benchmark queries to give a little bit more extra information in the results. </p>"},{"location":"architecture/#result-processor","title":"result processor","text":"<p>The result processor consist of the metrics to apply to the query execution results and how to save the results.  Most of the SOtA metrics are implemented in Iguana. If one's missing it is pretty easy to add a metric though. </p> <p>By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store.  On the processing side, it calculates various metrics.</p> <p>Per run metrics: * Query Mixes Per Hour (QMPH) * Number of Queries Per Hour (NoQPH) * Number of Queries (NoQ) * Average Queries Per Second (AvgQPS)</p> <p>Per query metrics: * Queries Per Second (QPS)     * Number of successful and failed queries     * result size     * queries per second     * sum of execution times</p> <p>You can change these in the Iguana Benchmark suite config.</p> <p>If you use the basic configuration, it will save all mentioned metrics to a file called <code>results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt</code></p>"},{"location":"architecture/#more-information","title":"More Information","text":"<ul> <li>SPARQL</li> <li>RDF</li> <li>Iguana @ Github</li> <li>Our Paper from 2017 (outdated)</li> </ul>"},{"location":"download/","title":"Download","text":""},{"location":"download/#prerequisites","title":"Prerequisites","text":"<p>You need to have Java 11 or higher installed. </p> <p>In Ubuntu you can do this by </p> <pre><code>sudo apt-get install java\n</code></pre>"},{"location":"download/#download_1","title":"Download","text":"<p>Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest</p> <p>The zip file contains 3 files. </p> <ul> <li>iguana-3.3.3.jar</li> <li>example-suite.yml</li> <li>start-iguana.sh</li> </ul> <p>The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.</p>"},{"location":"quick-config/","title":"Quickly Configure Iguana","text":"<p>Here we will setup a quick configuration which will benchmark one triple store (e.g. apache jena fuseki) using one simulated user. We assume that your triple store (or whatever HTTP GET endpoint you want to use) is running and loaded with data. For now we assume that the endpoint is at <code>http://localhost:3030/ds/sparql</code> and uses GET with the parameter <code>query</code></p> <p>Further on the benchmark should take 10 minutes (or 60.000 ms) and uses plain text queries located in <code>queries.txt</code>. </p> <p>If you do not have created some queries yet, use these for example</p> <pre><code>SELECT * {?s ?p ?o}\nSELECT * {?s ?p ?o} LIMIT 10\nSELECT * {?s &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; ?o}\n</code></pre> <p>and save them to <code>queries.txt</code>.</p> <p>Your results will be written as an N-Triple file to <code>first-benchmark-results.nt</code></p> <p>The following configuration works with these demands. </p> <pre><code># you can ignore this for now\ndatasets:\n  - name: \"Dataset\"\n\n#Your connection\nconnections:\n  - name: \"Fuseki\"\n    # Change this to your actual endpoint you want to use\n    endpoint: \"http://localhost:3030/ds/sparql\"\n\n# The benchmark task\ntasks:\n  - className: \"Stresstest\"\n    configuration:\n      # 10 minutes (time Limit is in ms)\n      timeLimit: 600000\n      # we are using plain text queries\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n\n      # create one SPARQL Worker (it's basically a HTTP get worker using the 'query' parameter\n      # it uses the queries.txt file as benchmark queries\n      workers:\n        - threads: 1\n          className: \"SPARQLWorker\"\n          queriesFile: \"queries.txt\"\n\n# tell Iguana where to save your results to          \nstorages:\n  - className: \"NTFileStorage\"\n    configuration:\n      fileName: \"first-benchmark-results.nt\"\n</code></pre> <p>For more information on the confguration have a look at Configuration </p>"},{"location":"run-iguana/","title":"Start a Benchmark","text":"<p>Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script</p> <pre><code>./start-iguana.sh example-suite.yml\n</code></pre> <p>To set JVM options, you can use <code>$IGUANA_JVM</code></p> <p>For example to let Iguana use 4GB of RAM you can set the <code>IGUANA_JVM</code> as follows</p> <pre><code>export IGUANA_JVM=-Xmx4g\n</code></pre> <p>and start as above.</p> <p>or using the jar with java 11 as follows</p> <pre><code>java -jar iguana-corecontroller-3.3.3.jar example-suite.yml\n</code></pre>"},{"location":"shorthand-mapping/","title":"Shorthand mapping","text":"Shorthand Class Name Stresstest <code>org.aksw.iguana.cc.tasks.impl.Stresstest</code> ---------- ------- InstancesQueryHandler <code>org.aksw.iguana.cc.query.impl.InstancesQueryHandler</code> DelimInstancesQueryHandler <code>org.aksw.iguana.cc.query.impl.DelimInstancesQueryHandler</code> PatternQueryHandler <code>org.aksw.iguana.cc.query.impl.PatternQueryHandler</code> ---------- ------- lang.RDF <code>org.aksw.iguana.cc.lang.impl.RDFLanguageProcessor</code> lang.SPARQL <code>org.aksw.iguana.cc.lang.impl.SPARQLLanguageProcessor</code> lang.SIMPLE <code>org.aksw.iguana.cc.lang.impl.ThrowawayLanguageProcessor</code> ---------- ------- SPARQLWorker <code>org.aksw.iguana.cc.worker.impl.SPARQLWorker</code> UPDATEWorker <code>org.aksw.iguana.cc.worker.impl.UPDATEWorker</code> HttpPostWorker <code>org.aksw.iguana.cc.worker.impl.HttpPostWorker</code> HttpGetWorker <code>org.aksw.iguana.cc.worker.impl.HttpGetWorker</code> CLIWorker <code>org.aksw.iguana.cc.worker.impl.CLIWorker</code> CLIInputWorker <code>org.aksw.iguana.cc.worker.impl.CLIInputWorker</code> CLIInputFileWorker <code>org.aksw.iguana.cc.worker.impl.CLIInputFileWorker</code> CLIInputPrefixWorker <code>org.aksw.iguana.cc.worker.impl.CLIInputPrefixWorker</code> MultipleCLIInputWorker <code>org.aksw.iguana.cc.worker.impl.MultipleCLIInputWorker</code> ---------- ------- NTFileStorage <code>org.aksw.iguana.rp.storages.impl.NTFileStorage</code> RDFFileStorage <code>org.aksw.iguana.rp.storages.impl.RDFFileStorage</code> TriplestoreStorage <code>org.aksw.iguana.rp.storages.impl.TriplestoreStorage</code> ---------- ------- QPS <code>org.aksw.iguana.rp.metrics.impl.QPSMetric</code> AvgQPS <code>org.aksw.iguana.rp.metrics.impl.AvgQPSMetric</code> NoQ <code>org.aksw.iguana.rp.metrics.impl.NoQMetric</code> NoQPH <code>org.aksw.iguana.rp.metrics.impl.NoQPHMetric</code> QMPH <code>org.aksw.iguana.rp.metrics.impl.QMPHMetric</code> EachQuery <code>org.aksw.iguana.rp.metrics.impl.EQEMetric</code>"},{"location":"develop/architecture/","title":"Architecture","text":""},{"location":"develop/architecture/#test1","title":"Test1","text":""},{"location":"develop/architecture/#test2","title":"Test2","text":""},{"location":"develop/extend-lang/","title":"Extend Languages","text":"<p>If you want to add query specific statistics and/or using the correct result size for an HTTP Worker (Post or Get) you can do so. (This may be interesting if you're not using SPARQL)</p> <p>Let's start by implementing the <code>LanguageProcessor</code></p> <pre><code>@Shorthand(\"lang.MyLanguage\")\npublic class MyLanguageProcessor implements LanguageProcessor {\n\n    @Override\n    public String getQueryPrefix() {\n    }\n\n\n    @Override\n    public Model generateTripleStats(List&lt;QueryWrapper&gt; queries, String resourcePrefix, String taskID) {\n    }\n\n    @Override\n    public Long getResultSize(CloseableHttpResponse response) throws ParserConfigurationException, SAXException, ParseException, IOException {\n    }\n\n    @Override\n    Long getResultSize(Header contentTypeHeader, BigByteArrayOutputStream content) throws ParserConfigurationException, SAXException, ParseException, IOException{\n    }\n\n    @Override\n    long readResponse(InputStream inputStream, BigByteArrayOutputStream responseBody) throws IOException{\n    }\n\n\n}\n</code></pre>"},{"location":"develop/extend-lang/#query-prefix","title":"Query prefix","text":"<p>Set a query prefix which will be used in the result set, f.e. \"sql\"</p> <pre><code>    @Override\n    public String getQueryPrefix() {\n        return \"sql\";\n    }\n</code></pre>"},{"location":"develop/extend-lang/#generate-query-statistics","title":"Generate Query Statistics","text":"<p>Generating query specific statistics (which will be added in the result file)</p> <p>You will get the queries (containg of an ID and the query itself) a resourcePrefix you may use to create the URIs and the current taskID.</p> <p>A basic pretty standard exmaple is </p> <pre><code>    @Override\n    public Model generateTripleStats(List&lt;QueryWrapper&gt; queries, String resourcePrefix, String taskID) {\n                Model model = ModelFactory.createDefaultModel();\n        for(QueryWrapper wrappedQuery : queries) {\n            Resource subject = ResourceFactory.createResource(COMMON.RES_BASE_URI + resourcePrefix + \"/\" + wrappedQuery.getId());\n            model.add(subject, RDF.type, Vocab.queryClass);\n            model.add(subject, Vocab.rdfsID, wrappedQuery.getId().replace(queryPrefix, \"\").replace(\"sql\", \"\"));\n            model.add(subject, RDFS.label, wrappedQuery.getQuery().toString());\n\n            //ADD YOUR TRIPLES HERE which contains query specific statistics\n        }\n        return model;\n\n    }\n</code></pre>"},{"location":"develop/extend-lang/#get-the-result-size","title":"Get the result size","text":"<p>To generate the correct result size in the result file do the following</p> <pre><code>    @Override\n    public Long getResultSize(CloseableHttpResponse response) throws ParserConfigurationException, SAXException, ParseException, IOException {\n\n\n    InputStream inStream = response.getEntity().getContent();\n    Long size = -1L;\n    //READ INSTREAM ACCORDINGLY \n\n\n    return size;\n    }\n\n\n    @Override\n    public Long getResultSize(Header contentTypeHeader, BigByteArrayOutputStream content) throws ParserConfigurationException, SAXException, ParseException, IOException {\n    //Read content from Byte Array instead of InputStream\n    InputStream is = new BigByteArrayInputStream(content);\n    Long size=-1L;\n    ...\n\n        return size;\n    }\n\n    @Override\n    public long readResponse(InputStream inputStream, BigByteArrayOutputStream responseBody) throws IOException {\n    //simply moves content from inputStream to the byte array responseBody and returns the size;\n    //will be used for parsing the anwser in another thread.\n    return Streams.inputStream2ByteArrayOutputStream(inputStream, responseBody);\n    }\n\n\n</code></pre>"},{"location":"develop/extend-metrics/","title":"Extend Metrics","text":"<p>Developed a new metric or simply want to use one that isn't implemented?</p> <p>Start by extending the <code>AbstractMetric</code></p> <pre><code>package org.benchmark.metric\n\n@Shorthand(\"MyMetric\")\npublic class MyMetric extends AbstractMetric{\n\n    @Override\n    public void receiveData(Properties p) {\n    }\n\n    @Override\n    public void close() {\n        callbackClose();\n        super.close();\n\n    }\n\n    protected void callbackClose() {\n        //ADD YOUR CLOSING HERE\n    }\n}\n</code></pre>"},{"location":"develop/extend-metrics/#receive-data","title":"Receive Data","text":"<p>This method will receive all the results during the benchmark. </p> <p>You'll receive a few values regarding that one query execution, the time it took, if it succeeded, if not if it was a timeout, a wrong HTTP Code or unkown.  Further on the result size of the query.</p> <p>If your metric is a single value metric you can use the <code>processData</code> method,  which will automatically add each value together.  However if your metric is query specific you can use the <code>addDataToContainter</code> method. (Look at the QPSMetric.</p> <p>Be aware that both mehtods will save the results for each worker used. This allows to calcualte the overall metric as well the metric for each worker itself. </p> <p>We will go with the single-value metric for now.</p> <p>An example on how to retrieve every possible value and saving the time and success.</p> <pre><code>    @Override\n    public void receiveData(Properties p) {\n\n        double time = Double.parseDouble(p.get(COMMON.RECEIVE_DATA_TIME).toString());\n        long tmpSuccess = Long.parseLong(p.get(COMMON.RECEIVE_DATA_SUCCESS).toString());\n        long success = tmpSuccess&gt;0?1:0;\n        long failure = success==1?0:1;\n        long timeout = tmpSuccess==COMMON.QUERY_SOCKET_TIMEOUT?1:0;\n        long unknown = tmpSuccess==COMMON.QUERY_UNKNOWN_EXCEPTION?1:0;\n        long wrongCode = tmpSuccess==COMMON.QUERY_HTTP_FAILURE?1:0;\n        if(p.containsKey(COMMON.RECEIVE_DATA_SIZE)) {\n            size = Long.parseLong(p.get(COMMON.RECEIVE_DATA_SIZE).toString());\n        }\n\n        Properties results = new Properties();\n        results.put(TOTAL_TIME, time);\n        results.put(TOTAL_SUCCESS, success);\n\n        Properties extra = getExtraMeta(p);\n        processData(extra, results);\n    }\n</code></pre>"},{"location":"develop/extend-metrics/#close","title":"Close","text":"<p>In this method you should finally calculate your metric and send the results. </p> <pre><code>    protected void callbackClose() {\n        //create model to contain results \n        Model m = ModelFactory.createDefaultModel();\n\n        Property property = getMetricProperty();\n        Double sum = 0.0;\n\n        // Go over each worker and add metric results to model.\n        for(Properties key : dataContainer.keySet()){\n            Double totalTime = (Double) dataContainer.get(key).get(TOTAL_TIME);\n            Integer success = (Integer) dataContainer.get(key).get(TOTAL_SUCCESS);\n            Double noOfQueriesPerHour = hourInMS*success*1.0/totalTime;\n            sum+=noOfQueriesPerHour;\n            Resource subject = getSubject(key);\n            m.add(getConnectingStatement(subject));\n            m.add(subject, property, ResourceFactory.createTypedLiteral(noOfQueriesPerHour));\n        }\n\n        // Add overall metric to model\n        m.add(getTaskResource(), property, ResourceFactory.createTypedLiteral(sum));\n\n        //Send data to storage\n        sendData(m);\n    }\n\n\n</code></pre>"},{"location":"develop/extend-metrics/#constructor","title":"Constructor","text":"<p>The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task.</p>"},{"location":"develop/extend-queryhandling/","title":"Extend Query Handling","text":"<p>If you want to use another query generating method as the implemented ones you can do so. </p> <p>Start by extend the <code>AbstractWorkerQueryHandler</code>. It will split up the generation for UPDATE queries and Request queries.</p> <pre><code>package org.benchmark.query\n\n\npublic class MyQueryHandler extends AbstractWorkerQueryHandler{\n\n    protected abstract QuerySet[] generateQueries(String queryFileName) {\n\n    }\n\n    protected abstract QuerySet[] generateUPDATE(String updatePath) {\n\n    }\n\n}\n\n</code></pre> <p>for simplicity we will only show the <code>generateQueries</code> as it is pretty much the same. However be aware that the <code>generateUPDATE</code> will use a directory or file instead of just a query file.</p>"},{"location":"develop/extend-queryhandling/#generate-queries","title":"Generate Queries","text":"<p>The class will get a query file containing all the queries.  How you read them and what to do with them is up to you.  You just need to return an array of <code>QuerySet</code>s </p> <p>A query set is simply a container which contains the name/id of the query as well as the query or several queries (f.e. if they are of the same structure but different values). For simplicity we assume that we deal with only one query per query set. </p> <p>Parse your file and for each query create a QuerySet</p> <pre><code>    protected QuerySet[] generateQueries(String queryFileName) {\n        File queryFile = new File(queryFileName);\n        List&lt;QuerySet&gt; ret = new LinkedList&lt;QuerySet&gt;();\n\n        int id=0;       \n        //TODO parse your queries\n            ...\n\n                ret.add(new InMemQuerySet(idPrefix+id++, queryString));\n            ...\n\n\n        return ret.toArray(new QuerySet[]{});\n    }\n</code></pre> <p>This function will parse your query accodringly and add an In Memory QuerySet (another option is a File Based Query Set, where each QuerySet will be stored in a file and IO happens during the benchmark itself.</p>"},{"location":"develop/extend-result-storages/","title":"Extend Result Storages","text":"<p>If you want to use a different storage than RDF you can extend the storages</p> <p>However it is highly optimized for RDF so we suggest to work on top of the <code>TripleBasedStorage</code></p> <pre><code>package org.benchmark.storage\n\n@Shorthand(\"MyStorage\")\npublic class MyStorage extends TripleBasedStorage {\n\n    @Override\n    public void commit() {\n\n    }\n\n\n    @Override\n    public String toString(){\n        return this.getClass().getSimpleName();\n    }\n\n}\n\n</code></pre>"},{"location":"develop/extend-result-storages/#commit","title":"Commit","text":"<p>This should take all the current results, store them and remove them from memory. </p> <p>You can access the results at the Jena Model <code>this.metricResults</code>. </p> <p>For example:</p> <pre><code>\n    @Override\n    public void commit() {\n            try (OutputStream os = new FileOutputStream(file.toString(), true)) {\n            RDFDataMgr.write(os, metricResults, RDFFormat.NTRIPLES);\n            metricResults.removeAll();\n        } catch (IOException e) {\n            LOGGER.error(\"Could not commit to NTFileStorage.\", e);\n        }\n    }\n</code></pre>"},{"location":"develop/extend-result-storages/#constructor","title":"Constructor","text":"<p>The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task.</p>"},{"location":"develop/extend-task/","title":"Extend Tasks","text":"<p>You can extend Iguana with your benchmark task, if the Stresstest doesn't fit your needs. F.e. you may want to check systems if they answer correctly rather than stresstest them.</p> <p>You will need to create your own task either in the Iguana code itself or by using Iguana as a library.  Either way start by extending the AbstractTask.</p> <pre><code>package org.benchmark\n\n@Shorthand(\"MyBenchmarkTask\")\npublic class MyBenchmarkTask extend AbstractTask {\n\n}\n\n</code></pre> <p>You will need to override some functions. For now include them and go through them step by step</p> <pre><code>package org.benchmark\n\n@Shorthand(\"MyBenchmarkTask\")\npublic class MyBenchmarkTask extend AbstractTask {\n\n    //Your constructor(s)\n    public MyBenchmarkTask(Integer timeLimit, ArrayList workers, LinkedHashMap queryHandler) throws FileNotFoundException {\n    }\n\n\n    //Meta Data (which will be added in the resultsfile)\n    @Override\n    public void addMetaData() {\n        super.addMetaData();\n    }\n\n    //Initializing \n    @Override\n    public void init(String[] ids, String dataset, Connection connection)  {\n        super.init(ids, dataset, connection);\n    }\n\n    //Your actual Task \n    @Override\n    public void execute() {\n    }\n\n\n    //Closing the benchmark, freeing some stuff etc.\n    @Override\n    public void close() {\n        super.close();\n    }\n}\n\n</code></pre>"},{"location":"develop/extend-task/#constructor-and-configuration","title":"Constructor and Configuration","text":"<p>Let's start with the Constructor.  The YAML benchmark configuration will provide you the constructor parameters.</p> <p>Imagine you want to have three different parameters.  The first one should provide an integer (e.g. the time limit of the task)  The second one should provide a list of objects (e.g. a list of integers to use) The third parameter should provide a map of specific key-value pairs. </p> <p>You can set this up by using the following parameters:</p> <pre><code>public MyBenchmarkTask(Integer param1, ArrayList param2, LinkedHashMap param3) throws FileNotFoundException {\n    //TODO whatever you need to do with the parameters\n}\n</code></pre> <p>Then Your configuration may look like the following</p> <pre><code>...\n  className: \"MyBenchmarkTask\"\n  configuration:\n    param1: 123\n    param2: \n      - \"1\"\n      - \"2\"\n    param3: \n      val1: \"abc\"\n      val2: 123\n\n</code></pre> <p>The parameters will then be matched by their names to the names of the parameters of your constructor, allowing multiple constructors</p> <p>These are the three types you can represent in a Yaml configuration. * Single Values * Lists of Objects * Key-Value Pairs</p>"},{"location":"develop/extend-task/#add-meta-data","title":"Add Meta Data","text":"<p>If you want to add Meta Data to be written in the results file do the following,</p> <p>Let noOfWorkers a value you already set. </p> <pre><code>    /**\n     * Add extra Meta Data\n     */\n    @Override\n    public void addMetaData() {\n        super.addMetaData();\n\n        Properties extraMeta = new Properties();\n        extraMeta.put(\"noOfWorkers\", noOfWorkers);\n\n        //Adding them to the actual meta data\n        this.metaData.put(COMMON.EXTRA_META_KEY, extraMeta);\n    }\n\n</code></pre> <p>Then the resultsfile will contain all the mappings you put in extraMeta. </p>"},{"location":"develop/extend-task/#initialize-the-task","title":"Initialize the Task","text":"<p>You may want to initialize your task, set some more values, start something in the background etc. etc. </p> <p>You will be provided the <code>suiteID</code>, <code>experimentID</code> and the <code>taskID</code> in the <code>ids</code> array, as well as the name of the dataset and the connection currently beeing benchmarked. </p> <pre><code>    @Override\n    public void init(String[] ids, String dataset, Connection connection)  {\n        super.init(ids, dataset, connection);\n        //ADD YOUR CODE HERE\n    }\n</code></pre> <p>The ids, the dataset and the connection will be set in the <code>AbstractTask</code> which you can simply access by using <code>this.connection</code> for example.</p>"},{"location":"develop/extend-task/#execute","title":"Execute","text":"<p>Now you can create the actual benchmark task you want to use. </p> <pre><code>    @Override\n    public void execute() {\n        //ADD YOUR CODE HERE\n    }\n</code></pre> <p>Be aware that if you are using the <code>workers</code> implemented in Iguana, you need to stop them after your benchmark using the <code>worker.stopSending()</code> method.</p>"},{"location":"develop/extend-task/#close","title":"Close","text":"<p>If you need to close some streams at the end of your benchmark task, you can do that in the <code>close</code> function.</p> <p>Simply override the existing one and call the super method and implement what you need.</p> <pre><code>    @Override\n    public void close() {\n        super.close();\n    }\n</code></pre>"},{"location":"develop/extend-task/#full-overview","title":"Full overview","text":"<pre><code>package org.benchmark\n\n@Shorthand(\"MyBenchmarkTask\")\npublic class MyBenchmarkTask extend AbstractTask {\n\n    private Integer param1;\n    private ArrayList param2;\n    private LinkedHashMap param3;\n\n    //Your constructor(s)\n    public MyBenchmarkTask(Integer param1, ArrayList param2, LinkedHashMap param3) throws FileNotFoundException {\n\n        this.param1=param1;\n        this.param2=param2;\n        this.param3=param3;\n\n    }\n\n\n    //Meta Data (which will be added in the resultsfile)\n    @Override\n    public void addMetaData() {\n        super.addMetaData();\n\n        Properties extraMeta = new Properties();\n        extraMeta.put(\"noOfWorkers\", noOfWorkers);\n\n        //Adding them to the actual meta data\n        this.metaData.put(COMMON.EXTRA_META_KEY, extraMeta);\n    }\n\n    @Override\n    public void init(String[] ids, String dataset, Connection connection)  {\n        super.init(ids, dataset, connection);\n        //ADD YOUR CODE HERE\n    }\n\n    @Override\n    public void execute() {\n        //ADD YOUR CODE HERE\n    }\n\n\n    //Closing the benchmark, freeing some stuff etc.\n    @Override\n    public void close() {\n        super.close();\n    }\n}\n\n</code></pre>"},{"location":"develop/extend-workers/","title":"Extend Workers","text":"<p>If the implemented workers aren't sufficient you can create your own one. </p> <p>Start by extending the <code>AbstractWorker</code> </p> <pre><code>package org.benchmark.workers\n\n@Shorthand(\"MyWorker\")\npublic class MyWorker extends AbstractWorker{\n\n\n    //Setting the next query to be benchmarked in queryStr and queryID\n    public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException{\n\n    }\n\n\n    //Executing the current benchmark query\n    public void executeQuery(String query, String queryID){\n\n    }\n\n}\n</code></pre> <p>These are the only two functions you need to implement, the rest is done by the <code>AbstractWorker</code>.</p> <p>You can override more functions, please consider looking into the javadoc for that.</p>"},{"location":"develop/extend-workers/#constructor","title":"Constructor","text":"<p>The constructor parameters will be provided the same way the Task get's the parameters, thus simply look at Extend Task.</p>"},{"location":"develop/extend-workers/#get-the-next-query","title":"Get the next query","text":"<p>The benchmark task should create and initialize the benchmark queries and will set them accordingly to the worker.</p> <p>You can access these queries using the <code>queryFileList</code> array.  Each element consists of one query set, containing the queryID/name and a list of one to several queries.</p> <p>In the following we will choose the next query set, counted by <code>currentQueryID</code> and use a random query of this. </p> <pre><code>\n    @Override\n    public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException {\n        // get next Query File and next random Query out of it.\n        QuerySet currentQuery = this.queryFileList[this.currentQueryID++];\n        queryID.append(currentQuery.getName());\n\n        int queriesInSet = currentQuery.size();\n        int queryLine = queryChooser.nextInt(queriesInSet);\n        queryStr.append(currentQuery.getQueryAtPos(queryLine));\n\n        // If there is no more query(Pattern) start from beginning.\n        if (this.currentQueryID &gt;= this.queryFileList.length) {\n            this.currentQueryID = 0;\n        }\n\n    }\n</code></pre> <p>Thats it.</p> <p>This exact method is implemented in the <code>AbstractRandomQueryChooserWorker</code> class and instead of extend the <code>AbstractWorker</code> class, you can also extend this and spare your time.  However if you need another way like only executing one query and if there are no mery queries to test end the worker you can do so: </p> <pre><code>\n    @Override\n    public void getNextQuery(StringBuilder queryStr, StringBuilder queryID) throws IOException {\n        // If there is no more query(Pattern) start from beginning.\n        if (this.currentQueryID &gt;= this.queryFileList.length) {\n            this.stopSending();\n        }\n\n\n        // get next Query File and the first Query out of it.\n        QuerySet currentQuery = this.queryFileList[this.currentQueryID++];\n        queryID.append(currentQuery.getName());\n\n        int queriesInSet = currentQuery.size();\n        queryStr.append(currentQuery.getQueryAtPos(0));\n\n    }\n</code></pre>"},{"location":"develop/extend-workers/#execute-the-current-query","title":"Execute the current query","text":"<p>Now you can execute the query against the current connection (<code>this.con</code>).</p> <p>As this is up to you how to do that, here is an example implementation for using HTTP Get.</p> <pre><code>@Override\n    public void executeQuery(String query, String queryID) {\n        Instant start = Instant.now();\n\n        try {\n            String qEncoded = URLEncoder.encode(query, \"UTF-8\");\n            String addChar = \"?\";\n            if (con.getEndpoint().contains(\"?\")) {\n                addChar = \"&amp;\";\n            }\n            String url = con.getEndpoint() + addChar + parameter+\"=\" + qEncoded;\n            HttpGet request = new HttpGet(url);\n            RequestConfig requestConfig = RequestConfig.custom().setSocketTimeout(timeOut.intValue())\n                    .setConnectTimeout(timeOut.intValue()).build();\n\n            if(this.responseType != null)\n                request.setHeader(HttpHeaders.ACCEPT, this.responseType);\n\n            request.setConfig(requestConfig);\n            CloseableHttpClient client = HttpClients.createDefault();\n            CloseableHttpResponse response = client.execute(request, getAuthContext(con.getEndpoint()));\n\n            // method to process the result in background\n            super.processHttpResponse(queryID, start, client, response);\n\n        } catch (Exception e) {\n            LOGGER.warn(\"Worker[{{}} : {{}}]: Could not execute the following query\\n{{}}\\n due to\", this.workerType,\n                    this.workerID, query, e);\n            super.addResults(new QueryExecutionStats(queryID, COMMON.QUERY_UNKNOWN_EXCEPTION, durationInMilliseconds(start, Instant.now())));\n        }\n    }\n</code></pre>"},{"location":"develop/maven/","title":"Use Iguana as a Maven dependency","text":"<p>Iguana provides 3 packages</p> <p>iguana.commons which consists of some helper classes.</p> <p>iguana.resultprocessor which consists of metrics and the result storage workflow</p> <p>and iguana.corecontroller which contains the tasks, the workers, the query handlers, and the overall Iguana workflow</p> <p>to use one of these packages in your maven project add the following repository to your pom:</p> <pre><code>&lt;repository&gt;\n    &lt;id&gt;iguana-github&lt;/id&gt;\n    &lt;name&gt;Iguana Dice Group repository&lt;/name&gt;\n    &lt;url&gt;https://maven.pkg.github.com/dice-group/Iguana&lt;/url&gt;\n&lt;/repository&gt;\n</code></pre> <p>Afterwards add the package you want to add using the following, </p> <p>for the core controller, which will also include the result processor as well as the commons.</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.aksw&lt;/groupId&gt;\n  &lt;artifactId&gt;iguana.corecontroller&lt;/artifactId&gt;\n  &lt;version&gt;${iguana-version}&lt;/version&gt;\n&lt;/dependency&gt; \n</code></pre> <p>for the result processor which will also include the commons. </p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.aksw&lt;/groupId&gt;\n  &lt;artifactId&gt;iguana.resultprocessor&lt;/artifactId&gt;\n  &lt;version&gt;${iguana-version}&lt;/version&gt;\n&lt;/dependency&gt; \n</code></pre> <p>or for the commons.</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.aksw&lt;/groupId&gt;\n  &lt;artifactId&gt;iguana.commons&lt;/artifactId&gt;\n  &lt;version&gt;${iguana-version}&lt;/version&gt;\n&lt;/dependency&gt; \n</code></pre>"},{"location":"develop/overview/","title":"Development Overview","text":"<p>Iguana is open source and available at Github here. There are two main options to work on Iguana. </p> <ul> <li>Fork the git repository and work directly on Iguana</li> <li>or use the Iguana Maven Packages as a library</li> </ul> <p>Iguana is a benchmark framework which can be extended to fit your needs. </p>"},{"location":"develop/overview/#extend","title":"Extend","text":"<p>There are several things you can extend in Iguana. </p> <ul> <li>Tasks - Add your benchmark task</li> <li>Workers - Your system won't work with HTTP GET or POST, or work completely different? Add your specific worker.</li> <li>Query Handling - You do not use Plain Text queries or SPARQL? Add your query handler.</li> <li>Language - Want more statistics about your specific queries? The result size isn't accurate? add your language support</li> <li>Result Storage - Don't want to use  RDF? Add your own solution to store the benchmark results.</li> <li>Metrics - The metrics won't fit your needs? Add your own.</li> </ul>"},{"location":"develop/overview/#bugs","title":"Bugs","text":"<p>For bugs please open an issue at our Github Issue Tracker</p>"},{"location":"usage/configuration/","title":"Configuration","text":"<p>The Configuration explains Iguana how to execute your benchmark. It is divided into 5 categories</p> <ul> <li>Connections</li> <li>Datasets</li> <li>Tasks</li> <li>Storages</li> <li>Metrics</li> </ul> <p>Additionally a pre and post task script hook can be set. </p> <p>The configuration has to be either in YAML or JSON. Each section will be detailed out and shows configuration examples. At the end the full configuration will be shown.  For this we will stick to the YAML format, however the equivalent JSON is also valid and can be parsed by Iguana.</p>"},{"location":"usage/configuration/#connections","title":"Connections","text":"<p>Every benchmark suite can execute several connections (e.g. an HTTP endpoint, or a CLI application).  A connection has the following items</p> <ul> <li>name - the name you want to give the connection, which will be saved in the results.</li> <li>endpoint - the HTTP endpoint or CLI call. </li> <li>updateEndpoint - If your HTTP endpoint is an HTTP Post endpoint set this to the post endpoint. (optional)</li> <li>user - for authentication purposes (optional)</li> <li>password - for authentication purposes (optional)</li> <li>version - setting the version of the tested triplestore, if set resource URI will be ires:name-version (optional)</li> </ul> <p>To setup an endpoint as well as an updateEndpoint might be confusing at first, but if you to test read and write performance simultanously and how updates might have an impact on read performance, you can set up both.</p> <p>For more detail on how to setup the CLI call look at Implemented Workers. There are all CLI Workers explained and how to set the endpoint such that the application will be run correctly.</p> <p>Let's look at an example: </p> <pre><code>connections:\n  - name: \"System1\"\n    endpoint: \"http://localhost:8800/query\"\n    version: 1.0-SNAP\n  - name: \"System2\"\n    endpoint: \"http://localhost:8802/query\"\n    updateEndpoint: \"http://localhost:8802/update\"\n    user: \"testuser\"\n    password: \"secret\"\n</code></pre> <p>Here we have two connections: System1 and System2. System1 is only setup to use an HTTP Get endpoint at http://localhost:8800/query. System2 however uses authentication and has an update endpoint as well, and thus will be correctly test with updates (POSTs) too. </p>"},{"location":"usage/configuration/#datasets","title":"Datasets","text":"<p>Pretty straight forward. You might want to test your system with different datasets (e.g. databases, triplestores etc.)  If you system does not work on different datasets, just add one datasetname like</p> <pre><code>datasets:\n  - name: \"DoesNotMatter\"\n</code></pre> <p>otherwise you might want to benchmark different datasets. Hence you can setup a Dataset Name, as well as file.  The dataset name will be added to the results, whereas both can be used in the task script hooks, to automatize dataset load into your system. </p> <p>Let's look at an example: </p> <pre><code>datasets:\n  - name: \"DatasetName\"\n    file: \"your-data-base.nt\"\n  - name: \"Dataset2\"\n</code></pre>"},{"location":"usage/configuration/#tasks","title":"Tasks","text":"<p>A Task is one benchmark Task which will be executed against all connections for all datasets.  A Task might be a stresstest which we will be using in this example. Have a look at the full configuration of the Stresstest</p> <p>The configuration of one Task consists of the following: </p> <ul> <li>className - The className or Shorthand </li> <li>configuration - The parameters of the task</li> </ul> <pre><code>tasks:\n  - className: \"YourTask\"\n    configuration: \n      parameter1: value1\n      parameter2: \"value2\"\n</code></pre> <p>Let's look at an example: </p> <pre><code>tasks:\n  - className: \"Stresstest\"\n    configuration:\n      #timeLimit is in ms\n      timeLimit: 3600000\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n      workers:\n        - threads: 2\n          className: \"SPARQLWorker\"\n          queriesFile: \"queries.txt\"\n          timeOut: 180000\n  - className: \"Stresstest\"\n    configuration:\n      noOfQueryMixes: 1\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n      workers:\n        - threads: 2\n          className: \"SPARQLWorker\"\n          queriesFile: \"queries.txt\"\n          timeOut: 180000\n</code></pre> <p>We configured two Tasks, both Stresstests. The first one will be executed for one hour and uses simple text queries which can be executed right away. Further on it uses 2 simulated SPARQLWorkers with the same configuration.  At this point it's recommend to check out the Stresstest Configuration in detail for further configuration.</p>"},{"location":"usage/configuration/#storages","title":"Storages","text":"<p>Tells Iguana how to save your results. Currently Iguana supports two solutions</p> <ul> <li>NTFileStorage - will save your results into one NTriple File.</li> <li>RDFFileStorage - will save your results into an RDF File (default TURTLE).</li> <li>TriplestoreStorage - Will upload the results into a specified Triplestore</li> </ul> <p>This is optional. The default storage is <code>NTFileStorage</code>.</p> <p>NTFileStorage can be setup by just stating to use it like </p> <pre><code>storages:\n  - className: \"NTFileStorage\" \n</code></pre> <p>However it can be configured to use a different result file name. The default is <code>results_{DD}-{MM}-{YYYY}_{HH}-{mm}.nt</code>. See example below. </p> <pre><code>storages:\n  - className: \"NTFileStorage\" \n    #optional\n    configuration:\n      fileName: \"results-of-my-benchmark.nt\"\n</code></pre> <p>The RDFFileStorage is similar to the NTFileStorage but will determine the RDF format from the file extension To use RDF/XML f.e. you would end the file on .rdf, for TURTLE end it on .ttl </p> <pre><code>storages:\n  - className: \"NTFileStorage\" \n    #optional\n    configuration:\n      fileName: \"results-of-my-benchmark.rdf\"\n</code></pre> <p>The TriplestoreStorage can be configured as follows: </p> <pre><code>storages:\n  - className: TriplestoreStorage\n    configuration:\n       endpoint: \"http://localhost:9999/sparql\"\n       updateEndpoint: \"http://localhost:9999/update\"\n</code></pre> <p>if you triple store uses authentication you can set that up as follows: </p> <pre><code>storages:\n  - className: TriplestoreStorage\n    configuration:\n       endpoint: \"http://localhost:9999/sparql\"\n       updateEndpoint: \"http://localhost:9999/update\"\n       user: \"UserName\"\n       password: \"secret\"\n</code></pre> <p>For further detail on how to read the results have a look here</p>"},{"location":"usage/configuration/#metrics","title":"Metrics","text":"<p>Let's Iguana know what Metrics you want to include in the results. </p> <p>Iguana supports the following metrics:</p> <ul> <li>Queries Per Second (QPS)</li> <li>Average Queries Per Second (AvgQPS)</li> <li>Query Mixes Per Hour (QMPH)</li> <li>Number of Queries successfully executed (NoQ)</li> <li>Number of Queries per Hour (NoQPH)</li> <li>Each query execution (EachQuery) - experimental</li> </ul> <p>For more detail on each of the metrics have a look at Metrics</p> <p>Let's look at an example:</p> <pre><code>metrics:\n  - className: \"QPS\" \n  - className: \"AvgQPS\"\n  - className: \"QMPH\"\n  - className: \"NoQ\"\n  - className: \"NoQPH\"\n</code></pre> <p>In this case we use all the default metrics which would be included if you do not specify <code>metrics</code> in the configuration at all.  However you can also just use a subset of these like the following:</p> <pre><code>metrics:\n   - className: \"NoQ\"\n   - className: \"AvgQPS\"\n</code></pre> <p>For more detail on how the results will include these metrics have a look at Results.</p>"},{"location":"usage/configuration/#task-script-hooks","title":"Task script hooks","text":"<p>To automatize the whole benchmark workflow, you can setup a script which will be executed before each task, as well as a script which will be executed after each task. </p> <p>To make it easier, the script can get the following values</p> <ul> <li>dataset.name - The current dataset name</li> <li>dataset.file - The current dataset file name if there is anyone</li> <li>connection - The current connection name</li> <li>connection.version - The current connection version, if no version is set -&gt; {{connection.version}}</li> <li>taskID - The current taskID</li> </ul> <p>You can set each one of them as an argument using brackets like <code>{{connection}}</code>.  Thus you can setup scripts which will start your system and load it with the correct dataset file beforehand and stop the system after every task. </p> <p>However these script hooks are completely optional.</p> <p>Let's look at an example:</p> <pre><code>preScriptHook: \"/full/path/{{connection}}-{{connection.version}}/load-and-start.sh {{dataset.file}}\"\npostScriptHook: \"/full/path/{{connection}}/stop.sh\"\n\n</code></pre>"},{"location":"usage/configuration/#full-example","title":"Full Example","text":"<pre><code>connections:\n  - name: \"System1\"\n    endpoint: \"http://localhost:8800/query\"\n  - name: \"System2\"\n    endpoint: \"http://localhost:8802/query\"\n    updateEndpoint: \"http://localhost:8802/update\"\n    user: \"testuser\"\n    password: \"secret\"\n\ndatasets:\n  - name: \"DatasetName\"\n    file: \"your-data-base.nt\"\n  - name: \"Dataset2\"\n\ntasks:\n  - className: \"Stresstest\"\n    configuration:\n      #timeLimit is in ms\n      timeLimit: 3600000\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n      workers:\n        - threads: 2\n          className: \"SPARQLWorker\"\n          queriesFile: \"queries.txt\"\n          timeOut: 180000\n  - className: \"Stresstest\"\n    configuration:\n      noOfQueryMixes: 1\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n      workers:\n        - threads: 2\n          className: \"SPARQLWorker\"\n          queriesFile: \"queries.txt\"\n          timeOut: 180000\n\npreScriptHook: \"/full/path/{{connection}}/load-and-start.sh {{dataset.file}}\"\npostScriptHook: \"/full/path/{{connection}}/stop.sh\"\n\n\nmetrics:\n  - className: \"QMPH\"\n  - className: \"QPS\"\n  - className: \"NoQPH\"\n  - className: \"NoQ\"\n  - className: \"AvgQPS\"\n\nstorages:\n  - className: \"NTFileStorage\" \n  #optional\n  - configuration:\n      fileName: \"results-of-my-benchmark.nt\"\n</code></pre>"},{"location":"usage/configuration/#shorthand","title":"Shorthand","text":"<p>A shorthand is a short name for a class in Iguana which can be used in the configuration instead of the complete class name:  e.g. instead of </p> <pre><code>storages:\n   - className: \"org.aksw.iguana.rp.storage.impl.NTFileStorage\"\n</code></pre> <p>you can use the shortname NTFileStorage:</p> <pre><code>storages:\n   - className: \"NTFileStorage\"\n</code></pre> <p>For a full map of the Shorthands have a look at Shorthand-Mapping</p>"},{"location":"usage/getting-started/","title":"Getting started","text":""},{"location":"usage/getting-started/#what-is-iguana","title":"What is Iguana","text":"<p>Iguana is a HTTP and CLI read/write performance benchmark framework suite.  It can stresstest HTTP get and post endpoints as well as CLI applications using a bunch of simulated users which will bombard the endpoint using queries.  Queries can be anything. SPARQL, SQL, Text and anything else you can fit in one line. </p>"},{"location":"usage/getting-started/#what-can-be-benchmarked","title":"What can be benchmarked","text":"<p>Iguana is capable of benchmarking and stresstesting the following applications</p> <ul> <li>HTTP GET and POST endpoint (e.g. Triple Stores, REST Services, Question Answering endpoints)</li> <li>CLI Applications which either</li> <li>exit after every query</li> <li>or awaiting input after each query</li> </ul>"},{"location":"usage/getting-started/#what-benchmarks-are-possible","title":"What Benchmarks are possible","text":"<p>Every simulated User (named Worker in the following) gets a set of queries.  These queries have to be saved in one file, whereas each query is one line. Hence everything you can fit in one line (e.g a SPARQL query, a text question, an RDF document) can be used as a query and a set of these queries represent the benchmark.  Iguana will then let every Worker execute these queries against the endpoint. </p>"},{"location":"usage/getting-started/#download","title":"Download","text":"<p>Please download the latest release at https://github.com/dice-group/IGUANA/releases/latest</p> <p>The zip file contains 3 files. </p> <ul> <li>iguana-corecontroller-x.y.z.jar</li> <li>example-suite.yml</li> <li>start.sh</li> </ul> <p>The example-suite.yml is a valid benchmark configuration which you can adjust to your needs using the Configuration wiki.</p>"},{"location":"usage/getting-started/#start-a-benchmark","title":"Start a Benchmark","text":"<p>Start Iguana with a benchmark suite (e.g the example-suite.yml) either using the start script</p> <pre><code>./start-iguana.sh example-suite.yml\n</code></pre> <p>or using java 11 if you want to give Iguana more RAM or in general set JVM options.</p> <pre><code>java -jar iguana-corecontroller-3.3.2.jar example-suite.yml\n</code></pre>"},{"location":"usage/languages/","title":"Supported Languages","text":"<p>The Language tag is set to assure that the result size returned by the benchmarked system is correctly read and that result can give a little extra query statistics. </p> <p>Currently two languages are implemented, however you can use <code>lang.SPARQL</code> or simply ignore it all the way.  If they are not in <code>SPARQL</code> the query statistics will be just containing the query text and the result size will be read as if each returned line were one result.</p> <p>Additionaly a SIMPLE language tag is added which parses nothing and sets the result size as the content length of the results.</p> <p>If you work with results which have a content length &gt;=2GB please use <code>lang.SIMPLE</code>, as <code>lang.SPARQL</code> and <code>lang.RDF</code> cannot work with results &gt;=2GB at the moment.</p> <p>The 3 languages are: </p> <ul> <li><code>lang.SPARQL</code></li> <li><code>lang.RDF</code></li> <li><code>lang.SIMPLE</code></li> </ul>"},{"location":"usage/metrics/","title":"Implemented Metrics","text":"<p>Every metric will be calculated globally (for one Experiment Task) and locally (for each Worker)  Hence you can just analyze the overall metrics or if you want to look closer, you can look at each worker. </p>"},{"location":"usage/metrics/#noq","title":"NoQ","text":"<p>The number of successfully executed Queries</p>"},{"location":"usage/metrics/#qmph","title":"QMPH","text":"<p>The number of executed Query Mixes Per Hour </p>"},{"location":"usage/metrics/#noqph","title":"NoQPH","text":"<p>The number of successfully executed Number of Queries Per Hour </p>"},{"location":"usage/metrics/#qps","title":"QPS","text":"<p>For each query the <code>queries per second</code>, the <code>total time</code> in ms (summed up time of each execution), the no of <code>succeeded</code> and <code>failed</code> executions and the <code>result size</code> will be saved. Additionaly will try to tell how many times a query failed with what reason. (<code>timeout</code>, <code>wrong return code</code> e.g. 400, or <code>unknown</code>)</p> <p>Further on the QPS  metrics provides a penalized QPS which penalizes queries which will fail.  As some systems who cannot resolve a query just returns an error code and thus can have a very high score, even though they could only handle a few queries it would be rather unfair to the compared systems. Thus we introduced the penalty QPS. It is calculated the same as the QPS score, but for each failed query it uses the penalty instead of the actual time the failed query took.</p> <p>The default is set to the timeOut of the task.  However you can override it as follows:</p> <pre><code>metrics:\n  - className: \"QPS\"\n    configuration:\n      #in MS\n      penality: 10000\n</code></pre>"},{"location":"usage/metrics/#avgqps","title":"AvgQPS","text":"<p>The average of all queries per second. Also adding a penalizedAvgQPS. Default penalty is timeOut, can be overwritten as follows: </p> <pre><code>metrics: \n  - className: \"AvgQPS\"\n    confiugration:\n      # in ms\n      penalty: 10000\n</code></pre>"},{"location":"usage/metrics/#eachquery","title":"EachQuery","text":"<p>Will save every query execution. (Experimental)</p>"},{"location":"usage/queries/","title":"Supported Queries","text":"<p>There are currently two query types supported:</p> <ul> <li>plain text queries</li> <li>SPARQL pattern queries</li> </ul>"},{"location":"usage/queries/#plain-text-queries","title":"Plain Text Queries","text":"<p>This can be anything: SPARQL, SQL, a whole book if you need to.  The only limitation is that it has to fit in one line per query. If that isn't possible use the Multiple Line Plain Text Queries. Every query can be executed as is. </p> <p>This can be set using the following: </p> <pre><code>...\n    queryHandler:\n      className: \"InstancesQueryHandler\"\n</code></pre>"},{"location":"usage/queries/#sparql-pattern-queries","title":"SPARQL Pattern Queries","text":"<p>This only works for SPARQL Queries at the moment.  The idea came from the DBpedia SPARQL Benchmark paper from 2011 and 2012. </p> <p>Instead of SPARQL queries as they are, you can set variables, which will be exchanged with real data.  Hence Iguana can create thousands of queries using a SPARQL pattern query. </p> <p>A pattern query might look like the following:</p> <pre><code>SELECT * {?s rdf:type %%var0%% ; %%var1%% %%var2%%. %%var2%% ?p ?o}\n</code></pre> <p>This query in itself cannot be send to a triple store, however we can exchange the variables using real data.  Thus we need a reference endpoint (ideally) containing the same data as the dataset which will be tested. </p> <p>This query will then be exchanged to </p> <pre><code>SELECT ?var0 ?var1 ?var2 {?s rdf:type ?var0 ; ?var1 ?var2. ?var2 ?p ?o} LIMIT 2000 \n</code></pre> <p>and be queried against the reference endpoint.</p> <p>For each result (limited to 2000) a query instance will be created.</p> <p>This will be done for every query in the benchmark queries.  All instances of these query patterns will be subsummed as if they were one query in the results. </p> <p>This can be set using the following:</p> <pre><code>...\n    queryHandler:\n      className: \"PatternQueryHandler\"\n      endpoint: \"http://your-reference-endpoint/sparql\"\n</code></pre> <p>or</p> <pre><code>...\n    queryHandler:\n      className: \"PatternQueryHandler\"\n      endpoint: \"http://your-reference-endpoint/sparql\"\n      limit: 4000 \n</code></pre>"},{"location":"usage/queries/#multiple-line-plain-text-queries","title":"Multiple Line Plain Text Queries","text":"<p>Basically like Plain Text Queries. However allows queries which need more than one line.  You basically seperate queries using a delimiter line.</p> <p>Let's look at an example, where the delimiter line is simply an empty line (this is the default)</p> <pre><code>QUERY  1 {\nstill query 1\n}\n\nQUERY 2 {\nstill Query2\n}\n</code></pre> <p>however if you set the delim=<code>###</code> for example the file has to look like: </p> <pre><code>QUERY  1 {\nstill query 1\n}\n###\nQUERY 2 {\nstill Query2\n}\n</code></pre> <p>The delimiter query handler can be set as follows</p> <pre><code>...\n    queryHandler:\n      className: \"DelimInstancesQueryHandler\"\n</code></pre> <p>or if you want to set the delimiter line</p> <pre><code>...\n    queryHandler:\n      className: \"DelimInstancesQueryHandler\"\n      delim: \"###\"\n</code></pre>"},{"location":"usage/results/","title":"Experiment Results","text":""},{"location":"usage/results/#fundamentals","title":"Fundamentals","text":"<p>The results are saved into RDF.  For those who don't know what RDF is, it is best described as a way to represent a directed graph.  The according query language is called SPARQL.  The graph schema of an iguana result is shown above, where as each node represents a class object containg several annotations.</p> <p>To retrieve all TaskIDs you can do the following:</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\n\nSELECT ?taskID {\n    ?suiteID rdf:type iont:Suite . \n    ?suiteID iprop:experiment ?expID .\n    ?expID iprop:task ?taskID .\n}\n</code></pre> <p>Let's look at an example to clarify how to request the global NoQ metric for a taskID you already know. Let's assume the taskID is <code>123/1/1</code></p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\n\nSELECT ?noq {\n    ires:123/1/1 iprop:NoQ ?noq\n}\n</code></pre> <p>If you want to get all the local worker NoQ metrics do the following:</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\n\nSELECT ?workerID ?noq {\n    ires:123/1/1 iprop:workerResult ?workerID\n    ?workerID iprop:NoQ ?noq\n}\n</code></pre> <p>However if you just want to see the global NoQ metric for all taskIDs in your results do the following:</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\n\nSELECT ?taskID ?noq {\n    ?suiteID rdf:type iont:Suite . \n    ?suiteID iprop:experiment ?expID .\n    ?expID iprop:task ?taskID .\n    ?taskID iprop:NoQ ?noq.\n}\n</code></pre> <p>Instead of the NoQ metric you can do this for all other metrics, except <code>QPS</code>.</p> <p>To retrieve <code>QPS</code> look above in the results schema and let's look at an example. Let's assume the taskID is <code>123/1/1</code> again. You can retrieve the global qps values (seen above in ExecutedQueries, e.g <code>QPS</code>, <code>succeeded</code> etc.) as follows,</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\n\nSELECT ?executedQuery ?qps ?failed ?resultSize {\n    ires:123/1/1 iprop:query ?executedQuery .\n    ?executedQuery iprop:QPS ?qps.\n    ?executedQuery iprop:failed ?failed .\n    ?executedQuery iprop:resultSize ?resultSize .\n}\n</code></pre> <p>This will get you the QPS value, the no. of failed queries and the result size of the query.</p> <p>Further on you can show the dataset and connection names. </p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT ?taskID ?datasetLabel ?connectionLabel ?noq {\n    ?suiteID rdf:type iont:Suite . \n    ?suiteID iprop:experiment ?expID .\n    ?expID iprop:dataset ?dataset .\n    ?dataset rdfs:label ?datasetLabel\n    ?expID iprop:task ?taskID .\n    ?taskID iprop:connection ?connection.\n    ?connection rdfs:label ?connectionLabel .\n    ?taskID iprop:NoQ ?noq.\n}\n\n</code></pre> <p>This query will show a table containing for each task, the taskID, the dataset name, the connection name and the no. of queries succesfully executed.</p>"},{"location":"usage/results/#sparql-query-statistics","title":"SPARQL Query statistics","text":"<p>If you were using SPARQL queries as your benchmark queries you can add addtional further statistics of a query, such as: does the query has a FILTER.</p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\n\nSELECT ?executedQuery ?qps ?hasFilter ?queryText {\n    ires:123/1/1 iprop:query ?executedQuery .\n    ?executedQuery iprop:QPS ?qps.\n    ?executedQuery iprop:queryID ?query .\n    ?query iprop:filter ?hasFilter .\n    ?query rdfs:label ?queryText .\n}\n</code></pre> <p>This provides the qps value, if the SPARQL query has a filter and the actual query string. </p>"},{"location":"usage/results/#ontology","title":"Ontology","text":"<p>The results ontology (description of what each property and class means) can be found here</p>"},{"location":"usage/results/#adding-lsq-analyzation","title":"Adding LSQ analyzation","text":"<p>If you're using SPARQL and want some more indepth analysation of the query statistics, you can use LSQ to do so.  Iguana will add an <code>owl:sameAs</code> link between the SPARQL queries used in your benchmark and the equivalent LSQ query links. </p> <p>Hence you can run the performance measurement using Iguana and the query analyzation using LSQ independently and combine both results afterwards</p>"},{"location":"usage/stresstest/","title":"Stresstest","text":"<p>Iguanas implemented Stresstest benchmark task tries to emulate a real case scenario under which an endpoint or application is under high stress.  As in real life endpoints might get multiple simultaneous request within seconds, it is very important to verify that you application can handle this. </p> <p>The stresstest emulates users or applications which will bombard the endpoint using a set of queries for a specific amount of time or a specific amount of queries executed. Each simulated user is called Worker in the following.  As you might want to test read and write performance or just want to emulate different user behaviour, the stresstest allows to configure several workers.  Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time.  However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query.</p>"},{"location":"usage/stresstest/#configuration","title":"Configuration","text":"<p>To configure this task you have to first tell Iguana to use the implemented task like the following:</p> <pre><code>tasks:\n  - className: \"Stresstest\"\n</code></pre> <p>Further on you need to configure the Stresstest using the configuration parameter like:</p> <pre><code>tasks:\n  - className: \"Stresstest\"\n    configuration:\n      timeLimit: 600000\n      ...\n</code></pre> <p>As an end restriction you can either use <code>timeLimit</code> which will stop the stresstest after the specified amount in ms or you can set <code>noOfQueryMixes</code> which stops every worker after they executed the amount of queries in the provided query set.</p> <p>Additionaly to either <code>timeLimit</code> or <code>noOfQueryMixes</code> you can set the following parameters</p> <ul> <li>queryHandler</li> <li>workers</li> <li>warmup (optional)</li> </ul>"},{"location":"usage/stresstest/#query-handling","title":"Query Handling","text":"<p>The queryHandler parameter let's the stresstest know what queries will be used.  Normally you will need the <code>InstancesQueryHandler</code> which will use plain text queries (could be SQL, SPARQL, a whole RDF document). The only restriction is that each query has to be in one line.</p> <p>You can set the query handler like the following:</p> <pre><code>tasks:\n  - className: \"Stresstest\"\n    queryHandler:\n      className: \"InstancesQueryHandler\"\n    ...\n</code></pre> <p>To see which query handlers are supported see  Supported Queries</p>"},{"location":"usage/stresstest/#workers-simulated-users","title":"Workers (simulated Users)","text":"<p>Further on you have to add which workers to use.  As described above you can set different worker configurations.  Let's look at an example:</p> <pre><code>  - className: \"Stresstest\"\n    timeLimit: 600000\n    workers:\n      - threads: 4\n        className: \"SPARQLWorker\"\n        queriesFile: \"/path/to/your/queries.txt\"\n      - threads: 16\n        className: \"SPARQLWorker\"\n        queriesFile: \"/other/queries.txt\"\n        fixedLatency: 5000      \n</code></pre> <p>In this example we have two different worker configurations we want to use. The first want will create 4 <code>SPARQLWorker</code>s using queries at <code>/path/to/your/queries.txt</code> with any latencym thus every query will be executed immediatly after another. The second worker configuration will execute 16 <code>SPARQLWorker</code>s using queries at <code>/other/queries.txt</code> using a fixed waiting time of <code>5000ms</code> between each query.  Hence every worker will execute their queries independently from each other but will wait 5s after each of their query execution before executing the next one. This configuration may simulate that we have a few Users requesting your endpoint locally (e.g. some of your application relying on your database) and several users querying your endpoint from outside the network where we would have network latency and other interferences which we will try to simulate with 5s. </p> <p>A full list of supported workers and their parameters can be found at Supported Workers</p> <p>In this example our Stresstest would create 20 workers, which will simultaenously request the endpoint  for 60000ms (10 minutes).</p>"},{"location":"usage/stresstest/#warmup","title":"Warmup","text":"<p>Additionaly to these you can optionally set a warmup, which will aim to let the system be benchmarked under a normal situation (Some times a database is faster when it was already running for a bit)  The configuration is similar to the stresstest itself you can set a <code>timeLimit</code> (however not a certain no of query executions), you can set different <code>workers</code>, and a <code>queryHandler</code> to use.  If you don't set the <code>queryHandler</code> parameter the warmup will simply use the <code>queryHandler</code> specified in the Stresstest itself. </p> <p>You can set the Warmup as following:</p> <pre><code>tasks:\n  - className: \"Stresstest\"\n    warmup:\n      timeLimit: 600000\n      workers: \n        ...\n      queryHandler:\n        ...\n</code></pre> <p>That's it.  A full example might look like this</p> <pre><code>tasks:\n  - className: \"Stresstest\"\n    configuration:\n      # 1 hour (time Limit is in ms)\n      timeLimit: 3600000\n      # warmup is optional\n      warmup:\n        # 10 minutes (is in ms)\n        timeLimit: 600000\n        # queryHandler could be set too, same as in the stresstest configuration, otherwise the same queryHandler will be use.\n        # workers are set the same way as in the configuration part\n        workers:\n          - threads: 1\n            className: \"SPARQLWorker\"\n            queriesFile: \"queries_warmup.txt\"\n            timeOut: 180000\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n      workers:\n        - threads: 16\n          className: \"SPARQLWorker\"\n          queriesFile: \"queries_easy.txt\"\n          timeOut: 180000\n        - threads: 4\n          className: \"SPARQLWorker\"\n          queriesFile: \"queries_complex.txt\"\n          fixedLatency: 100\n</code></pre>"},{"location":"usage/stresstest/#references","title":"References","text":"<ul> <li>Supported Queries</li> <li>Supported Workers</li> </ul>"},{"location":"usage/tutorial/","title":"Tutorial","text":"<p>In this tutorial we will go through one benchmark using two systems, two datasets and one Stresstest. </p> <p>We are using the following</p> <ul> <li>Iguana v3.0.2</li> <li>Apache Jena Fuseki 3</li> <li>Blazegraph</li> </ul>"},{"location":"usage/tutorial/#download","title":"Download","text":"<p>First lets create a working directory</p> <pre><code>mkdir myBenchmark\ncd myBenchmark\n</code></pre> <p>Now let's download all required systems and Iguana. </p> <p>Starting with Iguana</p> <pre><code>wget https://github.com/dice-group/IGUANA/releases/download/v3.0.2/iguana-3.0.2.zip\nunzip iguana-3.0.2.zip\n</code></pre> <p>Now we will download Blazegraph</p> <pre><code>mkdir blazegraph &amp;&amp; cd blazegraph \nwget https://downloads.sourceforge.net/project/bigdata/bigdata/2.1.5/blazegraph.jar?r=https%3A%2F%2Fsourceforge.net%2Fprojects%2Fbigdata%2Ffiles%2Fbigdata%2F2.1.5%2Fblazegraph.jar%2Fdownload%3Fuse_mirror%3Dmaster%26r%3Dhttps%253A%252F%252Fwww.blazegraph.com%252Fdownload%252F%26use_mirror%3Dnetix&amp;ts=1602007009\ncd ../\n</code></pre> <p>At last we just need to download Apache Jena Fuseki and Apache Jena</p> <pre><code>mkdir fuseki &amp;&amp; cd fuseki\nwget https://downloads.apache.org/jena/binaries/apache-jena-3.16.0.zip\nunzip apache-jena-3.16.0.zip\n\nwget https://downloads.apache.org/jena/binaries/apache-jena-fuseki-3.16.0.zip\nunzip apache-jena-fuseki-3.16.0.zip\n</code></pre> <p>Finally we have to download our datasets.  We use two small datasets from scholarly data.  The ISWC 2010 and the ekaw 2012 rich dataset.</p> <pre><code>mkdir datasets/\ncd datasets\nwget http://www.scholarlydata.org/dumps/conferences/alignments/iswc-2010-complete-alignments.rdf\nwget http://www.scholarlydata.org/dumps/conferences/alignments/ekaw-2012-complete-alignments.rdf\ncd ..\n</code></pre> <p>That's it. Let's setup blazegraph and fuseki.</p>"},{"location":"usage/tutorial/#setting-up-systems","title":"Setting Up Systems","text":"<p>To simplify the benchmark workflow we will use the pre and post script hook, in which we will load the current system and after the benchmark stop the system.</p>"},{"location":"usage/tutorial/#blazegraph","title":"Blazegraph","text":"<p>First let's create the script files</p> <pre><code>cd blazegraph\ntouch load-and-start.sh \ntouch stop.sh\n</code></pre> <p>The <code>load-and-start.sh</code> script will start blazegraph and use curl to POST our dataset.  In our case the datasets are pretty small, hence the loading time is minimal.  Otherwise it would be wise to load the dataset beforehand, backup the <code>blazegraph.jnl</code> file and simply exchanging the file in the pre script hook.</p> <p>For now put this into the script <code>load-and-start.sh</code></p> <pre><code>#starting blazegraph with 4 GB ram\ncd ../blazegraph &amp;&amp; java -Xmx4g -server -jar blazegraph.jar &amp;\n\n#load the dataset file in, which will be set as the first script argument\ncurl -X POST H 'Content-Type:application/rdf+xml' --data-binary '@$1' http://localhost:9999/blazegraph/sparql\n</code></pre> <p>Now edit <code>stop.sh</code> and adding the following:</p> <pre><code>pkill -f blazegraph\n</code></pre> <p>Be aware that this kills all blazegraph instances, so make sure that no other process which includes the word blazegraph is running. </p> <p>finally get into the correct working directory again </p> <pre><code>cd ..\n</code></pre>"},{"location":"usage/tutorial/#fuseki","title":"Fuseki","text":"<p>Now the same for fuseki:</p> <pre><code>cd fuseki\ntouch load-and-start.sh \ntouch stop.sh\n</code></pre> <p>The <code>load-and-start.sh</code> script will load the dataset into a TDB directory and start fuseki using the directory.</p> <p>Edit the script <code>load-and-start.sh</code> as follows</p> <pre><code>cd ../fuseki\n# load the dataset as a tdb directory\napache-jena-3.16.0/bin/tdbloader2 --loc DB $1\n\n# start fuseki\napache-jena-fuseki-3.16.0/fuseki-server --loc DB /ds &amp;\n\n</code></pre> <p>To assure fairness and provide Fuseki with 4GB as well edit <code>apache-jena-fuseki-3.16.0/fuseki-server</code> and go to the last bit exchange the following</p> <pre><code>JVM_ARGS=${JVM_ARGS:--Xmx1200M}\n</code></pre> <p>to </p> <pre><code>JVM_ARGS=${JVM_ARGS:--Xmx4G}\n</code></pre> <p>Now edit <code>stop.sh</code> and adding the following:</p> <pre><code>pkill -f fuseki\n</code></pre> <p>Be aware that this kills all Fuseki instances, so make sure that no other process which includes the word fuseki is running. </p> <p>finally get into the correct working directory again </p> <pre><code>cd ..\n</code></pre>"},{"location":"usage/tutorial/#benchmark-queries","title":"Benchmark queries","text":"<p>We need some queries to benchmark. </p> <p>For now we will just use 3 simple queryies</p> <pre><code>SELECT * {?s ?p ?o}\nSELECT * {?s ?p ?o} LIMIT 10\nSELECT * {?s &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; ?o}\n</code></pre> <p>save this to <code>queries.txt</code></p>"},{"location":"usage/tutorial/#creating-the-benchmark-configuration","title":"Creating the Benchmark Configuration","text":"<p>Now let's create the Iguana benchmark configuration. Create a file called <code>benchmark-suite.yml</code></p> <pre><code>touch benchmark-suite.yml\n</code></pre> <p>Add the following subscections to this file, or simply go to #Full Configuration and add the whole piece to it.</p> <p>Be aware that the configuration will be started on directory level below our working directory and thus paths will use <code>../</code> to get the correct path.</p>"},{"location":"usage/tutorial/#datasets","title":"Datasets","text":"<p>We have two datasets, the ekaw 2012 and the iswc 2010 datasets. Let's name them as such and set the file path, s.t. the script hooks can use the file paths. </p> <pre><code>datasets:\n  - name: \"ekaw-2012\"\n    file: \"../datasets/ekaw-2012-complete-alignments.rdf\"\n  - name: \"iswc-2010\"\n    file: \"../datasets/iswc-2010-complete-alignments.rdf\"\n</code></pre>"},{"location":"usage/tutorial/#connections","title":"Connections","text":"<p>We have two connections, blazegraph and fuseki with their respective endpoint at them as following:</p> <pre><code>connections:\n  - name: \"blazegraph\"\n    endpoint: \"http://localhost:9999/blazegraph/sparql\"\n  - name: \"fuseki\"\n    endpoint: \"http://localhost:3030/ds/sparql\"\n</code></pre>"},{"location":"usage/tutorial/#task-script-hooks","title":"Task script hooks","text":"<p>To assure that the correct triple store will be loaded with the correct dataset add the following pre script hook <code>../{{connection}}/load-and-start.sh {{dataset.file}}</code> <code>{{connection}}</code> will be set to the current benchmarked connection name (e.g. <code>fuseki</code>) and the <code>{{dataset.file}}</code> will be set to the current dataset file path. </p> <p>For example the start script of fuseki is located at <code>fuseki/load-and-start.sh</code>. </p> <p>Further on add the <code>stop.sh</code> script as the post-script hook, assuring that the store will be stopped after each task</p> <p>This will look like this:</p> <pre><code>pre-script-hook: \"../{{connection}}/load-and-start.sh {{dataset.file}}\"\npost-script-hook: \"../{{connection}}/stop.sh\n</code></pre>"},{"location":"usage/tutorial/#task-configuration","title":"Task configuration","text":"<p>We want to stresstest our stores using 10 minutes (60.000 ms)for each dataset connection pair.  We are using plain text queries (<code>InstancesQueryHandler</code>) and want to have two simulated users querying SPARQL queries.  The queries file is located at our working directory at <code>queries.txt</code>. Be aware that we start Iguana one level below, which makes the correct path <code>../queries.txt</code></p> <p>To achieve this restrictions add the following to your file</p> <pre><code>tasks:\n  - className: \"Stresstest\"\n    configuration:\n      timeLimit: 600000\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n      workers:\n        - threads: 2\n          className: \"SPARQLWorker\"\n          queriesFile: \"../queries.txt\"      \n</code></pre>"},{"location":"usage/tutorial/#result-storage","title":"Result Storage","text":"<p>Let's put the results as an NTriple file and for smootheness of this tutorial let's put it into the file <code>my-first-iguana-results.nt</code> </p> <p>Add the following to do this.</p> <pre><code>storages:\n  - className: \"NTFileStorage\"\n    configuration:\n      fileName: \"my-first-iguana-results.nt\"\n</code></pre>"},{"location":"usage/tutorial/#full-configuration","title":"Full configuration","text":"<pre><code>datasets:\n  - name: \"ekaw-2012\"\n    file: \"../datasets/ekaw-2012-complete-alignments.rdf\"\n  - name: \"iswc-2010\"\n    file: \"../datasets/iswc-2010-complete-alignments.rdf\"\n\nconnections:\n  - name: \"blazegraph\"\n    endpoint: \"http://localhost:9999/blazegraph/sparql\"\n  - name: \"fuseki\"\n    endpoint: \"http://localhost:3030/ds/sparql\"\n\npre-script-hook: \"../{{connection}}/load-and-start.sh {{dataset.file}}\"\npost-script-hook: \"../{{connection}}/stop.sh\n\ntasks:\n  - className: \"Stresstest\"\n    configuration:\n      timeLimit: 600000\n      queryHandler:\n        className: \"InstancesQueryHandler\"\n      workers:\n        - threads: 2\n          className: \"SPARQLWorker\"\n          queriesFile: \"../queries.txt\"          \n\nstorages:\n  - className: \"NTFileStorage\"\n    configuration:\n      fileName: \"my-first-iguana-results.nt\"\n</code></pre>"},{"location":"usage/tutorial/#starting-benchmark","title":"Starting Benchmark","text":"<p>Simply use the previous created <code>benchmark-suite.yml</code> and start with</p> <pre><code>cd iguana/\n./start-iguana.sh ../benchmark-suite.yml\n</code></pre> <p>Now we wait for 40 minutes until the benchmark is finished.</p>"},{"location":"usage/tutorial/#results","title":"Results","text":"<p>As previously shown, our results will be shown in <code>my-first-iguana-results.nt</code>.</p> <p>Load this into a triple store of your choice and query for the results you want to use.</p> <p>Just use blazegraph for example:</p> <pre><code>cd blazegraph\n../load-and-start.sh ../my-first-iguana-results.nt\n</code></pre> <p>To query the results go to <code>http://localhost:9999/blazegraph/</code>.</p> <p>An example: </p> <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX iprop: &lt;http://iguana-benchmark.eu/properties/&gt;\nPREFIX iont: &lt;http://iguana-benchmark.eu/class/&gt;\nPREFIX ires: &lt;http://iguana-benchmark.eu/resource/&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT ?taskID ?datasetLabel ?connectionLabel ?noq {\n    ?suiteID rdf:type iont:Suite . \n    ?suiteID iprop:experiment ?expID .\n    ?expID iprop:dataset ?dataset .\n    ?dataset rdfs:label ?datasetLabel\n    ?expID iprop:task ?taskID .\n    ?taskID iprop:connection ?connection.\n    ?connection rdfs:label ?connectionLabel .\n    ?taskID iprop:NoQ ?noq.\n}\n\n</code></pre> <p>This will provide a list of all task, naming the dataset, the connection and the no. of queries which were succesfully executed</p> <p>We will however not go into detail on how to read the results.  This can be read at Benchmark Results</p>"},{"location":"usage/workers/","title":"Supported Workers","text":"<p>A Worker is basically just a thread querying the endpoint/application. It tries to emulate a single user/application requesting your system until it should stop.  In a task (e.g. the stresstest) you can configure several worker configurations which will then be used inside the task.</p> <p>Every worker configuration can additionaly be started several times, hence if you want one configuration executed multiple times, you can simply tell Iguana to run this worker configuration the specified amount of time.  However to assure that the endpoint can't just cache the repsonse of the first request of a query, every worker starts at a pre determined random query, meaning that the single worker will always start at that query to assure fairness in benchmark comparisons, while every worker will start at a different query.</p> <p>There a few workers implemented, which can be seperated into two main categories</p> <ul> <li>Http Workers</li> <li>CLI Workers</li> </ul>"},{"location":"usage/workers/#http-workers","title":"Http Workers","text":"<p>These Workers can be used to benchmark Http Applications (such as a SPARQL endpoint).</p>"},{"location":"usage/workers/#http-get-worker","title":"Http Get Worker","text":"<p>A Http worker using GET requests.  This worker will use the <code>endpoint</code> of the connection.</p> <p>This worker has several configurations listed in the following table:</p> parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) responseType yes The content type the endpoint should return. Setting the <code>Accept:</code> header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>Let's look at an example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"HttpGetWorker\"\n        timeOut: 180000\n        parameterName: \"text\"\n</code></pre> <p>This will use one HttpGetWOrker using a timeout of 3 minutes and the get parameter text to request the query through.</p>"},{"location":"usage/workers/#http-post-worker","title":"Http Post Worker","text":"<p>A Http worker using POST requests.  This worker will use the <code>updateEndpoint</code> of the connection.</p> <p>This worker has several configurations listed in the following table:</p> parameter optional default description queriesFile no File containg the queries this worker should use. parameterName yes query the GET paremter to set the query as value to. (see also Supported Queries ) contentType yes <code>text/plain</code> The content type of the update queries. Setting the <code>Content-Type:</code> header responseType yes The content type the endpoint should return. Setting the <code>Accept:</code> header language yes lang.SPARQL (plain text) The language the queries and response are in (e.g. SPARQL). Basically just creates some more statistics (see Supported Langauges ) timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>Let's look at an example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"HttpPostWorker\"\n        timeOut: 180000\n</code></pre> <p>This will use one HttpGetWOrker using a timeout of 3 minutes.</p>"},{"location":"usage/workers/#sparql-worker","title":"SPARQL Worker","text":"<p>Simply a GET worker but the language parameter is set to <code>lang.SPARQL</code>.  Otherwise see the Http Get Worker for configuration</p> <p>An Example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"SPARQLWorker\"\n        timeOut: 180000\n</code></pre>"},{"location":"usage/workers/#sparql-update-worker","title":"SPARQL UPDATE Worker","text":"<p>Simply a POST worker but specified for SPARQL Updates.</p> <p>Parameters are : </p> parameter optional default description queriesFile no File containg the queries this worker should use. timerStrategy yes <code>NONE</code> <code>NONE</code>, <code>FIXED</code> or <code>DISTRIBUTED</code>. see below for explanation. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>The timerStrategy parameter let's the worker know how to distribute the updates. The fixedLatency and gaussianLatency parameters are not affected, the worker will wait those additionally. </p> <ul> <li>NONE: the worker just updates each update query after another </li> <li>FIXED: calculating the distribution by <code>timeLimit / #updates</code> at the start and waiting the amount between each update. Time Limit will be used of the task the worker is executed in.</li> <li>DISTRIBUTED: calculating the time to wait between two updates after each update by  <code>timeRemaining / #updatesRemaining</code>.</li> </ul> <p>An Example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"UPDATEWorker\"\n        timeOut: 180000\n        timerStrategy: \"FIXED\"\n</code></pre>"},{"location":"usage/workers/#cli-workers","title":"CLI Workers","text":"<p>These workers can be used to benchmark a CLI application. </p>"},{"location":"usage/workers/#cli-worker","title":"CLI Worker","text":"<p>This Worker should be used if the CLI application runs a query once and exits afterwards.  Something like </p> <pre><code>$ cli-script.sh query\nHEADER\nQUERY RESULT 1\nQUERY RESULT 2\n...\n$ \n</code></pre> <p>Parameters are : </p> parameter optional default description queriesFile no File containg the queries this worker should use. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>An Example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"CLIWorker\"\n</code></pre>"},{"location":"usage/workers/#cli-input-worker","title":"CLI Input Worker","text":"<p>This Worker should be used if the CLI application runs and the query will be send using the Input.</p> <p>Something like </p> <pre><code>$ cli-script.sh start\nYour Input: QUERY\nHEADER\nQUERY RESULT 1\nQUERY RESULT 2\n...\n\nYour Input: \n</code></pre> <p>Parameters are : </p> parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>An Example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"CLIInputWorker\"\n        initFinished: \"loading finished\"\n        queryFinished: \"query execution took:\"\n        queryError: \"Error happend during request\"\n</code></pre>"},{"location":"usage/workers/#multiple-cli-input-worker","title":"Multiple CLI Input Worker","text":"<p>This Worker should be used if the CLI application runs and the query will be send using the Input and will quit on errors. </p> <p>Something like </p> <pre><code>$ cli-script.sh start\nYour Input: QUERY\nHEADER\nQUERY RESULT 1\nQUERY RESULT 2\n...\n\nYour Input: ERROR\nERROR happend, exiting \n$ \n</code></pre> <p>To assure a smooth benchmark, the CLI application will be run multiple times instead of once, and if the application quits, the next running process will be used, while in the background the old process will be restarted.  Thus as soon as an error happend, the benchmark can continue without a problem.</p> <p>Parameters are : </p> parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see above. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>An Example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"MultipleCLIInputWorker\"\n        initFinished: \"loading finished\"\n        queryFinished: \"query execution took:\"\n        queryError: \"Error happend during request\"\n</code></pre>"},{"location":"usage/workers/#cli-input-file-worker","title":"CLI Input File Worker","text":"<p>Same as the Multiple CLI Input Worker. However the query won't be send to the input but written to a file and the file will be send to the input</p> <p>Something like </p> <pre><code>$ cli-script.sh start\nYour Input: file-containg-the-query.txt\nHEADER\nQUERY RESULT 1\nQUERY RESULT 2\n...\n\n</code></pre> <p>Parameters are : </p> parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend directory no Directory in which the file including the query should be saved. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>An Example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"CLIInputFileWorker\"\n        initFinished: \"loading finished\"\n        queryFinished: \"query execution took:\"\n        queryError: \"Error happend during request\"      \n        directory: \"/tmp/\"  \n</code></pre>"},{"location":"usage/workers/#cli-input-prefix-worker","title":"CLI Input Prefix Worker","text":"<p>Same as the Multiple CLI Input Worker. However the CLI application might need a pre and suffix. </p> <p>Something like </p> <pre><code>$ cli-script.sh start\nYour Input: PREFIX QUERY SUFFIX\nHEADER\nQUERY RESULT 1\nQUERY RESULT 2\n...\n\n</code></pre> <p>Parameters are : </p> parameter optional default description queriesFile no File containg the queries this worker should use. initFinished no String which occurs when the application is ready to be requested (e.g. after loading) queryFinished no String which occurs if the query response finished queryError no String which occurs when an error during the query execution happend queryPrefix no String to use as a PREFIX before the query. querySuffix no String to use as a SUFFIX after the query. numberOfProcesses yes 5 The number of times the application should be started to assure a smooth benchmark. see Multiple CLI Input Worker. timeOut yes 180000 (3 minutes) The timeout in MS after a query should be aborted fixedLatency yes 0 If the value (in MS) should be waited between each query. Simulating network latency or user behaviour. gaussianLatency yes 0 A random value between <code>[0, 2*value]</code> (in MS) will be waited between each query. Simulating network latency or user behaviour. <p>An Example:</p> <pre><code> ... \n    workers:\n      - threads: 1\n        className: \"CLIInputPrefixWorker\"\n        initFinished: \"loading finished\"\n        queryFinished: \"query execution took:\"\n        queryError: \"Error happend during request\"\n        queryPrefix: \"SPARQL\"\n        querySuffix: \";\"\n</code></pre> <p>Will send the following as Input <code>SPARQL QUERY ;</code></p>"},{"location":"usage/workflow/","title":"Workflow","text":"<p>Iguana will first parse configuration and afterwards will execute each task for each connection for each dataset. </p> <p>Imagine it like the following:</p> <ul> <li>for each dataset D<ul> <li>for each connection C<ul> <li>for each task T<ol> <li>execute pre script hook</li> <li>execute task T(D, C)</li> <li>collect and calculate results</li> <li>write results</li> <li>execute post script hook</li> </ol> </li> </ul> </li> </ul> </li> </ul>"}]}